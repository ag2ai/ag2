# Copyright (c) 2023 - 2025, AG2ai, Inc., AG2ai open-source projects maintainers and core contributors
#
# SPDX-License-Identifier: Apache-2.0

import asyncio
from copy import deepcopy
from typing import Any, Optional, Union

from pydantic import BaseModel, Field

from .... import (
    Agent,
    ConversableAgent,
    UpdateSystemMessage,
)
from ....doc_utils import export_module
from ....oai.client import OpenAIWrapper

__all__ = ["EvaluationAgent"]


@export_module("autogen.agents.contrib")
class EvaluationAgent(ConversableAgent):
    """Utilises multiple agents, evaluating their performance then selecting and returning the best one."""

    # Internal process:
    # 1. Synthesize the task from the input
    # 2. Each agent gives their response
    # 3. Evaluator evaluates and selects the response
    # 4. Return the selected response

    DEFAULT_EVALUATOR_MESSAGE = (
        "You are responsible for evaluating and selecting the best response from a set of agents. "
        "Each agent, identified by a name, will be given a chance to respond. "
        "Evaluation Criteria:\n[evaluation_guidance]\n"
        "[agent_outputs]"
    )

    DEFAULT_EVALUATON_GUIDANCE = (
        "1. Carefully review each approach and result\n"
        "2. Evaluate each solution based on criteria appropriate to the task\n"
        "3. Select the absolute best response\n"
        "4. You must select a response as the best response"
    )

    DEFAULT_REPLY_TEMPLATE = "AGENT '[agent_name]' RESPONSE SELECTED.\n\nREASON:\n[reason]\n\nRESPONSE:\n[response]"

    def __init__(
        self,
        *,
        llm_config: dict[str, Any],
        agents: list[ConversableAgent],
        evaluation_guidance: Optional[str] = None,
        reply_template: Optional[str] = None,
        **kwargs: Any,
    ) -> None:
        """Initialize the EvaluationAgent.

        Args:
            llm_config (dict[str, Any]): LLM Configuration for the internal synthesizer and evaluator agents.
            agents (list[ConversableAgent]): List of agents that will provide their responses for evaluation.
            evaluation_guidance (str): Guidance on how to evaluate the agents, used by the internal evaluator agent.
                Default is:
                "1. Carefully review each approach and result\n2. Evaluate each solution based on criteria appropriate to the task\n3. Select the absolute best response\n4. You must select a response as the best response"
            reply_template (str): Template for the reply to be generated by the EvaluationAgent.
                Three placeholders are available for substitution: [agent_name], [reason], and [response].
                Default is:
                "AGENT '[agent_name]' RESPONSE SELECTED.\n\nREASON:\n[reason]\n\nRESPONSE:\n[response]"
            **kwargs (Any): Additional keyword arguments to pass to the base class.
        """

        assert len(agents) > 1, "EvaluationAgent requires at least two agents for evaluation."
        assert llm_config, "EvaluationAgent requires llm_config for the internal synthesizer and evaluator agents."

        # Initialise the base class, ignoring llm_config as we'll put that on internal agents
        super().__init__(**kwargs)

        # Store your custom parameters
        self._evaluation_agents = agents
        self._evaluation_llm_config = llm_config
        self._evaluation_guidance = evaluation_guidance if evaluation_guidance else self.DEFAULT_EVALUATON_GUIDANCE
        self._evaluation_reply_template = reply_template if reply_template else self.DEFAULT_REPLY_TEMPLATE

        # Create agents
        self._create_synthesizer()
        self._create_evaluator()

        # Register our reply function for evaluation with the agent
        # This will be the agent's only reply function
        self.register_reply(
            trigger=[Agent, None], reply_func=self._generate_evaluate_reply, remove_other_reply_funcs=True
        )

    # Structured Output for the synthesizer agent
    class EvaluationTask(BaseModel):
        task: str = Field(description="The task to be solved by the agents.")
        clarification_needed: Optional[str] = Field(
            description="If the task is not clear, describe clarity needed. Only ask if absolutely critical."
        )

    def _create_synthesizer(self) -> None:
        """Create the internal synthesizer agent."""

        # Add the response_format to the agent
        synthesizer_llm_config = deepcopy(self._evaluation_llm_config)
        synthesizer_llm_config["response_format"] = EvaluationAgent.EvaluationTask

        self._synthesizer_agent = ConversableAgent(
            name="evaluationagent_synthesizer",
            llm_config=synthesizer_llm_config,
            system_message="Analyze the messages and determine the task being asked to be solved and reply with it, keeping it as close to word-to-word as possible. If clarification is needed, provide details on the clarity needed.",
        )

    def _generate_evaluator_system_message(self, agent: ConversableAgent, messages: list[dict[str, Any]]) -> str:
        """Generate the system message for the internal evaluator agent."""
        system_message = EvaluationAgent.DEFAULT_EVALUATOR_MESSAGE.replace(
            "[evaluation_guidance]", self._evaluation_guidance
        )

        # Compile the responses to the answers here.

        return system_message

    # Structured Output for the evaluator agent
    class NominatedResponse(BaseModel):
        agent_name: str = Field(description="Name of agent that provided the response.")
        response: str = Field(description="Exact, word-for-word, response selected.")
        reason: str = Field(description="Brief reason why it was the best response.")

    def _create_evaluator(self) -> None:
        """Create the internal evaluator agent."""

        # Add the response_format to the agent
        evaluator_llm_config = deepcopy(self._evaluation_llm_config)
        evaluator_llm_config["response_format"] = EvaluationAgent.NominatedResponse

        self._evaluator_agent = ConversableAgent(
            name="evaluationagent_evaluator",
            llm_config=evaluator_llm_config,
            update_agent_state_before_reply=[UpdateSystemMessage(self._generate_evaluator_system_message)],
        )

    def _consolidate_messages(self, messages: Optional[Union[list[dict[str, Any]], str]]) -> str:  # type: ignore[type-arg]
        """Consolidates the external chat's messages for the Synthesizer to analyse"""
        if isinstance(messages, str):
            return messages
        elif isinstance(messages, list) and len(messages) > 0:
            # Loop through messages and consolidate, taking into account same may be tool calls and some may be tool responses, which we'll ignore.
            # If the message has content and name then it should combine as "name:\ncontent\n\n"
            consolidated_message = ""
            for message in messages:
                if "content" in message and "name" in message:
                    consolidated_message += f"{message['name']}:\n{message['content']}\n\n"
            return consolidated_message.strip()
        else:
            raise NotImplementedError("Invalid messages format. Must be a list of messages or a string.")

    def _compile_responses_nested_chat(self, task: str) -> list[dict[str, Any]]:
        """Compile the nested chat for the responses part of the evaluation process."""

        nested_chats = []

        for i, agent in enumerate(self._evaluation_agents):
            agent_dict = {
                "recipient": agent,
                "chat_id": agent.name,
                "message": f"Please provide your response to the task:\n\n{task}",
                "summary_method": "last_msg",
                "max_turns": 1,
                # Exclude all chat results before this one from being carried over (so they don't influence this agent)
                "finished_chat_indexes_to_exclude_from_carryover": [] if i == 0 else list(range(i)),
            }

            nested_chats.append(agent_dict)

        return nested_chats

    def _compile_nested_responses(
        self, sender: ConversableAgent, recipient: ConversableAgent, summary_args: dict[str, Any]
    ) -> str:
        response: str = ""
        self._evaluation_agent_responses: dict[str, str] = {}

        for agent in self._evaluation_agents:
            if recipient.chat_messages[agent] and len(recipient.chat_messages[agent]) == 2:
                agent_response = recipient.chat_messages[agent][-1]
                response += f"AGENT '{agent.name}' RESPONSE:\n{agent_response['content']}\n\n" + "-" * 50 + "\n\n"
                self._evaluation_agent_responses[agent.name] = agent_response["content"]
            else:
                return ""  # At least one of the agents didn't respond, abort.

        return response

    # Inner evaluation process
    def _generate_evaluate_reply(
        self,
        agent: ConversableAgent,
        messages: Optional[list[dict[str, Any]]] = None,
        sender: Optional[Agent] = None,
        config: Optional[OpenAIWrapper] = None,
    ) -> tuple[bool, Union[str, dict[str, Any]]]:
        # 1. Synthesize the task from the input
        user_agent = ConversableAgent(
            name="evaluation_user",
            human_input_mode="NEVER",
        )

        consolidate_incoming_messages = self._consolidate_messages(messages)

        sythesized_result = user_agent.initiate_chat(
            recipient=self._synthesizer_agent, message=consolidate_incoming_messages, max_turns=1
        )

        # Evaluate the result of the task synthesis
        try:
            evaluation_task = EvaluationAgent.EvaluationTask.model_validate_json(sythesized_result.summary)
        except Exception as e:
            return True, {"content": f"EvaluationAgent was unable to determine the task: {e}"}

        if not evaluation_task.task:
            return True, {"content": "EvaluationAgent was unable to determine the task."}

        if evaluation_task.clarification_needed:
            return True, {"content": f"I need clarity on the task: {evaluation_task.clarification_needed}"}

        task = evaluation_task.task

        # 2. Each agent gives their response using an asynchronous nested chat
        gathering_agent = ConversableAgent(
            name="evaluation_gather",
        )

        # Create the nested chats for all the agents to respond
        responses_nested_chat = self._compile_responses_nested_chat(task=task)

        # Associate that with the gathering_agent
        gathering_agent.register_nested_chats(
            chat_queue=responses_nested_chat,
            position=0,
            use_async=True,
            trigger=Agent,  # Any agent sender will trigger this
        )

        # Synchronously get the responses
        """
        responses_result = user_agent.initiate_chat(
            recipient=gathering_agent,
            max_turns=1,
            message="", # Prevent it trying to get user input
            summary_method=self._compile_nested_responses
        )
        """

        # Asynchronously get the responses
        responses_result = asyncio.run(
            user_agent.a_initiate_chat(
                recipient=gathering_agent,
                max_turns=1,
                message="",  # Prevent it trying to get user input
                summary_method=self._compile_nested_responses,
            )
        )

        compiled_responses = responses_result.summary

        if compiled_responses == "":
            return True, {"content": "EvaluationAgent was unable to gather responses from all agents."}

        # 3. Evaluator evaluates and selects the response
        evaluation = user_agent.initiate_chat(recipient=self._evaluator_agent, message=compiled_responses, max_turns=1)

        # Extract the nominated response
        try:
            nominated_response = EvaluationAgent.NominatedResponse.model_validate_json(evaluation.summary)
        except Exception as e:
            return True, {"content": f"EvaluationAgent was unable to select the best response: {e}"}

        if not nominated_response.response:
            return True, {"content": "EvaluationAgent was unable to select a response."}

        # Ensure the nominated agent name exists
        if nominated_response.agent_name not in [a.name for a in self._evaluation_agents]:
            return True, {"content": "EvaluationAgent provided an invalid agent name when selecting a response."}

        nominated_agent = nominated_response.agent_name
        nominated_reason = nominated_response.reason
        agent_response = self._evaluation_agent_responses[nominated_agent]

        # We'll get the response from the agent's original response, rather than this structured output one
        # so that we can ensure it remains as it was originally

        # Compile the response and return it using the self._evaluation_reply_template
        compiled_reply = (
            self._evaluation_reply_template.replace("[agent_name]", nominated_agent)
            .replace("[reason]", nominated_reason)
            .replace("[response]", agent_response)
        )

        return True, compiled_reply
