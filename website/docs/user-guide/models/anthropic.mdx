---
title: Anthropic
sidebarTitle: Anthropic
---

Anthropic's Claude is a family of large language models developed by Anthropic and designed to revolutionize the way you interact with AI. Claude excels at a wide variety of tasks involving language, reasoning, analysis, coding, and more. The models are highly capable, easy to use, and can be customized to suit your needs.

In this notebook, we demonstrate how to use Anthropic Claude model for AgentChat in AG2.

## Features

- Function/tool calling
- Structured Outputs ([Notebook example](/docs/use-cases/notebooks/notebooks/agentchat_structured_outputs))
- Token usage and cost correctly as per Anthropic's API costs (as of December 2024)
- [Extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking?q=extended_thinking#pricing-and-token-usage-for-extended-thinking)
- Streaming responses via the Anthropic API (`stream=True`)

## Requirements
To use Anthropic Claude with AG2, you need to install the `ag2[anthropic]` package, which will install a recent Anthropic SDK with streaming and GA structured outputs support.

To use function call, tool use, structured outputs, or streaming features of Claude models, you **must** install `anthropic[vertex]>=0.79.0`.

```bash
# If you need to install AG2 with Anthropic
pip install ag2[anthropic]
```

<Tip>
If you have been using `autogen` or `ag2`, all you need to do is upgrade it using:
```bash
pip install -U autogen[anthropic]
```
or
```bash
pip install -U ag2[anthropic]
```
as `autogen` and `ag2` are aliases for the same PyPI package.
</Tip>

## Set the config for the Anthropic API

You can add any parameters that are needed for the custom model loading in the same configuration list.

It is important to add the `api_type` field and set it to a string that corresponds to the client type used: `anthropic`.

Example:
```
[
    {
        "model": "claude-3-5-sonnet-20240620",
        "api_key": "your api key",
        "api_type": "anthropic",
    },
    {
        "model": "claude-3-7-sonnet-20250219",
        "api_key": "your api key",
        "api_type": "anthropic",
        "max_tokens": 8192, # override the default value of 4096, max tokens must be greater than thinking budget
        "timeout": 600, # for larger thinking budgets, increase the timeout OR enable streaming
        "thinking": {"type": "enabled", "budget_tokens": 2048},
    }
    {
        "model": "claude-3-sonnet-20240229",
        "api_key": "your api key",
        "api_type": "anthropic",
        "temperature": 0.5,
        "top_p": 0.2, # Note: It is recommended to set temperature or top_p but not both.
        "max_tokens": 10000,
    },
    {
        "model":"claude-3-opus-20240229",
        "api_key":"your api key",
        "api_type":"anthropic",
    },
    {
        "model":"claude-2.0",
        "api_key":"your api key",
        "api_type":"anthropic",
    },
    {
        "model":"claude-2.1",
        "api_key":"your api key",
        "api_type":"anthropic",
    },
    {
        "model":"claude-3.0-haiku",
        "api_key":"your api key",
        "api_type":"anthropic",
    },
]
```

### Alternative

As an alternative to the api_key key and value in the config, you can set the environment variable `ANTHROPIC_API_KEY` to your Anthropic API key.

Linux/Mac:
```
export ANTHROPIC_API_KEY="your Anthropic API key here"
```
Windows:
```
set ANTHROPIC_API_KEY=your_anthropic_api_key_here
```

```python
import os

from typing_extensions import Annotated

import autogen

llm_config_claude = autogen.LLMConfig(config_list={
    # Choose your model name.
    "model": "claude-3-5-sonnet-20240620",
    # You need to provide your API key here.
    "api_key": os.getenv("ANTHROPIC_API_KEY"),
    "api_type": "anthropic",
})
```

### Alternative Anthropic VertexAI Client (GCP)

To use the Anthropic VertexAI client in AG2, you need to configure it for use with Google Cloud Platform (GCP). Ensure you have the necessary project credentials and install the required package.

Configuration

The following configuration example demonstrates how to set up Anthropic VertexAI:

```python
import os

llm_config_vertexai = LLMConfig({
    "model": "claude-3-5-sonnet-20240620-v1:0",
    "gcp_project_id": "your_project_id",
    "gcp_region": "us-west-2",  # Replace with your GCP region
    "gcp_auth_token": None,  # Optional: If not passed, Google Default Authentication will be used
    "api_type": "anthropic",
})

assistant = autogen.AssistantAgent("assistant", llm_config=llm_config_vertexai)
```

### Alternative Anthropic VertexAI Client (Google Default Authentication)

If the `gcp_auth_token` is not provided in the configuration, the client will use Google’s default authentication mechanism. This requires the appropriate credentials to be configured in your environment, such as:

- Service account key: You can set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to your service account key file.
- Cloud Shell or GCP Compute Engine: When running in a GCP-managed environment, default authentication is automatically applied.


Example of setting up the environment variable:

```bash
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your/service-account-key.json"
```

This allows seamless integration without explicitly specifying the authentication token in your code.

## Streaming Responses with Anthropic

Anthropic API now supports streaming responses. To enable this, pass `"stream": True` in your LLM configuration:

```python
llm_config = {
    "config_list": [
        {
            "model": "claude-3-7-sonnet-20250219",
            "api_key": os.getenv("ANTHROPIC_API_KEY"),
            "api_type": "anthropic",
            "stream": True,  # Enable streaming
            "max_tokens": 4096,
        }
    ]
}
```

When streaming is enabled, Claude returns results as they are generated, allowing real-time display or incremental processing. The AgentChat interface emits partial completions as streaming events, and the final result is accumulated as a normal message for your workflow.

This is especially useful for large completions, long answers, or interactive UIs. You may also wish to use streaming when working with large `thinking` budgets or in any use-case benefiting from immediate model responses.

## Structured Outputs (GA API)

Anthropic now supports structured outputs as a generally available (GA) feature (no longer requiring any beta endpoint). This allows you to request responses in a precise schema, such as a JSON format defined by your OpenAPI/Pydantic schema.

- For Pydantic models (no tools), use the `output_format` parameter with your model class.
- For raw JSON schema or with tools, supply an `output_config` (which includes a JSON schema) in your config list or directly in the call.
- There is no need for custom beta headers; strict and structured outputs are now supported via the main API.

Example (with a simple JSON schema):
```python
config_list = [{
    "model": "claude-sonnet-4-5",
    "api_key": os.getenv("ANTHROPIC_API_KEY"),
    "api_type": "anthropic",
    # Use output_config for structured output
    "output_config": {
        "format": {
            "type": "json_schema",
            "schema": your_json_schema,
        }
    },
}]
```

If using Pydantic models without tools, you can pass the model itself as `output_format`.

Tool/function calling with strict argument checking is now generally available. You can use the same OpenAI tool/function specification for Anthropic, and strict validation happens by default—no beta parameters are needed.

---

## Two-agent Coding Example

### Construct Agents

Construct a simple conversation between a User proxy and an ConversableAgent based on Claude-3 model.

```python
assistant = autogen.AssistantAgent("assistant", llm_config=llm_config_claude)

user_proxy = autogen.UserProxyAgent(
    "user_proxy",
    human_input_mode="NEVER",
    code_execution_config={
        "work_dir": "coding",
        "use_docker": False,
    },
    is_termination_msg=lambda x: x.get("content", "") and x.get("content", "").rstrip().endswith("TERMINATE"),
    max_consecutive_auto_reply=1,
)

user_proxy.initiate_chat(
    assistant, message="Write a python program to print the first 10 numbers of the Fibonacci sequence."
)
```
... (rest of content unchanged) ...

## Thinking mode

You can utilize Anthropic's thinking mode by setting it in the configuration.

```
from autogen import ConversableAgent

# Here we configure the thinking mode and the token budget for thinking
llm_config = {
    "config_list": [
        {
            "model": "claude-3-7-sonnet-20250219",
            "api_type": "anthropic",
            "max_tokens": 8192, # override the default value of 4096, max tokens must be greater than thinking budget
            "timeout": 600, # for larger thinking budgets, increase the timeout OR enable streaming
            "thinking": {"type": "enabled", "budget_tokens": 2048},
        }
    ],
    # Note: don't pass in temperature or top_p to avoid exceptions in thinking mode
}

agent = ConversableAgent(
    name="test",
    llm_config=llm_config,
)

# Create the run
response = agent.run(
    message="Please provide a comparison of JS and TS",
    user_input=False,
    max_turns=1,
)

# Process the run
response.process()

# Print out the final response (thinking tokens are not shown)
print(response.summary)
```
 