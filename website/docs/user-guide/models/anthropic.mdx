---
title: Anthropic
sidebarTitle: Anthropic
---

Anthropic's Claude is a family of large language models developed by Anthropic and designed to revolutionize the way you interact with AI. Claude excels at a wide variety of tasks involving language, reasoning, analysis, coding, and more. The models are highly capable, easy to use, and can be customized to suit your needs.

In this notebook, we demonstrate how to use Anthropic Claude model for AgentChat in AG2.

## Features

- Function/tool calling
- Streaming
- Structured Outputs ([Notebook example](/docs/use-cases/notebooks/notebooks/agentchat_structured_outputs))
- Token usage and cost correctly as per Anthropic's API costs (as of December 2024)
- [Extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking?q=extended_thinking#pricing-and-token-usage-for-extended-thinking)

## Requirements
To use Anthropic Claude with AG2, first you need to install the `ag2[anthropic]` package.

To use the full feature set of Claude models (tool calls, streaming, structured outputs), you must install `anthropic>=0.79.0`.

```bash
# If you need to install AG2 with Anthropic and the correct Anthropic package:
pip install 'ag2[anthropic]'
# The ag2[anthropic] or autogen[anthropic] install will pull in anthropic>=0.79.0.
```

<Tip>
If you have been using `autogen` or `ag2`, all you need to do is upgrade it using:
```bash
pip install -U 'autogen[anthropic]'
```
or
```bash
pip install -U 'ag2[anthropic]'
```
as `autogen` and `ag2` are aliases for the same PyPI package.
</Tip>

## Set the config for the Anthropic API

You can add any parameters that are needed for the custom model loading in the same configuration list.

It is important to add the `api_type` field and set it to a string that corresponds to the client type used: `anthropic`.

Example:
```
[
    {
        "model": "claude-opus-4",
        "api_key": "your api key",
        "api_type": "anthropic",
        "max_tokens": 8192,
    },
    {
        "model": "claude-sonnet-4-5-20250929",
        "api_key": "your api key",
        "api_type": "anthropic",
        "max_tokens": 8192, # override the default value, max tokens must be greater than thinking budget
        "timeout": 600, # for larger thinking budgets, increase the timeout OR enable streaming
        "thinking": {"type": "enabled", "budget_tokens": 2048},
    },
    {
        "model": "claude-haiku-4-5",
        "api_key": "your api key",
        "api_type": "anthropic",
        "temperature": 0.5,
        "top_p": 0.2, # It is recommended to set temperature or top_p but not both.
        "max_tokens": 10000,
    },
]
```

### Alternative

As an alternative to the api_key key and value in the config, you can set the environment variable `ANTHROPIC_API_KEY` to your Anthropic API key.

Linux/Mac:
```
export ANTHROPIC_API_KEY="your Anthropic API key here"
```
Windows:
```
set ANTHROPIC_API_KEY=your_anthropic_api_key_here
```

```python
import os

from typing_extensions import Annotated

import autogen

llm_config_claude = autogen.LLMConfig(config_list={
    # Choose your model name.
    "model": "claude-sonnet-4-5",
    # You need to provide your API key here.
    "api_key": os.getenv("ANTHROPIC_API_KEY"),
    "api_type": "anthropic",
})
```

### Alternative Anthropic VertexAI Client (GCP)

To use the Anthropic VertexAI client in AG2, you need to configure it for use with Google Cloud Platform (GCP). Ensure you have the necessary project credentials and install the required package.

Configuration

The following configuration example demonstrates how to set up Anthropic VertexAI:

```python
import os

llm_config_vertexai = LLMConfig({
    "model": "claude-sonnet-4-5-20250929-v1:0",
    "gcp_project_id": "your_project_id",
    "gcp_region": "us-west-2",  # Replace with your GCP region
    "gcp_auth_token": None,  # Optional: If not passed, Google Default Authentication will be used
    "api_type": "anthropic",
})

assistant = autogen.AssistantAgent("assistant", llm_config=llm_config_vertexai)
```

### Alternative Anthropic VertexAI Client (Google Default Authentication)

If the `gcp_auth_token` is not provided in the configuration, the client will use Googleâ€™s default authentication mechanism. This requires the appropriate credentials to be configured in your environment, such as:

- Service account key: You can set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to your service account key file.
- Cloud Shell or GCP Compute Engine: When running in a GCP-managed environment, default authentication is automatically applied.


Example of setting up the environment variable:

```bash
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your/service-account-key.json"
```

This allows seamless integration without explicitly specifying the authentication token in your code.

## Two-agent Coding Example

### Construct Agents

Construct a simple conversation between a User proxy and a ConversableAgent based on Claude 4.x model. Ensure your model name matches the supported Anthropic model list.

```python
assistant = autogen.AssistantAgent("assistant", llm_config=llm_config_claude)

user_proxy = autogen.UserProxyAgent(
    "user_proxy",
    human_input_mode="NEVER",
    code_execution_config={
        "work_dir": "coding",
        "use_docker": False,
    },
    is_termination_msg=lambda x: x.get("content", "") and x.get("content", "").rstrip().endswith("TERMINATE"),
    max_consecutive_auto_reply=1,
)

user_proxy.initiate_chat(
    assistant, message="Write a python program to print the first 10 numbers of the Fibonacci sequence."
)
```

... (console output remains unchanged) ...

## Tool Call Example with the Latest Anthropic API
Anthropic announced that tool use is supported in the Anthropic API. **You must use `anthropic>=0.79.0` for all new tool/structured output features and streaming**.

### Register the function

```python
@user_proxy.register_for_execution()  # Decorator factory for registering a function to be executed by an agent
@assistant.register_for_llm(
    name="get_weather", description="Get the current weather in a given location."
)  # Decorator factory for registering a function to be used by an agent
def preprocess(location: Annotated[str, "The city and state, e.g. Toronto, ON."]) -> str:
    return "Absolutely cloudy and rainy"

user_proxy.initiate_chat(
    assistant,
    message="What's the weather in Toronto?",
)
```

... (console output remains unchanged) ...

## Group Chat Example with both Claude and GPT Agents

... (rest unchanged - but ensure the Claude models referenced match current naming, such as "claude-sonnet-4-5", "claude-opus-4", etc.) ...

## Thinking mode

You can utilize Anthropic's thinking mode by setting it in the configuration. (The settings and sample code remain the same, but ensure you use a supported Claude 4.x model in the example.)

```
from autogen import ConversableAgent

# Here we configure the thinking mode and the token budget for thinking
llm_config = {
    "config_list": [
        {
            "model": "claude-sonnet-4-5-20250929",
            "api_type": "anthropic",
            "max_tokens": 8192, # override the default value of 4096, max tokens must be greater than thinking budget
            "timeout": 600, # for larger thinking budgets, increase the timeout OR enable streaming
            "thinking": {"type": "enabled", "budget_tokens": 2048},
        }
    ],
    # Note: don't pass in temperature or top_p to avoid exceptions in thinking mode
}

agent = ConversableAgent(
    name="test",
    llm_config=llm_config,
)

# Create the run
response = agent.run(
    message="Please provide a comparison of JS and TS",
    user_input=False,
    max_turns=1,
)

# Process the run
response.process()

# Print out the final response (thinking tokens are not shown)
print(response.summary)
```
 
