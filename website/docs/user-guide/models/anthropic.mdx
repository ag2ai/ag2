---
title: Anthropic
sidebarTitle: Anthropic
---

Anthropic's Claude is a family of large language models developed by Anthropic and designed to revolutionize the way you interact with AI. Claude excels at a wide variety of tasks involving language, reasoning, analysis, coding, and more. The models are highly capable, easy to use, and can be customized to suit your needs.

In this notebook, we demonstrate how to use Anthropic Claude model for AgentChat in AG2.

## Features

- Function/tool calling (including strict type enforcement with `strict: true` on supported models)
- Structured Outputs (guaranteed schema-compliant JSON output for supported models — [see this in a notebook example](/docs/use-cases/notebooks/notebooks/agentchat_anthropic_structured_outputs))
- Automatic fallback to "JSON Mode" for legacy Anthropic models
- Token usage and cost correctly as per Anthropic's API costs (as of December 2024)
- [Extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking?q=extended_thinking#pricing-and-token-usage-for-extended-thinking)

## Requirements
To use Anthropic Claude with AG2, install the `ag2[anthropic]` package.

To use tool/function calling or structured outputs (guaranteed/validated JSON responses or strict tool schema validation):

- Native structured outputs and strict tools REQUIRE **Claude Sonnet 4.5+ (model: `claude-sonnet-4-5` or later, or `claude-opus-4-1`)**, and **Anthropic SDK >= 0.74.1**. The special `structured-outputs-2025-11-13` beta header is managed automatically by AG2.
- If you use older Claude models (e.g., Haiku or Sonnet 4.0), AG2 will automatically fallback to "JSON Mode" (schema as part of the prompt, best-effort, not guaranteed).

```bash
# If you need to install AG2 with Anthropic
pip install ag2[anthropic]
```

<Tip>
If you have been using `autogen` or `ag2`, all you need to do is upgrade it using:
```bash
pip install -U autogen[anthropic]
```
or
```bash
pip install -U ag2[anthropic]
```
as `autogen` and `ag2` are aliases for the same PyPI package.
</Tip>

## Structured Outputs and Strict Tool Use (Native Mode)

Anthropic models Sonnet 4.5 and higher (and Opus 4.1+) provide native support for constrained decoding with structured outputs and strict schema validation for tool inputs.

**Key properties:**
- If your config contains a `response_format` (e.g., a Pydantic model or dict schema), and the model supports structured outputs, Anthropic will guarantee that the output matches your schema exactly—no more `JSON.parse()` errors or missing/incorrect types.
- For tool calling, setting `strict: true` in your tool/function definition will strictly validate all arguments according to your schema, enforcing types and enum values.
- If your model does **not** support these capabilities (e.g., Sonnet 4.0 or Haiku), AG2 will automatically fallback to JSON Mode: the prompt instructs the model to output JSON, and validates with best effort, but no strict guarantee.
- AG2 automatically detects model and SDK compatibility, choosing the safest and most precise execution.

### Example: Structured Output (`response_format`)
```python
from pydantic import BaseModel
import autogen
import os

class MathReasoning(BaseModel):
    steps: list[str]
    final_answer: str

llm_config = {
    "config_list": [
        {
            "model": "claude-sonnet-4-5",  # Must be 4-5+ for native structured outputs
            "api_key": os.environ["ANTHROPIC_API_KEY"],
            "api_type": "anthropic",
            "response_format": MathReasoning,  # AG2 configures this automatically
        }
    ],
}
agent = autogen.AssistantAgent("Assistant", llm_config=llm_config)
```
> For more details on defining the schema, combining tools and response formats, and fallback logic, see the [Structured Outputs notebook](/docs/use-cases/notebooks/notebooks/agentchat_anthropic_structured_outputs).

### Example: Strict Tools
```python
tool = {
    "name": "get_weather",
    "description": "Get the weather for a location",
    "strict": True,  # This enforces exact type and enum compliance!
    "parameters": {
        "type": "object",
        "properties": {
            "location": {"type": "string"},
            "unit": {"type": "string", "enum": ["celsius","fahrenheit"]}
        },
        "required": ["location"]
    }
}
llm_config = {
    "config_list": [...],
    "functions": [tool],
}
assistant = autogen.AssistantAgent("Assistant", llm_config=llm_config)
```
*Note: Only Sonnet 4.5/Opus 4.1+ and Anthropic SDK >=0.74.1 support strict mode. Older models will ignore `strict: true` or fallback to prompt-based (non-guaranteed) tool input.*

### Schema Limitations (Anthropic)
- Supported: All basic JSON types, enum, required, object, array, string formats
- **NOT supported**: Recursive schemas, minimum/maximum, minLength/maxLength, complex regex, or references (`$ref`)
- If you use a Pydantic model, AG2 will transform your schema for Anthropic as needed.

For a detailed walk-through, troubleshooting tips, and advanced usage, see the [notebook: Anthropic Structured Outputs](/docs/use-cases/notebooks/notebooks/agentchat_anthropic_structured_outputs) and [Anthropic's docs](https://docs.anthropic.com/en/docs/build-with-claude/structured-outputs).

## Set the config for the Anthropic API

You can add any parameters that are needed for the custom model loading in the same configuration list.

It is important to add the `api_type` field and set it to a string that corresponds to the client type used: `anthropic`.

Example:
```
[
    {
        "model": "claude-3-5-sonnet-20240620",
        "api_key": "your api key",
        "api_type": "anthropic",
    },
    {
        "model": "claude-3-7-sonnet-20250219",
        "api_key": "your api key",
        "api_type": "anthropic",
        "max_tokens": 8192, # override the default value of 4096, max tokens must be greater than thinking budget
        "timeout": 600, # for larger thinking budgets, increase the timeout OR enable streaming
        "thinking": {"type": "enabled", "budget_tokens": 2048},
    }
    {
        "model": "claude-3-sonnet-20240229",
        "api_key": "your api key",
        "api_type": "anthropic",
        "temperature": 0.5,
        "top_p": 0.2, # Note: It is recommended to set temperature or top_p but not both.
        "max_tokens": 10000,
    },
    {
        "model":"claude-3-opus-20240229",
        "api_key":"your api key",
        "api_type":"anthropic",
    },
    {
        "model":"claude-2.0",
        "api_key":"your api key",
        "api_type":"anthropic",
    },
    {
        "model":"claude-2.1",
        "api_key":"your api key",
        "api_type":"anthropic",
    },
    {
        "model":"claude-3.0-haiku",
        "api_key":"your api key",
        "api_type":"anthropic",
    },
]
```

### Alternative

As an alternative to the api_key key and value in the config, you can set the environment variable `ANTHROPIC_API_KEY` to your Anthropic API key.

Linux/Mac:
```
export ANTHROPIC_API_KEY="your Anthropic API key here"
```
Windows:
```
set ANTHROPIC_API_KEY=your_anthropic_api_key_here
```

```python
import os

from typing_extensions import Annotated

import autogen

llm_config_claude = autogen.LLMConfig(config_list={
    # Choose your model name.
    "model": "claude-3-5-sonnet-20240620",
    # You need to provide your API key here.
    "api_key": os.getenv("ANTHROPIC_API_KEY"),
    "api_type": "anthropic",
})
```

### Alternative Anthropic VertexAI Client (GCP)

To use the Anthropic VertexAI client in AG2, you need to configure it for use with Google Cloud Platform (GCP). Ensure you have the necessary project credentials and install the required package.

Configuration

The following configuration example demonstrates how to set up Anthropic VertexAI:

```python
import os

llm_config_vertexai = LLMConfig(config_list={
    "model": "claude-3-5-sonnet-20240620-v1:0",
    "gcp_project_id": "your_project_id",
    "gcp_region": "us-west-2",  # Replace with your GCP region
    "gcp_auth_token": None,  # Optional: If not passed, Google Default Authentication will be used
    "api_type": "anthropic",
})

assistant = autogen.AssistantAgent("assistant", llm_config=llm_config_vertexai)
```

### Alternative Anthropic VertexAI Client (Google Default Authentication)

If the `gcp_auth_token` is not provided in the configuration, the client will use Google’s default authentication mechanism. This requires the appropriate credentials to be configured in your environment, such as:

- Service account key: You can set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to point to your service account key file.
- Cloud Shell or GCP Compute Engine: When running in a GCP-managed environment, default authentication is automatically applied.


Example of setting up the environment variable:

```bash
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/your/service-account-key.json"
```

This allows seamless integration without explicitly specifying the authentication token in your code.

## Two-agent Coding Example

### Construct Agents

Construct a simple conversation between a User proxy and an ConversableAgent based on Claude-3 model.

```python
assistant = autogen.AssistantAgent("assistant", llm_config=llm_config_claude)

user_proxy = autogen.UserProxyAgent(
    "user_proxy",
    human_input_mode="NEVER",
    code_execution_config={
        "work_dir": "coding",
        "use_docker": False,
    },
    is_termination_msg=lambda x: x.get("content", "") and x.get("content", "").rstrip().endswith("TERMINATE"),
    max_consecutive_auto_reply=1,
)

user_proxy.initiate_chat(
    assistant, message="Write a python program to print the first 10 numbers of the Fibonacci sequence."
)
```
...
[**(Example transcript omitted for brevity; unchanged)**]
...

## Tool Call Example with the Latest Anthropic API
...
[**(Unchanged: example of tool calling)**]
...

## Group Chat Example with both Claude and GPT Agents
...
[**(Unchanged: multi-agent usage examples)**]
...

## Thinking mode

You can utilize Anthropic's thinking mode by setting it in the configuration.

... (section unchanged)

