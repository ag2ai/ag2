---
title: Amazon Bedrock
sidebarTitle: Amazon Bedrock
---

AG2 allows you to use Amazon's generative AI Bedrock service to run inference with a number of open-weight models and as well as their own models.

Amazon Bedrock supports models from providers such as Meta, Anthropic, Cohere, and Mistral.

In this notebook, we demonstrate how to use Anthropic's Sonnet model for AgentChat in AG2.

## Model features / support

Amazon Bedrock supports a wide range of models, not only for text generation but also for image classification and generation. Not all features are supported by AG2 or by the Converse API used. Please see [Amazon's documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html) on the features supported by the Converse API.

At this point in time AG2 supports text generation and image classification (passing images to the LLM).

## Requirements
To use Amazon Bedrock with AG2, first you need to install the `ag2[bedrock]` package.

## Pricing

When we combine the number of models supported and costs being on a per-region basis, it's not feasible to maintain the costs for each model+region combination within the AG2 implementation. Therefore, it's recommended that you add the following to your config with cost per 1,000 input and output tokens, respectively:
```
{
    ...
    "price": [0.003, 0.015]
    ...
}
```

Amazon Bedrock pricing is available [here](https://aws.amazon.com/bedrock/pricing/).


```bash
# If you need to install AG2 with Amazon Bedrock
pip install ag2[bedrock]
```

## Set the config for Amazon Bedrock

Amazon's Bedrock does not use the `api_key` as per other cloud inference providers for authentication, instead it uses a number of access, token, and profile values. These fields will need to be added to your client configuration. Please check the Amazon Bedrock documentation to determine which ones you will need to add.

The available parameters are:

- aws_region (mandatory)
- aws_access_key (or environment variable: AWS_ACCESS_KEY)
- aws_secret_key (or environment variable: AWS_SECRET_KEY)
- aws_session_token (or environment variable: AWS_SESSION_TOKEN)
- aws_profile_name

Beyond the authentication credentials, the only mandatory parameters are `api_type` and `model`.

The following parameters are common across all models used:

- temperature
- topP
- maxTokens

You can also include parameters specific to the model you are using (see the model detail within Amazon's documentation for more information), the four supported additional parameters are:

- top_p
- top_k
- k
- seed

An additional parameter can be added that denotes whether the model supports a system prompt (which is where the system messages are not included in the message list, but in a separate parameter). This defaults to `True`, so set it to `False` if your model (for example Mistral's Instruct models) [doesn't support this feature](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html):

- supports_system_prompts

It is important to add the `api_type` field and set it to a string that corresponds to the client type used: `bedrock`.

### Advanced: additionalModelRequestFields

Amazon Bedrock models often offer advanced, model-specific features that are not part of general AG2 config. AG2 now supports the `additionalModelRequestFields` parameter, which allows you to send these advanced options directly to Bedrock.

Typical use cases include (but are not limited to):
- Anthropic Claude's *thinking configuration* (enables internal reasoning phases)
- Model provider experimental or proprietary features

**How to use:**
- Add `"additionalModelRequestFields": { ... }` to the config/dictionary you pass to AG2, filling in with the provider/model's supported fields as needed.

For example, to enable Claude's 'thinking' mode:
```python
llm_config_bedrock = autogen.LLMConfig(config_list={
        "api_type": "bedrock",
        "model": "anthropic.claude-3-7-sonnet-20250219-v1:0",
        "aws_region": "us-east-1",
        "aws_access_key": "[FILL THIS IN]",
        "aws_secret_key": "[FILL THIS IN]",
        "additionalModelRequestFields": {
            # Claude thinking configuration example
            "thinking": {
                "type": "enabled",
                "budget_tokens": 1024,  # Must be < max_tokens
            },
        },
        "temperature": 0.1,
        "max_tokens": 4096,
    },
    cache_seed=None,  # turn off caching
)
```

Refer to [the Amazon Bedrock model documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html) to discover which features are supported by your chosen model. AG2 will pass all fields in `additionalModelRequestFields` through to Bedrock as-is; validation is up to the Bedrock API.

### Example:
```
[
    {
        "api_type": "bedrock",
        "model": "amazon.titan-text-premier-v1:0",
        "api_key": BEDROCK_API_KEY,
        "aws_region": "us-east-1"
        "aws_access_key": "",
        "aws_secret_key": "",
        "aws_session_token": "",
        "aws_profile_name": "",
    },
    {
        "api_type": "bedrock",
        "model": "anthropic.claude-3-sonnet-20240229-v1:0",
        "api_key": BEDROCK_API_KEY,
        "aws_region": "us-east-1"
        "aws_access_key": "",
        "aws_secret_key": "",
        "aws_session_token": "",
        "aws_profile_name": "",
        "temperature": 0.5,
        "topP": 0.2,
        "maxTokens": 250,
        "additionalModelRequestFields": {
          # Example: Enable thinking mode
          "thinking": {"type": "enabled", "budget_tokens": 1024},
        },
    },
    {
        "api_type": "bedrock",
        "model": "mistral.mixtral-8x7b-instruct-v0:1",
        "api_key": BEDROCK_API_KEY,
        "aws_region": "us-east-1"
        "aws_access_key": "",
        "aws_secret_key": "",
        "supports_system_prompts": False, # Mistral Instruct models don't support a separate system prompt
        "price": [0.00045, 0.0007] # Specific pricing for this model/region
    }
]
```

## Using within an AWS Lambda function

If you are using your AG2 code within an AWS Lambda function, you can utilise the attached role to access the Bedrock service and do not need to provide access, token, or profile values.

## Two-agent Coding Example

### Configuration

Start with our configuration - we'll use Anthropic's Sonnet model and put in recent pricing. Additionally, we'll reduce the temperature to 0.1 so its responses are less varied.

Then we'll construct a simple conversation between a User proxy and an ConversableAgent, which uses the Sonnet model.

```python
from typing_extensions import Annotated

import autogen

llm_config_bedrock = autogen.LLMConfig(config_list={
        "api_type": "bedrock",
        "model": "anthropic.claude-3-sonnet-20240229-v1:0",
        "api_key": BEDROCK_API_KEY,
        "aws_region": "us-east-1",
        "aws_access_key": "[FILL THIS IN]",
        "aws_secret_key": "[FILL THIS IN]",
        "price": [0.003, 0.015],
    },
    temperature=0.1,
    cache_seed=None,  # turn off caching
)

assistant = autogen.AssistantAgent("assistant", llm_config=llm_config_bedrock)

user_proxy = autogen.UserProxyAgent(
    "user_proxy",
    human_input_mode="NEVER",
    code_execution_config={
        "work_dir": "coding",
        "use_docker": False,
    },
    is_termination_msg=lambda x: x.get("content", "") and "TERMINATE" in x.get("content", ""),
    max_consecutive_auto_reply=1,
)

user_proxy.initiate_chat(
    assistant,
    message="Write a python program to print the first 10 numbers of the Fibonacci sequence. Just output the python code, no additional information.",
)
```
```console
user_proxy (to assistant):

Write a python program to print the first 10 numbers of the Fibonacci sequence. Just output the python code, no additional information.

--------------------------------------------------------------------------------
assistant (to user_proxy):

'''python
# Define a function to calculate Fibonacci sequence
def fibonacci(n):
    if n <= 0:
        return []
    elif n == 1:
        return [0]
    elif n == 2:
        return [0, 1]
    else:
        sequence = [0, 1]
        for i in range(2, n):
            sequence.append(sequence[i-1] + sequence[i-2])
        return sequence

# Call the function to get the first 10 Fibonacci numbers
fib_sequence = fibonacci(10)
print(fib_sequence)
'''

--------------------------------------------------------------------------------

>>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...
user_proxy (to assistant):

exitcode: 0 (execution succeeded)
Code output:
[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]


--------------------------------------------------------------------------------
assistant (to user_proxy):

Great, the code executed successfully and printed the first 10 numbers of the Fibonacci sequence correctly.

TERMINATE

--------------------------------------------------------------------------------
```

[...document continues unchanged as before...]
