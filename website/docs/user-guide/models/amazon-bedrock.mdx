---
title: Amazon Bedrock
sidebarTitle: Amazon Bedrock
---

AG2 allows you to use Amazon's generative AI Bedrock service to run inference with a number of open-weight models and as well as their own models.

Amazon Bedrock supports models from providers such as Meta, Anthropic, Cohere, and Mistral.

In this notebook, we demonstrate how to use Anthropic's Sonnet model for AgentChat in AG2.

## Model features / support

Amazon Bedrock supports a wide range of models, not only for text generation but also for image classification and generation. Not all features are supported by AG2 or by the Converse API used. Please see [Amazon's documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html) on the features supported by the Converse API.

At this point in time AG2 supports text generation and image classification (passing images to the LLM).

## Requirements
To use Amazon Bedrock with AG2, first you need to install the `ag2[bedrock]` package.

## Pricing

When we combine the number of models supported and costs being on a per-region basis, it's not feasible to maintain the costs for each model+region combination within the AG2 implementation. Therefore, it's recommended that you add the following to your config with cost per 1,000 input and output tokens, respectively:
```
{
    ...
    "price": [0.003, 0.015]
    ...
}
```

Amazon Bedrock pricing is available [here](https://aws.amazon.com/bedrock/pricing/).


```bash
# If you need to install AG2 with Amazon Bedrock
pip install ag2[bedrock]
```

## Choosing Between V1 and V2 Clients

AG2 provides two Bedrock client implementations:

- **V1 Client** (`api_type: "bedrock"`): Legacy client that returns flattened `ChatCompletion` responses
- **V2 Client** (`api_type: "bedrock_v2"`): **Recommended** - Next-generation client that returns rich `UnifiedResponse` objects

### Why Prefer V2 Client?

The V2 client (`bedrock_v2`) is the recommended choice for new projects and offers significant advantages:

#### 1. **Rich Content Preservation**
V2 preserves all response content as typed blocks, while V1 flattens responses:
- **V1**: Tool calls, images, and other rich content are flattened or lost
- **V2**: All content types (text, images, tool calls, citations) are preserved as typed `ContentBlock` objects

#### 2. **Direct Property Access**
V2 provides intuitive, direct access to response content:
```python
# V2 - Recommended modern usage
from autogen.llm_clients.bedrock_v2 import BedrockV2Client

client = BedrockV2Client(
    aws_region="us-east-1",
    aws_access_key="[FILL THIS IN]",
    aws_secret_key="[FILL THIS IN]",
)

response = client.create({
    "model": "anthropic.claude-3-sonnet-20240229-v1:0",
    "messages": [{"role": "user", "content": "Hello!"}],
})

print(response.text)              # All text content, directly accessible
print(response.model)             # Model used
print(response.provider)          # "bedrock"
print(response.cost)              # Calculated cost

# Typed content blocks:
for message in response.messages:
    for content_block in message.content:
        if content_block.type == "text":
            print(f"Text: {content_block.text}")
        elif content_block.type == "tool_call":
            print(f"Tool call: {content_block.name}")
        elif content_block.type == "image":
            print(f"Image (data URI or URL): {getattr(content_block, 'data_uri', getattr(content_block, 'image_url', ''))}")
```

#### 3. **Type Safety**
V2 uses Pydantic models with type validation:
- Typed content blocks (`TextContent`, `ImageContent`, `ToolCallContent`, etc.)
- Enum-based content types
- Automatic validation of response structure

#### 4. **Forward Compatibility**
V2 handles unknown content types gracefully:
- New Bedrock features are automatically supported via `GenericContent`
- No code changes needed when Bedrock adds new content types
- V1 may require manual parsing for new features

#### 5. **Provider-Agnostic Format**
V2 uses a unified response format across providers:
- Same interface for OpenAI, Anthropic, Gemini, and Bedrock
- Easier to switch between providers
- Consistent developer experience

#### 6. **Better Developer Experience**
```python
# V2 - Clean, intuitive API
response = client.create(params)
print(response.text)
print(response.usage)

for message in response.messages:
    for content_block in message.content:
        if content_block.type == "tool_call":
            print(f"Tool: {content_block.name}")
        elif content_block.type == "image":
            print(f"Image block available.")
```
#### 7. **Backward Compatible**
V2 maintains full compatibility with V1:
- Can use `create_v1_compatible()` to get V1 format when needed
- Works seamlessly with existing V1 clients in group chats
- No breaking changes to existing code

### When to Use V1 Client

Use V1 (`api_type: "bedrock"`) only if:
- You have existing code that depends on V1-specific behavior
- You're migrating gradually and need to maintain exact compatibility
- You don't need access to rich content types

### Migration Path

Migrating from V1 to V2 is simple - just change `api_type`:

```python
# V1 Configuration
{
    "api_type": "bedrock",
    "model": "anthropic.claude-3-sonnet-20240229-v1:0",
    "aws_region": "us-east-1",
    # ... other config
}

# V2 Configuration (just change api_type)
{
    "api_type": "bedrock_v2",  # <-- That's it!
    "model": "anthropic.claude-3-sonnet-20240229-v1:0",
    "aws_region": "us-east-1",
    # ... other config (same as V1)
}
```

## Set the config for Amazon Bedrock

Amazon's Bedrock does not use the `api_key` as per other cloud inference providers for authentication, instead it uses a number of access, token, and profile values. These fields will need to be added to your client configuration. Please check the Amazon Bedrock documentation to determine which ones you will need to add.

The available parameters are:

- aws_region (mandatory)
- aws_access_key (or environment variable: AWS_ACCESS_KEY)
- aws_secret_key (or environment variable: AWS_SECRET_KEY)
- aws_session_token (or environment variable: AWS_SESSION_TOKEN)
- aws_profile_name

Beyond the authentication credentials, the only mandatory parameters are `api_type` and `model`.

The following parameters are common across all models used:

- temperature
- topP
- maxTokens

You can also include parameters specific to the model you are using (see the model detail within Amazon's documentation for more information), the four supported additional parameters are:

- top_p
- top_k
- k
- seed

An additional parameter can be added that denotes whether the model supports a system prompt (which is where the system messages are not included in the message list, but in a separate parameter). This defaults to `True`, so set it to `False` if your model (for example Mistral's Instruct models) [doesn't support this feature](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-supported-models-features.html):

- supports_system_prompts

### Retry Configuration

AG2 supports configuring exponential backoff and retry behavior for Bedrock API calls. This helps handle transient errors, rate limits, and network issues gracefully. The following retry configuration parameters are available:

- **`total_max_attempts`** (int, optional, default: 5): Maximum number of total attempts (initial request + retries). This is the preferred parameter as it aligns with AWS environment variables. Example: `10` means 1 initial attempt + 9 retries = 10 total attempts.

- **`max_attempts`** (int, optional, default: 5): Legacy parameter for maximum number of retry attempts. If both `total_max_attempts` and `max_attempts` are provided, `total_max_attempts` takes precedence.

- **`mode`** (str, optional, default: "standard"): Retry strategy mode. Valid values are:
  - `"legacy"`: Pre-existing retry behavior
  - `"standard"`: Standardized retry rules (defaults to 3 max attempts if not overridden)
  - `"adaptive"`: Retries with additional client-side throttling (recommended for handling rate limits)

**Best Practices:**
- Use `total_max_attempts` instead of `max_attempts` (preferred parameter)
- Use `"adaptive"` mode for high-throughput scenarios or when dealing with rate limits
- Use `"standard"` mode for general-purpose applications
- Set `total_max_attempts` to 5-7 for most applications, or 10+ for critical applications

**Environment Variable Support:**
You can also configure retries via the `AWS_MAX_ATTEMPTS` environment variable, which maps to `total_max_attempts`.

### V1 Client Configuration

It is important to add the `api_type` field and set it to a string that corresponds to the client type used: `bedrock`.

Example:
```
[
    {
        "api_type": "bedrock",
        "model": "amazon.titan-text-premier-v1:0",
        "api_key": BEDROCK_API_KEY,
        "aws_region": "us-east-1",
        "aws_access_key": "",
        "aws_secret_key": "",
        "aws_session_token": "",
        "aws_profile_name": "",
    },
    {
        "api_type": "bedrock",
        "model": "anthropic.claude-3-sonnet-20240229-v1:0",
        "api_key": BEDROCK_API_KEY,
        "aws_region": "us-east-1",
        "aws_access_key": "",
        "aws_secret_key": "",
        "aws_session_token": "",
        "aws_profile_name": "",
        "temperature": 0.5,
        "topP": 0.2,
        "maxTokens": 250,
        "total_max_attempts": 5,  # Retry configuration
        "mode": "standard",
    },
    {
        "api_type": "bedrock",
        "model": "mistral.mixtral-8x7b-instruct-v0:1",
        "api_key": BEDROCK_API_KEY,
        "aws_region": "us-east-1",
        "aws_access_key": "",
        "aws_secret_key": "",
        "supports_system_prompts": False, # Mistral Instruct models don't support a separate system prompt
        "total_max_attempts": 8,  # More retries for reliability
        "mode": "adaptive",  # Adaptive mode for rate limit handling
        "price": [0.00045, 0.0007] # Specific pricing for this model/region
    }
]
```

### V2 Client Configuration (Recommended)

For V2 client, use `api_type: "bedrock_v2"`. All other configuration parameters remain the same:

```python
from autogen import LLMConfig

# V2 Client Configuration
llm_config_v2 = LLMConfig(config_list=[{
    "api_type": "bedrock_v2",  # Use bedrock_v2 for V2 client
    "model": "anthropic.claude-3-sonnet-20240229-v1:0",
    "aws_region": "us-east-1",
    "aws_access_key": "[FILL THIS IN]",
    "aws_secret_key": "[FILL THIS IN]",
    "temperature": 0.5,
    "topP": 0.2,
    "maxTokens": 250,
    "total_max_attempts": 5,
    "mode": "standard",
    "price": [0.003, 0.015],
}])
```

**Key Differences:**
- Change `api_type` from `"bedrock"` to `"bedrock_v2"`
- All other parameters remain identical
- Returns `UnifiedResponse` instead of `ChatCompletion`
- Provides access to rich content blocks

### Using V2 Client Directly

You can also use the V2 client directly for more control:

```python
from autogen.llm_clients.bedrock_v2 import BedrockV2Client

client = BedrockV2Client(
    aws_region="us-east-1",
    aws_access_key="[FILL THIS IN]",
    aws_secret_key="[FILL THIS IN]",
)

# Create a request
response = client.create({
    "model": "anthropic.claude-3-sonnet-20240229-v1:0",
    "messages": [{"role": "user", "content": "Hello!"}],
})

# Access rich content
print(response.text)  # Direct text access
print(response.model)  # Model used
print(response.provider)  # "bedrock"
print(response.cost)  # Calculated cost

# Access typed content blocks
for message in response.messages:
    for content_block in message.content:
        if content_block.type == "text":
            print(f"Text: {content_block.text}")
        elif content_block.type == "tool_use":
            print(f"Tool: {content_block.name}")
```

### Advanced: additionalModelRequestFields

Amazon Bedrock models often offer advanced, model-specific features that are not part of general AG2 config. AG2 now supports the `additional_model_request_fields` parameter, which allows you to send these advanced options directly to Bedrock.

Typical use cases include (but are not limited to):
- Anthropic Claude's *thinking configuration* (enables internal reasoning phases)
- Model provider experimental or proprietary features

**How to use:**
- Add `"additional_model_request_fields": { ... }` to the config/dictionary you pass to AG2, filling in with the provider/model's supported fields as needed.

For example, to enable Claude's 'thinking' mode:
```python
llm_config_bedrock = autogen.LLMConfig(config_list={
        "api_type": "bedrock",
        "model": "anthropic.claude-3-7-sonnet-20250219-v1:0",
        "aws_region": "us-east-1",
        "aws_access_key": "[FILL THIS IN]",
        "aws_secret_key": "[FILL THIS IN]",
        "additional_model_request_fields": {
            # Claude thinking configuration example
            "thinking": {
                "type": "enabled",
                "budget_tokens": 1024,  # Must be < max_tokens and above or equal to 1024 tokens
            },
        },
        "max_tokens": 4096,
    },
)
```

Refer to [the Amazon Bedrock model documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html) to discover which features are supported by your chosen model. AG2 will pass all fields in `additional_model_request_fields` through to Bedrock as-is;

## Using within an AWS Lambda function

If you are using your AG2 code within an AWS Lambda function, you can utilise the attached role to access the Bedrock service and do not need to provide access, token, or profile values.

## Retry Configuration Examples

### Basic Retry Configuration

The following example shows a basic configuration with default retry settings (5 total attempts, standard mode):

```python
llm_config_default = autogen.LLMConfig(config_list={
    "api_type": "bedrock",
    "model": "anthropic.claude-3-5-sonnet-20241022-v2:0",
    "aws_region": "us-east-1",
    "aws_access_key": "[FILL THIS IN]",
    "aws_secret_key": "[FILL THIS IN]",
    # Default: total_max_attempts=5, mode="standard"
})
```

### High-Reliability Configuration

For critical applications that need maximum retry attempts:

```python
llm_config_reliable = autogen.LLMConfig(config_list={
    "api_type": "bedrock",
    "model": "anthropic.claude-3-5-sonnet-20241022-v2:0",
    "aws_region": "us-east-1",
    "aws_access_key": "[FILL THIS IN]",
    "aws_secret_key": "[FILL THIS IN]",
    "total_max_attempts": 10,  # More retries for reliability
    "mode": "adaptive",  # Best for handling various error types
})
```

### Rate-Limit Optimized Configuration

For handling rate limits and throttling:

```python
llm_config_rate_limit = autogen.LLMConfig(config_list={
    "api_type": "bedrock",
    "model": "anthropic.claude-3-5-sonnet-20241022-v2:0",
    "aws_region": "us-east-1",
    "aws_access_key": "[FILL THIS IN]",
    "aws_secret_key": "[FILL THIS IN]",
    "total_max_attempts": 8,
    "mode": "adaptive",  # Best for rate limit handling
})
```

### Fast-Fail Configuration

For applications that need quick failure detection:

```python
llm_config_fast_fail = autogen.LLMConfig(config_list={
    "api_type": "bedrock",
    "model": "anthropic.claude-3-5-sonnet-20241022-v2:0",
    "aws_region": "us-east-1",
    "aws_access_key": "[FILL THIS IN]",
    "aws_secret_key": "[FILL THIS IN]",
    "total_max_attempts": 2,  # Minimal retries for fast failure
    "mode": "standard",
})
```

For more detailed information on retry configuration, see the [Exponential Backoff and Retry Configuration notebook](/notebook/agentchat_bedrock_client_exponential_backoff_and_retry_config).

## Two-agent Coding Example

### Configuration

Start with our configuration - we'll use Anthropic's Sonnet model and put in recent pricing. Additionally, we'll reduce the temperature to 0.1 so its responses are less varied.

Then we'll construct a simple conversation between a User proxy and an ConversableAgent, which uses the Sonnet model.

```python
from typing_extensions import Annotated

import autogen

llm_config_bedrock = autogen.LLMConfig(config_list={
        "api_type": "bedrock",
        "model": "anthropic.claude-3-sonnet-20240229-v1:0",
        "api_key": BEDROCK_API_KEY,
        "aws_region": "us-east-1",
        "aws_access_key": "[FILL THIS IN]",
        "aws_secret_key": "[FILL THIS IN]",
        "price": [0.003, 0.015],
    },
    temperature=0.1,
    cache_seed=None,  # turn off caching
)

assistant = autogen.AssistantAgent("assistant", llm_config=llm_config_bedrock)

user_proxy = autogen.UserProxyAgent(
    "user_proxy",
    human_input_mode="NEVER",
    code_execution_config={
        "work_dir": "coding",
        "use_docker": False,
    },
    is_termination_msg=lambda x: x.get("content", "") and "TERMINATE" in x.get("content", ""),
    max_consecutive_auto_reply=1,
)

user_proxy.initiate_chat(
    assistant,
    message="Write a python program to print the first 10 numbers of the Fibonacci sequence. Just output the python code, no additional information.",
)
```
```console
user_proxy (to assistant):

Write a python program to print the first 10 numbers of the Fibonacci sequence. Just output the python code, no additional information.

--------------------------------------------------------------------------------
assistant (to user_proxy):

'''python
# Define a function to calculate Fibonacci sequence
def fibonacci(n):
    if n <= 0:
        return []
    elif n == 1:
        return [0]
    elif n == 2:
        return [0, 1]
    else:
        sequence = [0, 1]
        for i in range(2, n):
            sequence.append(sequence[i-1] + sequence[i-2])
        return sequence

# Call the function to get the first 10 Fibonacci numbers
fib_sequence = fibonacci(10)
print(fib_sequence)
'''

--------------------------------------------------------------------------------

>>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...
user_proxy (to assistant):

exitcode: 0 (execution succeeded)
Code output:
[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]


--------------------------------------------------------------------------------
assistant (to user_proxy):

Great, the code executed successfully and printed the first 10 numbers of the Fibonacci sequence correctly.

TERMINATE

--------------------------------------------------------------------------------
```

## Tool Call Example

In this example, instead of writing code, we will show how we can perform multiple tool calling with Meta's Llama 3.1 70B model, where it recommends calling more than one tool at a time.

We'll use a simple travel agent assistant program where we have a couple of tools for weather and currency conversion.

...[rest is unchanged]... 
