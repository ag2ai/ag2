---
title: "A2A (Agent-to-Agent) Communication"
description: "Learn how to enable agents to communicate across different processes and machines using the A2A protocol."
---

# A2A (Agent-to-Agent) Communication

The A2A (Agent-to-Agent) communication system in AG2 enables agents to communicate across different processes, machines, or even different applications. This powerful feature allows you to build distributed agent systems where agents can be deployed as separate services and interact with each other seamlessly.

## What is A2A?

A2A is a protocol that allows AG2 agents to communicate over HTTP using a standardized interface. It enables:

- **Distributed Agent Systems**: Deploy agents as separate services
- **Cross-Platform Communication**: Agents can run on different machines or environments
- **Scalable Architecture**: Scale individual agents independently
- **Service Discovery**: Automatic discovery of agent capabilities through agent cards
- **Real-time Communication**: Support for both streaming and polling-based communication

## Key Components

### A2aAgentServer
The server component that exposes an AG2 agent as a web service. It handles incoming requests and executes the agent's logic.

### A2aRemoteAgent
The client component that connects to a remote A2A agent server. It acts as a proxy, forwarding messages to the remote agent and returning responses.

### Agent Cards
Metadata that describes an agent's capabilities, including:
- Name and description
- Supported input/output modes
- Available skills and tools
- Streaming capabilities
- Authentication requirements

## Quick Start

### 1. Create a Server Agent

```python
import os
import uvicorn
from autogen import ConversableAgent, LLMConfig
from autogen.a2a import A2aAgentServer

# Configure your LLM
llm_config = LLMConfig(
    model="gpt-4o-mini",
    api_key=os.getenv("OPENAI_API_KEY"),
)

# Create your agent
agent = ConversableAgent(
    name="python_coder",
    system_message="You are an expert Python developer...",
    llm_config=llm_config,
    human_input_mode="NEVER",
)

# Create A2A server
server = A2aAgentServer(
    agent,
    url="http://0.0.0.0:8000",
)

# Run the server
if __name__ == "__main__":
    uvicorn.run(server.build(), host="0.0.0.0", port=8000)
```

### 2. Create a Client Agent

```python
from autogen import ConversableAgent, LLMConfig
from autogen.a2a import A2aRemoteAgent

# Create a local agent
reviewer = ConversableAgent(
    name="code_reviewer",
    system_message="You are a code reviewer...",
    llm_config=llm_config,
)

# Create a remote agent connection
coder = A2aRemoteAgent(
    url="http://localhost:8000",
    name="python_coder",
)

# Use the remote agent
await reviewer.a_initiate_chat(
    recipient=coder,
    message={"role": "user", "content": "Create a calculator function"},
)
```

## Use Cases

- **Microservices Architecture**: Deploy specialized agents as independent services
- **Cross-Platform Integration**: Connect agents running in different environments
- **Load Distribution**: Distribute agent workloads across multiple machines
- **Service Composition**: Build complex workflows using multiple agent services
- **API Integration**: Expose agent capabilities as REST APIs
