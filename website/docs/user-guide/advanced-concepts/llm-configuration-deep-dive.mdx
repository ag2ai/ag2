---
title: LLM Config Deep-dive
---

In this deep-dive we run through the LLM configuration in depth, including advanced sourcing from environment variables and the (now secondary) `OAI_CONFIG_LIST` file.

# LLM Configuration

In AG2, agents use LLMs as key components to understand and react. To configure an agent's access to LLMs, you can specify an `llm_config` argument in its constructor. For example, the following snippet shows a configuration that uses `gpt-4o`:

```python
import os
from autogen import LLMConfig

llm_config = LLMConfig(config_list={"api_type": "openai", "model": "gpt-4o", "api_key": os.environ["OPENAI_API_KEY"]})
```

<Warning>
It is important to never commit secrets into your code, therefore we read the OpenAI API key from an environment variable.
</Warning>

This `llm_config` can then be passed to an agent's constructor to enable it to use the LLM.

```python
import autogen

assistant = autogen.AssistantAgent(name="assistant", llm_config=llm_config)
```

## Recommended Credential Sourcing: Environment Variables and Provider Discovery

AG2 now supports a modern approach to credential configuration: you should set credentials for popular providers (OpenAI, Azure, Gemini/Google, Anthropic, etc.) as _environment variables_ such as `OPENAI_API_KEY`, `AZURE_OPENAI_API_KEY`, `GEMINI_API_KEY`, etc. This is the **recommended and primary** approach. The system will discover and aggregate all available credentials, supporting multiple providers and tags automatically (see also the [setup guide](/docs/contributor-guide/setup-development-environment)).

Example:
```bash
export OPENAI_API_KEY="<your_openai_api_key>"
export AZURE_OPENAI_API_KEY="<your_azure_api_key>"
export AZURE_OPENAI_API_BASE="<your_azure_base_url>"
export GEMINI_API_KEY="<your_gemini_api_key>"
export ANTHROPIC_API_KEY="<your_anthropic_api_key>"
```

AG2 test and utility code will automatically find and combine these credentials, including tags and current model names supported by each provider. **This is now the preferred method.**

<Tip>
If both individual environment variables and OAI_CONFIG_LIST are provided, AG2 will prioritize environment variables and provider aggregation for credentials.
</Tip>

## OAI_CONFIG_LIST Pattern (Secondary/Legacy)

AG2 also supports the legacy pattern of specifying a `config_list` via the environment variable `OAI_CONFIG_LIST` in JSON format or using a JSON file.

A `config_list` is a list of dictionaries, each of which contains the following keys depending on the kind of endpoint being used:

<Tabs>
  <Tab title="OpenAI">
- `api_type` (str, required): The model provider
- `model` (str, required): The identifier of the model to be used, such as 'gpt-4o', 'gpt-4.1-mini'.
- `api_key` (str, optional): The API key required for authenticating requests to the model's API endpoint.
- `base_url` (str, optional): The base URL of the API endpoint. This is the root address where API calls are directed.
- `tags` (List[str], optional): Tags which can be used for filtering.

    Example:
    ```json
    [
      {
        "api_type": "openai",
        "model": "gpt-4o",
        "api_key": os.environ['OPENAI_API_KEY']
      }
    ]
    ```
  </Tab>
  <Tab title="Azure OpenAI">
- `model` (str, required): The deployment to be used. For current Azure usage, deployments like 'gpt-4.1-mini' (official replacement for retired gpt-3.5-turbo) or 'gpt-4o', according to your Azure setup.
- `api_key` (str, optional): The API key required for authenticating requests to the model's API endpoint.
- `api_type`: `azure`
- `base_url` (str, optional): The base URL of the API endpoint.
- `api_version` (str, optional): The version of the Azure API you wish to use.
- `tags` (List[str], optional): Tags for filtering.

    Example:
    ```json
    [
      {
        "model": "my-gpt-4.1-mini-deployment",
        "api_type": "azure",
        "api_key": os.environ['AZURE_OPENAI_API_KEY'],
        "base_url": "https://ENDPOINT.openai.azure.com/",
        "api_version": "2025-01-01"
      }
    ]
    ```
  </Tab>
  <Tab title="Other OpenAI compatible">
- `model` (str, required): The identifier of the model to be used, such as 'llama-7B'.
- `api_key` (str, optional): API key as needed.
- `base_url` (str, optional): API endpoint.
- `tags` (List[str], optional): Filtering tags.

    Example:
    ```json
    [
      {
        "api_type": "openai",
        "model": "llama-7B",
        "base_url": "http://localhost:1234"
      }
    ]
    ```
  </Tab>
</Tabs>

<Tip>
Custom (non-OpenAI) model clients are supported. See [the relevant guide](https://github.com/ag2ai/ag2/blob/main/notebook/agentchat_custom_model.ipynb).
</Tip>

### Using OAI_CONFIG_LIST

The config list can be loaded from an environment variable string or path to JSON file, as shown below:

```python
llm_config = autogen.LLMConfig.from_json(
    env="OAI_CONFIG_LIST",  # Or path="path/to/config.json"
)

# Then, create the assistant agent with the config
assistant = autogen.AssistantAgent(name="assistant", llm_config=llm_config)
```

This will, in order of priority:
- First, try to load the file from the path specified in the environment variable.
- Next, if no file is found, interpret the environment variable as a JSON string.
- Or, if provided as a path, open the file at that path directly.

### Why is it a list?

A list allows you to define multiple models/providers to be tried in order or to enable filtering by tags, as described below. Modern AG2 will also aggregate providers discovered from individual environment variables, if set.

- If one model times out or fails, the agent can try another model.
- Tags and filtering make switching between deployment types and model providers easier.

<Warning>
Make sure to use only currently-supported models. For instance, `gpt-35-turbo-instruct` is being retired and should be replaced with `gpt-4.1-mini` as per current best practices. See provider release notes for deprecation timelines.
</Warning>

### Model selection and config filtering

The agent chooses the first working model from the config list, trying others if there are failures (e.g., throttling). There is no implicit logic in core agents; some specialized agents may attempt more sophisticated model selection. Developers should ensure order and filtering matches their intentions.

The list can be filtered using a filter dictionary, for example:

```python
# Single model
filter_dict = {"model": "gpt-4o"}

# Multiple models
filter_dict = {"model": ["gpt-4o", "gpt-4.1-mini"]}
```

Used as follows:
```python
llm_config = autogen.LLMConfig(path="OAI_CONFIG_LIST").where(**filter_dict)
# Or
llm_config = autogen.LLMConfig.from_json(path="OAI_CONFIG_LIST").where(**filter_dict)
```

#### Tags

Config list entries can include tags (e.g., `gpt-4o`, `gpt-4.1-mini`, `llama`), making filtering easier across provider and deployment name differences.

```python
config_list = [
    {"api_type": "openai", "model": "my-gpt-4o-deployment", "api_key": "", "tags": ["gpt-4o", "openai"]},
    {"api_type": "openai", "model": "llama-7B", "base_url": "http://127.0.0.1:8080", "tags": ["llama", "local"]},
]
```

Filter by single or multiple tags:
```python
filter_dict = {"tags": "llama"}
llm_config = autogen.LLMConfig(*config_list).where(**filter_dict)

filter_dict = {"tags": ["llama", "gpt-4o"]}
llm_config = autogen.LLMConfig(*config_list).where(**filter_dict)
```

### Adding http client in llm_config for proxy

Custom http clients/proxying are supported, with the caveat that classes must support deepcopy:

```python
#!pip install httpx
import httpx

from autogen import LLMConfig

class MyHttpClient(httpx.Client):
    def __deepcopy__(self, memo):
        return self

llm_config = LLMConfig(config_list={
    "api_type": "openai",
    "model": "my-gpt-4o-deployment",
    "api_key": "",
}, http_client=MyHttpClient(proxy="http://localhost:8030"))
```

### Using Azure Active Directory (AAD) Authentication

Azure Active Directory (AAD) provides secure access to resources and applications. See provider docs and examples for up-to-date setup of AAD credentials including use of environment variables, client ID, and tenant ID.

#### Prerequisites & Steps
- See the detailed guide above for registering an AAD application, granting permissions, obtaining client/tenant IDs, and configuring your app.
- Make sure you configure AG2 to use the appropriate `llm_config` dictionary, setting `azure_ad_token_provider` as needed.

<details>
<summary>Example</summary>

```
from autogen import LLMConfig

llm_config = LLMConfig(config_list={
    "model": "gpt-4o",
    "base_url": "YOUR_BASE_URL",
    "api_type": "azure",
    "api_version": "2025-01-01",
    "max_tokens": 1000,
    "azure_ad_token_provider": "DEFAULT"
})
```
</details>

#### Example of Initializing an Assistant Agent with AAD Auth
```
import autogen
assistant = autogen.AssistantAgent(name="assistant", llm_config=llm_config)
```

#### Troubleshooting
- Double-check all environment variables, especially for provider credentials (OPENAI_API_KEY, AZURE_OPENAI_API_KEY, etc.),
- Ensure "model" points to a currently-supported model/deployment,
- If using OAI_CONFIG_LIST, verify structure and file/location.

## Other configuration parameters

Besides the `config_list`, you may configure:

### AG2-specific parameters

- `cache_seed` - Used for controlling LLM request caching. Typically should be left as default unless altering caching is intentional.

### Extra model client parameters

Any parameters supported by the client can be passed through, including `temperature`, `timeout`, etc. See [`OpenAI` client documentation](https://github.com/openai/openai-python/blob/d231d1fa783967c1d3a1db3ba1b52647fff148ac/src/openai/_client.py#L67) for options.

## Example

```python
from autogen import LLMConfig

llm_config = LLMConfig(
    {
        "model": "my-gpt-4o-deployment",
        "api_key": os.environ.get("AZURE_OPENAI_API_KEY"),
        "api_type": "azure",
        "base_url": os.environ.get("AZURE_OPENAI_API_BASE"),
        "api_version": "2024-02-01",
    },
    {
        "api_type": "openai",
        "model": "llama-7B",
        "base_url": "http://127.0.0.1:8080",
        "api_type": "openai",
    },
    temperature = 0.9,
    timeout = 300,
)
```

## Other helpers for loading a config list

- [`get_config_list`](/docs/api-reference/autogen/get_config_list): Generates configurations for API calls, primarily from provided API keys or modern environment variables.
- [`config_list_openai_aoai`](/docs/api-reference/autogen/config_list_openai_aoai): Supports loading from OpenAI and Azure endpoints using modern environment variable-based patterns.
- [`config_list_from_models`](/docs/api-reference/autogen/config_list_from_models): Useful for targeting groups of models according to supported deployment names.
- [`config_list_from_dotenv`](/docs/api-reference/autogen/config_list_from_dotenv): Loads from a `.env` file for credential management in local/dev environments.

See [this notebook](/docs/use-cases/notebooks/notebooks/config_loader_utility_functions) for examples on using these functions with both environment variables and OAI_CONFIG_LIST.

<Tip>
For the most reliable experience, use environment variables to set credentials for all LLM providers you wish to use. Only use OAI_CONFIG_LIST for legacy compatibility or special projects.
</Tip>
 
