---
title: Two-Agent Chat
---

## Two-Agent Chat and Chat Result

Two-agent chat is the simplest form of conversation pattern.
We start a two-agent chat using the `initiate_chat` method of every `ConversableAgent` agent.

The following figure illustrates how two-agent chat works.

![Two-agent chat](./assets/two-agent-chat.png)

A two-agent chats takes two inputs: a message, which is a string provided by the caller; a context, which specifies various parameters of the chat.

The sender agent uses its chat initializer method (i.e., `generate_init_message` method of `ConversableAgent`) to generate an initial message from the inputs, and sends it to the recipient agent to start the chat.

The sender agent is the agent whose `initiate_chat` method is called, and the recipient agent is the other agent.

Once the chat terminates, the history of the chat is processed by a chat summarizer. The summarizer summarizes the chat history and calculates the token usage of the chat. You can configure the type of summary using the `summary_method` parameter of the `initiate_chat` method. By default, it is the last message of the chat (i.e., `summary_method='last_msg'`).

The example below is a two-agent chat between a student agent and a teacher agent. Its summarizer uses an LLM-based summary.

![Two-agent Chat Process](./assets/two_agent_chat.png)


```python
import os
from dotenv import load_dotenv
from autogen import ConversableAgent, LLMConfig
from autogen.events.agent_events import InputRequestEvent, TerminationEvent, TextEvent

load_dotenv()

llm_config=LLMConfig(
    config_list={
        "api_type": "openai",
        "model": "gpt-5-nano",
        "api_key": os.environ["OPENAI_API_KEY"]
    }
)

student_agent = ConversableAgent(
    name="Student_Agent",
    system_message="You are a student willing to learn.",
    llm_config=llm_config
)

teacher_agent = ConversableAgent(
    name="Teacher_Agent",
    system_message="You are a math teacher.",
    llm_config=llm_config
)

chat_result = student_agent.run_iter(
    teacher_agent,
    message="What is triangle inequality?",
    summary_method="reflection_with_llm",
    max_turns=2,
    yield_on=[TextEvent, TerminationEvent],
)

import pprint
event_count = 0
for event in chat_result:
    event_count += 1
    print(f"\n[Event {event_count}] Event type: {event.type}")

    if isinstance(event, InputRequestEvent):
        user_input = input("  Input requested: ")
        event.content.respond(user_input)
        continue

    if hasattr(event, "content") and hasattr(event.content, "content"):
        content = str(event.content.content)
        pprint.pprint(content)
        print("-" * 80)

    if isinstance(event, TerminationEvent):
        print("Termination event received")
        break

print("Chat completed")
```

In the above example, the summary method is set to `reflection_with_llm` which takes a list of messages from the conversation and summarize them  using a call to an LLM.

The summary method first tries to use the recipient's LLM, if it is not available then it uses the sender's LLM. In this case the recipient is "Teacher_Agent" and the sender is "Student_Agent".

The input prompt for the LLM is the following default prompt:

```python
print(ConversableAgent.DEFAULT_SUMMARY_PROMPT)
```

You can also use a custom prompt by setting the `summary_prompt` argument of `initiate_chat`.

There are some other useful information in the `ChatResult` object, including the conversation history, human input, and token cost.

```python
# Get the chat history.
import pprint

pprint.pprint(chat_result.chat_history)
```
