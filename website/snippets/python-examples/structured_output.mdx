```python
import json

from pydantic import BaseModel

from autogen import ConversableAgent, LLMConfig

# 1. Define our lesson plan structure, a lesson with a number of objectives
class LearningObjective(BaseModel):
    title: str
    description: str

class LessonPlan(BaseModel):
    title: str
    learning_objectives: list[LearningObjective]
    script: str

# 2. Add our lesson plan structure to the LLM configuration
llm_config = LLMConfig(config_list={
    "api_type": "openai",
    "model": "gpt-5-nano",
    "response_format": LessonPlan,
})

# 3. The agent's system message doesn't need any formatting instructions
system_message = """You are a classroom lesson agent.
Given a topic, write a lesson plan for a fourth grade class.
"""

my_agent = ConversableAgent(
    name="lesson_agent",
    system_message=system_message,
    llm_config=llm_config,
)

# 4. Chat directly with our agent
response = my_agent.run(
    message="Let's learn about the solar system!",
    user_input=True,
    max_turns=1,
)

# 5. Iterate through the chat automatically with console output
response.process()

# 6. Get and print our lesson plan
lesson_plan_json = json.loads(response.messages[-1]["content"])
print(json.dumps(lesson_plan_json, indent=2))
```

#### Using AgentConfig for Agent-Specific Configuration (New)

AG2 now supports an official `AgentConfig` class (from `autogen.llm_config`) for advanced per-agent configuration, such as specifying structured output schemas or agent-level API settings. `AgentConfig` can be passed to agents via the `agent_config` argument to enable schema-based results and other features, and is often preferred for more complex use cases, including setting up structured outputs.

```python
import os
from autogen import AssistantAgent, UserProxyAgent, AgentConfig
from pydantic import BaseModel

# Example: Define a structured output schema for the assistant reply
class JokeOutputSchema(BaseModel):
    joke: str
    explanation: str

assistant_config = AgentConfig(
    response_format=JokeOutputSchema,
    api_type="openai",
)

assistant = AssistantAgent("assistant", agent_config=assistant_config)
user_proxy = UserProxyAgent("user_proxy", code_execution_config=False)

# Start the chat
user_proxy.initiate_chat(
    assistant,
    message="Tell me a joke about NVDA and TESLA stock prices.",
)
```

You can pass either `llm_config` (using `LLMConfig`) or `agent_config` (using `AgentConfig`) to agents, depending on your needs. Use `AgentConfig` when you want per-agent structured output or need to control how a particular agent interacts with the LLM API.
