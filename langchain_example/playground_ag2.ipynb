{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from mcp.types import (\n",
    "    CallToolResult,\n",
    "    EmbeddedResource,\n",
    "    ImageContent,\n",
    "    TextContent,\n",
    ")\n",
    "from mcp.types import (\n",
    "    Tool as MCPTool,\n",
    ")\n",
    "\n",
    "from autogen.agentchat import AssistantAgent, ConversableAgent\n",
    "from autogen.tools import Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "class MCPTool(Tool):\n",
    "    def __init__(\n",
    "        self, name: str, description: str, func: Callable[..., Any], parameters_json_schema: dict[str, Any]\n",
    "    ) -> None:\n",
    "        super().__init__(name=name, description=description, func_or_tool=func)\n",
    "        self._func_schema = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": name,\n",
    "                \"description\": description,\n",
    "                \"parameters\": parameters_json_schema,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def register_for_llm(self, agent: ConversableAgent) -> None:\n",
    "        \"\"\"Registers the tool with the ConversableAgent for use with a language model (LLM).\n",
    "\n",
    "        This method updates the agent's tool signature to include the function schema,\n",
    "        allowing the agent to invoke the tool correctly during interactions with the LLM.\n",
    "\n",
    "        Args:\n",
    "            agent (ConversableAgent): The agent with which the tool will be registered.\n",
    "        \"\"\"\n",
    "        agent.update_tool_signature(self._func_schema, is_remove=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit modified langchain_mcp_adapters\n",
    "# https://github.com/langchain-ai/langchain-mcp-adapters/blob/main/langchain_mcp_adapters/tools.py\n",
    "NonTextContent = ImageContent | EmbeddedResource\n",
    "\n",
    "\n",
    "def _convert_call_tool_result(\n",
    "    call_tool_result: CallToolResult,\n",
    ") -> tuple[str | list[str], list[NonTextContent] | None]:\n",
    "    text_contents: list[TextContent] = []\n",
    "    non_text_contents = []\n",
    "    for content in call_tool_result.content:\n",
    "        if isinstance(content, TextContent):\n",
    "            text_contents.append(content)\n",
    "        else:\n",
    "            non_text_contents.append(content)\n",
    "\n",
    "    tool_content: str | list[str] = [content.text for content in text_contents]\n",
    "    if len(text_contents) == 1:\n",
    "        tool_content = tool_content[0]\n",
    "\n",
    "    if call_tool_result.isError:\n",
    "        # raise ToolException(tool_content)\n",
    "        raise ValueError(f\"Tool call failed: {tool_content}\")\n",
    "\n",
    "    return tool_content, non_text_contents or None\n",
    "\n",
    "\n",
    "def convert_mcp_tool_to_ag2_tool(\n",
    "    session: ClientSession,\n",
    "    tool: MCPTool,\n",
    ") -> Tool:\n",
    "    \"\"\"Convert an MCP tool to a LangChain tool.\n",
    "\n",
    "    NOTE: this tool can be executed only in a context of an active MCP client session.\n",
    "\n",
    "    Args:\n",
    "        session: MCP client session\n",
    "        tool: MCP tool to convert\n",
    "\n",
    "    Returns:\n",
    "        a LangChain tool\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Input schema: {tool.inputSchema}\")\n",
    "\n",
    "    async def call_tool(\n",
    "        **arguments: dict[str, Any],\n",
    "    ) -> tuple[str | list[str], list[NonTextContent] | None]:\n",
    "        print(f\"Arguments: {arguments}\")\n",
    "\n",
    "        call_tool_result = await session.call_tool(tool.name, arguments)\n",
    "        return _convert_call_tool_result(call_tool_result)\n",
    "\n",
    "    # ag2_tool = Tool(\n",
    "    #     name=tool.name,\n",
    "    #     description=tool.description or \"\",\n",
    "    #     func_or_tool=call_tool,\n",
    "    # )\n",
    "\n",
    "    # return ag2_tool\n",
    "\n",
    "    mcp_tool = MCPTool(\n",
    "        name=tool.name,\n",
    "        description=tool.description or \"\",\n",
    "        func=call_tool,\n",
    "        parameters_json_schema=tool.inputSchema,\n",
    "    )\n",
    "    return mcp_tool\n",
    "\n",
    "\n",
    "async def load_mcp_tools(session: ClientSession) -> list[Tool]:\n",
    "    \"\"\"Load all available MCP tools and convert them to LangChain tools.\"\"\"\n",
    "    tools = await session.list_tools()\n",
    "    return [convert_mcp_tool_to_ag2_tool(session, tool) for tool in tools.tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    # Make sure to update to the full absolute path to your math_server.py file\n",
    "    args=[\"math_server.py\"],\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # Initialize the connection\n",
    "        await session.initialize()\n",
    "\n",
    "        # Get tools\n",
    "        tools = await load_mcp_tools(session)\n",
    "        print(f\"Len tools: {len(tools)}\")\n",
    "        print(f\"Langchain tools 0: {tools[0].name}\")\n",
    "        print(f\"Langchain tools 1: {tools[1].name}\")\n",
    "\n",
    "        f_result = await tools[0].func(a=2, b=5)\n",
    "        print(f\"Result from Langchain tool 0: {f_result}\")\n",
    "\n",
    "        agent = AssistantAgent(name=\"assistant\", llm_config={\"model\": \"gpt-4o-mini\", \"api_type\": \"openai\"})\n",
    "        for tool in tools:\n",
    "            tool.register_for_llm(agent)\n",
    "\n",
    "        for tool_schema in agent.llm_config[\"tools\"]:\n",
    "            print(f\"Tool schema: {tool_schema}\")\n",
    "        agent.run(\n",
    "            message=\"Add 123223 and 456789\",\n",
    "            tools=tools,\n",
    "            max_turns=2,\n",
    "            user_input=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
