{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Optional\n",
    "\n",
    "import nest_asyncio\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import autogen\n",
    "from autogen import ConversableAgent\n",
    "from autogen.agentchat.user_proxy_agent import UserProxyAgent\n",
    "from autogen.agents.experimental import WebSurferAgent\n",
    "from autogen.tools.dependency_injection import BaseContext, Depends\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subtask(BaseModel):\n",
    "    question: str\n",
    "    answer: Optional[str] = None\n",
    "\n",
    "\n",
    "class Task(BaseContext, BaseModel):\n",
    "    in_progress_subtask: Optional[Subtask] = None\n",
    "    subtasks: List[Subtask]\n",
    "    answer: Optional[str] = None\n",
    "\n",
    "\n",
    "task_context: Task = Task(subtasks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\"tags\": [\"gpt-4o-mini\"]},\n",
    ")\n",
    "llm_config = {\"config_list\": config_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_analyser = ConversableAgent(\n",
    "    name=\"InitialAnalysisAgent\",\n",
    "    system_message=(\n",
    "        \"You are an expert at breaking down complex questions into smaller, focused subquestions.\\n\"\n",
    "        \"Your task is to take any question provided and divide it into clear, actionable subquestions that can be individually answered.\\n\"\n",
    "        \"Ensure the subquestions are logical, non-redundant, and cover all key aspects of the original question.\\n\"\n",
    "        \"Avoid providing answers or interpretationsâ€”focus solely on decomposition.\\n\"\n",
    "        \"Do NOT forward the initial task without breaking it down into subquestions!!!\\n\"\n",
    "    ),\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "\n",
    "@initial_analyser.register_for_llm(\n",
    "    description=\"Decompose the question into subtasks with the maximum of two subtasks. Web surfing will be done based on each subtask independently.\"\n",
    ")\n",
    "def decompose_question(\n",
    "    subtasks: Annotated[\n",
    "        list[str], \"Each subtask should be related to the original question and aim to achieve partial answering of it\"\n",
    "    ],\n",
    "    task_context: Annotated[Task, Depends(task_context)],\n",
    ") -> None:\n",
    "    task_context.subtasks.extend([Subtask(question=st) for st in subtasks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websurfer = WebSurferAgent(\n",
    "    llm_config=llm_config,\n",
    "    name=\"WebSurferAgent\",\n",
    "    system_message=(\n",
    "        \"You are a web surfer agent responsible for gathering information from the web to answer the subquestions provided.\\n\"\n",
    "        \"Always perform searches one subquestion at a time. If you get First get the subquestion, then perform the search and then call the answer function.\\n\"\n",
    "        \"The process to repeat the following steps until all subquestions are answered:\\n\"\n",
    "        \"1. Get the next subquestion - 'get_next_subtask'\\n\"\n",
    "        \"2. Search the web for the answer - 'browser_use'\\n\"\n",
    "        \"3. Answer the subquestion - 'answer_subtask'\\n\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "NO_MORE_SUBTASKS = \"No more subtasks, proceed to the next agent\"\n",
    "\n",
    "\n",
    "@websurfer.register_for_llm(description=\"Get next subtask\")\n",
    "def get_next_subtask(task_context: Annotated[Task, Depends(task_context)]) -> str:\n",
    "    if task_context.in_progress_subtask:\n",
    "        return (\n",
    "            f\"Subtask already in progress, first answer previous subtask: {task_context.in_progress_subtask.question}\"\n",
    "        )\n",
    "\n",
    "    unanswered_subtasks = [st for st in task_context.subtasks if st.answer is None]\n",
    "\n",
    "    if not unanswered_subtasks:\n",
    "        return NO_MORE_SUBTASKS\n",
    "\n",
    "    task_context.in_progress_subtask = unanswered_subtasks[0]\n",
    "\n",
    "    return task_context.in_progress_subtask.question\n",
    "\n",
    "\n",
    "@websurfer.register_for_llm(description=\"Answer the subtask\")\n",
    "def answer_subtask(answer: str, task_context: Annotated[Task, Depends(task_context)]) -> str:\n",
    "    if not task_context.in_progress_subtask:\n",
    "        return \"No subtask in progress, first get a subtask\"\n",
    "\n",
    "    task_context.in_progress_subtask.answer = answer\n",
    "    task_context.in_progress_subtask = None\n",
    "\n",
    "    return \"Subtask answered successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_agent = ConversableAgent(\n",
    "    name=\"SummaryAgent\",\n",
    "    system_message=(\n",
    "        \"You are a summarizer agent responsible for summarizing the answers to the subquestions provided.\"\n",
    "        \"Your task is to provide a concise summary of the answers to the subquestions.\"\n",
    "        \"Ensure the summary is coherent, logically structured, and covers all key aspects of the original question.\"\n",
    "    ),\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "\n",
    "@summary_agent.register_for_llm(description=\"Request more information\")\n",
    "def request_more_information(request_explanation: str) -> str:\n",
    "    return request_explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    system_message=\"A proxy to excute code\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    function_map={\n",
    "        answer_subtask.name: answer_subtask,\n",
    "        get_next_subtask.name: get_next_subtask,\n",
    "        decompose_question.name: decompose_question,\n",
    "        request_more_information.name: request_more_information,\n",
    "        websurfer.tool.name: websurfer.tool.func,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_transition(last_speaker, groupchat):\n",
    "    messages = groupchat.messages\n",
    "\n",
    "    # always start with the user\n",
    "    if len(messages) <= 1:\n",
    "        return initial_analyser\n",
    "\n",
    "    print(messages[-1])\n",
    "\n",
    "    # if the last message is a tool call, return the tool_execution agent\n",
    "    if \"tool_calls\" in messages[-1]:\n",
    "        return user_proxy\n",
    "\n",
    "    if last_speaker is user_proxy:\n",
    "        penultimate_speaker = groupchat.agent_by_name(name=messages[-2].get(\"name\", \"\"))\n",
    "\n",
    "        if penultimate_speaker is initial_analyser:\n",
    "            return websurfer\n",
    "\n",
    "        if penultimate_speaker is summary_agent:\n",
    "            return initial_analyser\n",
    "\n",
    "        if messages[-1].get(\"content\", \"\").startswith(NO_MORE_SUBTASKS):\n",
    "            return summary_agent\n",
    "\n",
    "        return groupchat.agent_by_name(name=messages[-2].get(\"name\", \"\"))\n",
    "\n",
    "    if last_speaker is initial_analyser:\n",
    "        return websurfer\n",
    "\n",
    "    if last_speaker is websurfer:\n",
    "        return summary_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[initial_analyser, websurfer, summary_agent, user_proxy],\n",
    "    messages=[],\n",
    "    speaker_selection_method=state_transition,\n",
    "    max_round=1000,\n",
    ")\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "chat_result = user_proxy.initiate_chat(manager, message=\"Who are the founders of AG2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-browser-use",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
