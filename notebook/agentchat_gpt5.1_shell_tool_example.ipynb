{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Shell Tool with OpenAI Responses API\n",
    "\n",
    "Author: [Priyanshu Deshmukh](https://github.com/priyansh4320)\n",
    "\n",
    "This notebook demonstrates how to use the shell tool with OpenAI's Responses API. The shell tool allows models to execute shell commands through your integration, enabling them to interact with your local computer through a controlled command-line interface.\n",
    "\n",
    "**Warning: Running arbitrary shell commands can be dangerous. Always sandbox execution or add strict allow-/deny-lists before forwarding commands to the system shell in production.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Install AG2 and dependencies\n",
    "\n",
    "To be able to run this notebook, you will need to install AG2 with the `openai` extra.\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Install `ag2` with 'openai' extra:\n",
    "\n",
    "pip install ag2[openai]\n",
    "```\n",
    "\n",
    "For more information, please refer to the [installation guide](https://docs.ag2.ai/latest/docs/user-guide/basic-concepts/installing-ag2).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's configure the LLM with the Responses API and enable the shell tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent, LLMConfig\n",
    "\n",
    "# Configure the LLM with Responses API and shell tool\n",
    "llm_config = LLMConfig(\n",
    "    config_list={\n",
    "        \"api_type\": \"responses\",\n",
    "        \"model\": \"gpt-5.1\",\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "        \"built_in_tools\": [\"shell\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create the assistant agent\n",
    "assistant = ConversableAgent(\n",
    "    name=\"Assistant\",\n",
    "    system_message=\"\"\"You are a helpful assistant with access to shell commands.\n",
    "    You can use the shell tool to execute commands and interact with the filesystem.\n",
    "    The local shell environment is on Mac/Linux.\n",
    "    Keep your responses concise and include command output when helpful.\n",
    "    \"\"\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Create a user proxy agent\n",
    "user_proxy = ConversableAgent(\n",
    "    name=\"UserProxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Example 1: Automating Filesystem Diagnostics\n",
    "\n",
    "The shell tool is perfect for automating filesystem or process diagnostics. In this example, we'll find the largest PDF file in a directory and show running processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Find the largest PDF and show processes\n",
    "result = assistant.initiate_chat(\n",
    "    recipient=user_proxy,\n",
    "    message=\"\"\"\n",
    "    Please help me with the following tasks:\n",
    "    1. Find the largest PDF file in the current directory (or create a test directory with PDFs)\n",
    "    2. Show me information about running Python processes\n",
    "    \"\"\",\n",
    "    max_turns=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Example 2: Extending Model Capabilities with UNIX Utilities\n",
    "\n",
    "The shell tool extends the model's capabilities by allowing it to use built-in UNIX utilities, Python runtime, and other CLIs in your environment. This enables the model to perform tasks that require system-level operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Use UNIX utilities and Python CLI\n",
    "result = assistant.initiate_chat(\n",
    "    recipient=user_proxy,\n",
    "    message=\"\"\"\n",
    "    Please help me:\n",
    "    1. Check the current Python version using the python CLI\n",
    "    2. Get system information like disk usage and memory\n",
    "    3. Create a simple text file and then use grep to search within it\n",
    "    \"\"\",\n",
    "    max_turns=6,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Chat Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Example 3: Multi-Step Build and Test Flows\n",
    "\n",
    "The shell tool excels at running multi-step build and test flows, chaining commands together to complete complex workflows. In this example, we'll set up a Python project, install dependencies, and run tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Multi-step build and test flow\n",
    "result = assistant.initiate_chat(\n",
    "    recipient=user_proxy,\n",
    "    message=\"\"\"\n",
    "    Please help me set up a simple Python project:\n",
    "    1. Create a directory called 'test_project'\n",
    "    2. Create a simple Python module with a function to test\n",
    "    3. Create a test file using pytest format\n",
    "    4. Install pytest if needed\n",
    "    5. Run the tests and show me the results\n",
    "    \"\"\",\n",
    "    max_turns=8,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Chat Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- The shell tool executes commands immediately when `shell_call` items are generated by the model\n",
    "- Each `shell_call` can contain multiple commands that are executed concurrently\n",
    "- Commands support timeouts and output length limits\n",
    "- Always be cautious when executing shell commands, especially in production environments\n",
    "- Consider implementing sandboxing or command allow-lists for security"
   ]
  }
 ],
 "metadata": {
  "front_matter": {
   "description": "Demonstrates how to use the shell tool with OpenAI's Responses API for filesystem diagnostics, extending capabilities with UNIX utilities, and running multi-step build and test flows.",
   "tags": [
    "responses",
    "shell",
    "filesystem",
    "automation",
    "cli"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
