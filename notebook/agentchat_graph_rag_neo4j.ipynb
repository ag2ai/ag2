{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Neo4jGraphRagCapability with agents for GraphRAG Question & Answering\n",
    "\n",
    "AG2 provides GraphRAG integration using agent capabilities. This is an example to integrate Neo4j (a Property/Knowledge Graph database).\n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "llama-index dependencies, which is required to use Neo4j prpoerty graph\n",
    "\n",
    "```bash\n",
    "pip llama-index llama-index-graph-stores-neo4j\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configuration and OpenAI API Key\n",
    "\n",
    "By default, in order to use FalkorDB you need to have an OpenAI key in your environment variable `OPENAI_API_KEY`.\n",
    "\n",
    "You can utilise an OAI_CONFIG_LIST file and extract the OpenAI API key and put it in the environment, as will be shown in the following cell.\n",
    "\n",
    "Alternatively, you can load the environment variable yourself.\n",
    "\n",
    "````{=mdx}\n",
    ":::tip\n",
    "Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\", file_location=\"../\")\n",
    "\n",
    "# Put the OpenAI API key into the environment\n",
    "os.environ[\"OPENAI_API_KEY\"] = config_list[0][\"api_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to allow nested asyncio calls for Neo4j in Jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Information: Using Neo4j with OpenAI Models ðŸš€\n",
    "\n",
    "> **Important**  \n",
    "> - **Default Models**:\n",
    ">   - **Question Answering**: OpenAI's `GPT-3.5-turbo` with `temperature=0.0`.\n",
    ">   - **Embedding**: OpenAI's `text-embedding-3-small`.\n",
    "> \n",
    "> - **Customization**:\n",
    ">   You can change these defaults by setting the following parameters on the `Neo4jGraphQueryEngine`:\n",
    ">   - `model`: Specify a different LLM model.\n",
    ">   - `temperature`: Specify a different temperature.\n",
    ">   - `embed_model`: Specify a different embedding model.\n",
    "\n",
    "### Additional Notes\n",
    "If you see an **Assertion error**, simply rerun the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Knowledge Graph with Your Own Data\n",
    "\n",
    "**Note:** You need to have a Neo4j database running. If you are running one in a Docker container, please ensure your Docker network is setup to allow access to it. \n",
    "\n",
    "In this example, the Neo4j endpoint is set to host=\"bolt://172.17.0.4\" and port=7687, please adjust accordingly. For how to spin up a Neo4j with Docker, you can refer to [this](https://docs.llamaindex.ai/en/stable/examples/property_graph/property_graph_neo4j/#:~:text=stores%2Dneo4j-,Docker%20Setup,%C2%B6,-To%20launch%20Neo4j)\n",
    "\n",
    "Below, we have some sample data from Paul Grahma's [essay](https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt).\n",
    "\n",
    "We then initialise the database with that text document, creating the graph in Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Example\n",
    "\n",
    "In this example, the graph schema is auto-generated. This allows you to load data without specifying the specific types of entities and relationships that will make up the database (however, this may not be optimal and not cost efficient). \n",
    "First, we create a Neo4j property graph with Paul Grahma's essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: \"MATCH (n:`Chunk`)\\nWITH collect(distinct substring(toString(coalesce(n.`doc_id`, '')), 0, 50)) AS `doc_id_values`,\\n     collect(distinct substring(toString(coalesce(n.`document_id`, '')), 0, 50)) AS `document_id_values`,\\n     collect(distinct substring(toString(coalesce(n.`creation_date`, '')), 0, 50)) AS `creation_date_values`,\\n     collect(distinct substring(toString(coalesce(n.`_node_type`, '')), 0, 50)) AS `_node_type_values`,\\n     collect(distinct substring(toString(coalesce(n.`file_type`, '')), 0, 50)) AS `file_type_values`,\\n     collect(distinct substring(toString(coalesce(n.`last_modified_date`, '')), 0, 50)) AS `last_modified_date_values`,\\n     collect(distinct substring(toString(coalesce(n.`file_name`, '')), 0, 50)) AS `file_name_values`,\\n     collect(distinct substring(toString(coalesce(n.`_node_content`, '')), 0, 50)) AS `_node_content_values`,\\n     collect(distinct substring(toString(coalesce(n.`ref_doc_id`, '')), 0, 50)) AS `ref_doc_id_values`,\\n     min(size(coalesce(n.`embedding`, []))) AS `embedding_size_min`, max(size(coalesce(n.`embedding`, []))) AS `embedding_size_max`,\\n     collect(distinct substring(toString(coalesce(n.`text`, '')), 0, 50)) AS `text_values`,\\n     collect(distinct substring(toString(coalesce(n.`file_path`, '')), 0, 50)) AS `file_path_values`,\\n     min(n.`file_size`) AS `file_size_min`,\\n     max(n.`file_size`) AS `file_size_max`,\\n     count(distinct n.`file_size`) AS `file_size_distinct`,\\n     collect(distinct substring(toString(coalesce(n.`id`, '')), 0, 50)) AS `id_values`\\nRETURN {`doc_id`: {values:`doc_id_values`[..10], distinct_count: size(`doc_id_values`)}, `document_id`: {values:`document_id_values`[..10], distinct_count: size(`document_id_values`)}, `creation_date`: {values:`creation_date_values`[..10], distinct_count: size(`creation_date_values`)}, `_node_type`: {values:`_node_type_values`[..10], distinct_count: size(`_node_type_values`)}, `file_type`: {values:`file_type_values`[..10], distinct_count: size(`file_type_values`)}, `last_modified_date`: {values:`last_modified_date_values`[..10], distinct_count: size(`last_modified_date_values`)}, `file_name`: {values:`file_name_values`[..10], distinct_count: size(`file_name_values`)}, `_node_content`: {values:`_node_content_values`[..10], distinct_count: size(`_node_content_values`)}, `ref_doc_id`: {values:`ref_doc_id_values`[..10], distinct_count: size(`ref_doc_id_values`)}, `embedding`: {min_size: `embedding_size_min`, max_size: `embedding_size_max`}, `text`: {values:`text_values`[..10], distinct_count: size(`text_values`)}, `file_path`: {values:`file_path_values`[..10], distinct_count: size(`file_path_values`)}, `file_size`: {min: toString(`file_size_min`), max: toString(`file_size_max`), distinct_count: `file_size_distinct`}, `id`: {values:`id_values`[..10], distinct_count: size(`id_values`)}} AS output\"\n",
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.39it/s]\n",
      "Extracting paths from text with schema: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:42<00:00,  1.93s/it]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: \"MATCH (n:`Chunk`)\\nWITH collect(distinct substring(toString(coalesce(n.`doc_id`, '')), 0, 50)) AS `doc_id_values`,\\n     collect(distinct substring(toString(coalesce(n.`document_id`, '')), 0, 50)) AS `document_id_values`,\\n     collect(distinct substring(toString(coalesce(n.`creation_date`, '')), 0, 50)) AS `creation_date_values`,\\n     collect(distinct substring(toString(coalesce(n.`_node_type`, '')), 0, 50)) AS `_node_type_values`,\\n     collect(distinct substring(toString(coalesce(n.`file_type`, '')), 0, 50)) AS `file_type_values`,\\n     collect(distinct substring(toString(coalesce(n.`last_modified_date`, '')), 0, 50)) AS `last_modified_date_values`,\\n     collect(distinct substring(toString(coalesce(n.`file_name`, '')), 0, 50)) AS `file_name_values`,\\n     collect(distinct substring(toString(coalesce(n.`_node_content`, '')), 0, 50)) AS `_node_content_values`,\\n     collect(distinct substring(toString(coalesce(n.`ref_doc_id`, '')), 0, 50)) AS `ref_doc_id_values`,\\n     min(size(coalesce(n.`embedding`, []))) AS `embedding_size_min`, max(size(coalesce(n.`embedding`, []))) AS `embedding_size_max`,\\n     collect(distinct substring(toString(coalesce(n.`text`, '')), 0, 50)) AS `text_values`,\\n     collect(distinct substring(toString(coalesce(n.`file_path`, '')), 0, 50)) AS `file_path_values`,\\n     min(n.`file_size`) AS `file_size_min`,\\n     max(n.`file_size`) AS `file_size_max`,\\n     count(distinct n.`file_size`) AS `file_size_distinct`,\\n     collect(distinct substring(toString(coalesce(n.`id`, '')), 0, 50)) AS `id_values`\\nRETURN {`doc_id`: {values:`doc_id_values`[..10], distinct_count: size(`doc_id_values`)}, `document_id`: {values:`document_id_values`[..10], distinct_count: size(`document_id_values`)}, `creation_date`: {values:`creation_date_values`[..10], distinct_count: size(`creation_date_values`)}, `_node_type`: {values:`_node_type_values`[..10], distinct_count: size(`_node_type_values`)}, `file_type`: {values:`file_type_values`[..10], distinct_count: size(`file_type_values`)}, `last_modified_date`: {values:`last_modified_date_values`[..10], distinct_count: size(`last_modified_date_values`)}, `file_name`: {values:`file_name_values`[..10], distinct_count: size(`file_name_values`)}, `_node_content`: {values:`_node_content_values`[..10], distinct_count: size(`_node_content_values`)}, `ref_doc_id`: {values:`ref_doc_id_values`[..10], distinct_count: size(`ref_doc_id_values`)}, `embedding`: {min_size: `embedding_size_min`, max_size: `embedding_size_max`}, `text`: {values:`text_values`[..10], distinct_count: size(`text_values`)}, `file_path`: {values:`file_path_values`[..10], distinct_count: size(`file_path_values`)}, `file_size`: {min: toString(`file_size_min`), max: toString(`file_size_max`), distinct_count: `file_size_distinct`}, `id`: {values:`id_values`[..10], distinct_count: size(`id_values`)}} AS output\"\n"
     ]
    }
   ],
   "source": [
    "from autogen import ConversableAgent, UserProxyAgent\n",
    "from autogen.agentchat.contrib.graph_rag.document import Document, DocumentType\n",
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_query_engine import Neo4jGraphQueryEngine\n",
    "\n",
    "# Auto generate graph schema from unstructured data\n",
    "input_path = \"../test/agentchat/contrib/graph_rag/paul_graham_essay.txt\"\n",
    "input_documents = [Document(doctype=DocumentType.TEXT, path_or_url=input_path)]\n",
    "\n",
    "# Create FalkorGraphQueryEngine\n",
    "query_engine = Neo4jGraphQueryEngine(\n",
    "    username=\"neo4j\",  # Change if you reset username\n",
    "    password=\"password\",  # Change if you reset password\n",
    "    host=\"bolt://172.17.0.4\",  # Change\n",
    "    port=7687,  # if needed\n",
    "    database=\"neo4j\",  # Change if you want to store the graphh in your custom database\n",
    ")\n",
    "\n",
    "# Ingest data and initialize the database\n",
    "query_engine.init_db(input_doc=input_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add capability to a ConversableAgent and query them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "What happened at Interleaf and Viaweb?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "At Interleaf, a scripting language inspired by Emacs was added, with the scripting language being a dialect of Lisp. At Viaweb, there was a code editor for users to define their own page styles, which involved editing Lisp expressions underneath. Additionally, Viaweb had to recruit an initial set of users privately before launching publicly to ensure they had decent-looking stores.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "How does paul graham do at Interleaf\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Paul Graham did not perform well at Interleaf. He mentioned that he was a bad employee, as he did not understand most of the software due to his lack of knowledge in C and his reluctance to learn it. Additionally, he admitted to being terribly irresponsible during his time at Interleaf.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "What's his relation to Viaweb? Did he do anything there?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Paul Graham worked on Viaweb and had a code editor implemented within the platform.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "What did Paul gramham do for KFC?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Paul Graham did not work for KFC.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'What happened at Interleaf and Viaweb?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'At Interleaf, a scripting language inspired by Emacs was added, with the scripting language being a dialect of Lisp. At Viaweb, there was a code editor for users to define their own page styles, which involved editing Lisp expressions underneath. Additionally, Viaweb had to recruit an initial set of users privately before launching publicly to ensure they had decent-looking stores.', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'How does paul graham do at Interleaf', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Paul Graham did not perform well at Interleaf. He mentioned that he was a bad employee, as he did not understand most of the software due to his lack of knowledge in C and his reluctance to learn it. Additionally, he admitted to being terribly irresponsible during his time at Interleaf.', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': \"What's his relation to Viaweb? Did he do anything there?\", 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Paul Graham worked on Viaweb and had a code editor implemented within the platform.', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'What did Paul gramham do for KFC?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Paul Graham did not work for KFC.', 'role': 'user', 'name': 'paul_graham_agent'}], summary='Paul Graham did not work for KFC.', cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=['How does paul graham do at Interleaf', \"What's his relation to Viaweb? Did he do anything there?\", 'What did Paul gramham do for KFC?', 'exit'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_rag_capability import Neo4jGraphCapability\n",
    "\n",
    "# Create a ConversableAgent (no LLM configuration)\n",
    "graph_rag_agent = ConversableAgent(\n",
    "    name=\"paul_graham_agent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Associate the capability with the agent\n",
    "graph_rag_capability = Neo4jGraphCapability(query_engine)\n",
    "graph_rag_capability.add_to_agent(graph_rag_agent)\n",
    "\n",
    "# Create a user proxy agent to converse with our RAG agent\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(graph_rag_agent, message=\"What happened at Interleaf and Viaweb?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit the example by defining custom entities, relations and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: \"MATCH (n:`Chunk`)\\nWITH collect(distinct substring(toString(coalesce(n.`doc_id`, '')), 0, 50)) AS `doc_id_values`,\\n     collect(distinct substring(toString(coalesce(n.`document_id`, '')), 0, 50)) AS `document_id_values`,\\n     collect(distinct substring(toString(coalesce(n.`creation_date`, '')), 0, 50)) AS `creation_date_values`,\\n     collect(distinct substring(toString(coalesce(n.`_node_type`, '')), 0, 50)) AS `_node_type_values`,\\n     collect(distinct substring(toString(coalesce(n.`file_type`, '')), 0, 50)) AS `file_type_values`,\\n     collect(distinct substring(toString(coalesce(n.`last_modified_date`, '')), 0, 50)) AS `last_modified_date_values`,\\n     collect(distinct substring(toString(coalesce(n.`file_name`, '')), 0, 50)) AS `file_name_values`,\\n     collect(distinct substring(toString(coalesce(n.`_node_content`, '')), 0, 50)) AS `_node_content_values`,\\n     collect(distinct substring(toString(coalesce(n.`ref_doc_id`, '')), 0, 50)) AS `ref_doc_id_values`,\\n     min(size(coalesce(n.`embedding`, []))) AS `embedding_size_min`, max(size(coalesce(n.`embedding`, []))) AS `embedding_size_max`,\\n     collect(distinct substring(toString(coalesce(n.`text`, '')), 0, 50)) AS `text_values`,\\n     collect(distinct substring(toString(coalesce(n.`file_path`, '')), 0, 50)) AS `file_path_values`,\\n     min(n.`file_size`) AS `file_size_min`,\\n     max(n.`file_size`) AS `file_size_max`,\\n     count(distinct n.`file_size`) AS `file_size_distinct`,\\n     collect(distinct substring(toString(coalesce(n.`id`, '')), 0, 50)) AS `id_values`\\nRETURN {`doc_id`: {values:`doc_id_values`[..10], distinct_count: size(`doc_id_values`)}, `document_id`: {values:`document_id_values`[..10], distinct_count: size(`document_id_values`)}, `creation_date`: {values:`creation_date_values`[..10], distinct_count: size(`creation_date_values`)}, `_node_type`: {values:`_node_type_values`[..10], distinct_count: size(`_node_type_values`)}, `file_type`: {values:`file_type_values`[..10], distinct_count: size(`file_type_values`)}, `last_modified_date`: {values:`last_modified_date_values`[..10], distinct_count: size(`last_modified_date_values`)}, `file_name`: {values:`file_name_values`[..10], distinct_count: size(`file_name_values`)}, `_node_content`: {values:`_node_content_values`[..10], distinct_count: size(`_node_content_values`)}, `ref_doc_id`: {values:`ref_doc_id_values`[..10], distinct_count: size(`ref_doc_id_values`)}, `embedding`: {min_size: `embedding_size_min`, max_size: `embedding_size_max`}, `text`: {values:`text_values`[..10], distinct_count: size(`text_values`)}, `file_path`: {values:`file_path_values`[..10], distinct_count: size(`file_path_values`)}, `file_size`: {min: toString(`file_size_min`), max: toString(`file_size_max`), distinct_count: `file_size_distinct`}, `id`: {values:`id_values`[..10], distinct_count: size(`id_values`)}} AS output\"\n",
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 19.52it/s]\n",
      "Extracting paths from text with schema: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:21<00:00,  1.03it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.10it/s]\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: \"MATCH (n:`Chunk`)\\nWITH collect(distinct substring(toString(coalesce(n.`doc_id`, '')), 0, 50)) AS `doc_id_values`,\\n     collect(distinct substring(toString(coalesce(n.`document_id`, '')), 0, 50)) AS `document_id_values`,\\n     collect(distinct substring(toString(coalesce(n.`creation_date`, '')), 0, 50)) AS `creation_date_values`,\\n     collect(distinct substring(toString(coalesce(n.`_node_type`, '')), 0, 50)) AS `_node_type_values`,\\n     collect(distinct substring(toString(coalesce(n.`file_type`, '')), 0, 50)) AS `file_type_values`,\\n     collect(distinct substring(toString(coalesce(n.`last_modified_date`, '')), 0, 50)) AS `last_modified_date_values`,\\n     collect(distinct substring(toString(coalesce(n.`file_name`, '')), 0, 50)) AS `file_name_values`,\\n     collect(distinct substring(toString(coalesce(n.`_node_content`, '')), 0, 50)) AS `_node_content_values`,\\n     collect(distinct substring(toString(coalesce(n.`ref_doc_id`, '')), 0, 50)) AS `ref_doc_id_values`,\\n     min(size(coalesce(n.`embedding`, []))) AS `embedding_size_min`, max(size(coalesce(n.`embedding`, []))) AS `embedding_size_max`,\\n     collect(distinct substring(toString(coalesce(n.`text`, '')), 0, 50)) AS `text_values`,\\n     collect(distinct substring(toString(coalesce(n.`file_path`, '')), 0, 50)) AS `file_path_values`,\\n     min(n.`file_size`) AS `file_size_min`,\\n     max(n.`file_size`) AS `file_size_max`,\\n     count(distinct n.`file_size`) AS `file_size_distinct`,\\n     collect(distinct substring(toString(coalesce(n.`id`, '')), 0, 50)) AS `id_values`\\nRETURN {`doc_id`: {values:`doc_id_values`[..10], distinct_count: size(`doc_id_values`)}, `document_id`: {values:`document_id_values`[..10], distinct_count: size(`document_id_values`)}, `creation_date`: {values:`creation_date_values`[..10], distinct_count: size(`creation_date_values`)}, `_node_type`: {values:`_node_type_values`[..10], distinct_count: size(`_node_type_values`)}, `file_type`: {values:`file_type_values`[..10], distinct_count: size(`file_type_values`)}, `last_modified_date`: {values:`last_modified_date_values`[..10], distinct_count: size(`last_modified_date_values`)}, `file_name`: {values:`file_name_values`[..10], distinct_count: size(`file_name_values`)}, `_node_content`: {values:`_node_content_values`[..10], distinct_count: size(`_node_content_values`)}, `ref_doc_id`: {values:`ref_doc_id_values`[..10], distinct_count: size(`ref_doc_id_values`)}, `embedding`: {min_size: `embedding_size_min`, max_size: `embedding_size_max`}, `text`: {values:`text_values`[..10], distinct_count: size(`text_values`)}, `file_path`: {values:`file_path_values`[..10], distinct_count: size(`file_path_values`)}, `file_size`: {min: toString(`file_size_min`), max: toString(`file_size_max`), distinct_count: `file_size_distinct`}, `id`: {values:`id_values`[..10], distinct_count: size(`id_values`)}} AS output\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from autogen import ConversableAgent, UserProxyAgent\n",
    "from autogen.agentchat.contrib.graph_rag.document import Document, DocumentType\n",
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_query_engine import Neo4jGraphQueryEngine\n",
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_rag_capability import Neo4jGraphCapability\n",
    "\n",
    "# Auto generate graph schema from unstructured data\n",
    "input_path = \"../test/agentchat/contrib/graph_rag/paul_graham_essay.txt\"\n",
    "input_documents = [Document(doctype=DocumentType.TEXT, path_or_url=input_path)]\n",
    "\n",
    "\n",
    "# best practice to use upper-case\n",
    "entities = Literal[\"PERSON\", \"PLACE\", \"ORGANIZATION\"]  #\n",
    "relations = Literal[\"HAS\", \"PART_OF\", \"WORKED_ON\", \"WORKED_WITH\", \"WORKED_AT\"]\n",
    "\n",
    "# define which entities can have which relations\n",
    "validation_schema = {\n",
    "    \"PERSON\": [\"HAS\", \"PART_OF\", \"WORKED_ON\", \"WORKED_WITH\", \"WORKED_AT\"],\n",
    "    \"PLACE\": [\"HAS\", \"PART_OF\", \"WORKED_AT\"],\n",
    "    \"ORGANIZATION\": [\"HAS\", \"PART_OF\", \"WORKED_WITH\"],\n",
    "}\n",
    "\n",
    "# Create FalkorGraphQueryEngine\n",
    "query_engine = Neo4jGraphQueryEngine(\n",
    "    username=\"neo4j\",  # Change if you reset username\n",
    "    password=\"password\",  # Change if you reset password\n",
    "    host=\"bolt://172.17.0.4\",  # Change\n",
    "    port=7687,  # if needed\n",
    "    database=\"neo4j\",  # Change if you want to store the graphh in your custom database\n",
    "    entities=entities,  # possible entities\n",
    "    relations=relations,  # possible relations\n",
    "    validation_schema=validation_schema,  # schema to validate the extracted triplets\n",
    "    strict=True,  # enofrce the extracted triplets to be in the schema\n",
    ")\n",
    "\n",
    "# Ingest data and initialize the database\n",
    "query_engine.init_db(input_doc=input_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add capability to a ConversableAgent and query them again\n",
    "You should find the answers conform to your custom schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "What happened at Interleaf and Viaweb?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Paul Graham worked at Interleaf where he learned valuable lessons about technology companies. Inspired by Emacs, Interleaf added a scripting language that was a dialect of Lisp. Paul Graham was not a fan of the working environment at Interleaf and felt worn out from the stress of running Viaweb. Viaweb, a company he co-founded, was an online store builder that aimed to make users' stores look professional. Viaweb had a code editor for users to define their page styles, which ran Lisp expressions underneath. Eventually, Viaweb was acquired by Yahoo in the summer of 1998, providing relief to Paul Graham.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "what happened at Interleaf?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Interleaf added a scripting language inspired by Emacs, making it a dialect of Lisp. They were looking for a Lisp hacker to write things in it. The experience at Interleaf taught valuable lessons, such as the importance of product-oriented leadership in technology companies, the risks of editing code by too many people, the drawbacks of cheap but depressing office space, the superiority of corridor conversations over planned meetings, and the mismatch between conventional office hours and optimal hacking times. Additionally, it was learned that being the \"entry level\" option is advantageous as the low end tends to dominate the high end, and prestige can be a warning sign.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "How does paul graham do at Interleaf\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Paul Graham did not perform well at Interleaf.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "What's his relation to Viaweb? Did he do anything there?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Paul Graham worked on Viaweb and was involved in various aspects of the company such as having a code editor, working with other individuals like Julian, Merchants, Yahoo, and Dan Giffin, being part of Y Combinator, and working at Viaweb.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "who worked at Interleaf?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Paul Graham worked at Interleaf.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "Does Paul gramham like eating burgers?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "There is no information provided in the context about Paul Graham's preference for eating burgers.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'What happened at Interleaf and Viaweb?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': \"Paul Graham worked at Interleaf where he learned valuable lessons about technology companies. Inspired by Emacs, Interleaf added a scripting language that was a dialect of Lisp. Paul Graham was not a fan of the working environment at Interleaf and felt worn out from the stress of running Viaweb. Viaweb, a company he co-founded, was an online store builder that aimed to make users' stores look professional. Viaweb had a code editor for users to define their page styles, which ran Lisp expressions underneath. Eventually, Viaweb was acquired by Yahoo in the summer of 1998, providing relief to Paul Graham.\", 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'what happened at Interleaf?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Interleaf added a scripting language inspired by Emacs, making it a dialect of Lisp. They were looking for a Lisp hacker to write things in it. The experience at Interleaf taught valuable lessons, such as the importance of product-oriented leadership in technology companies, the risks of editing code by too many people, the drawbacks of cheap but depressing office space, the superiority of corridor conversations over planned meetings, and the mismatch between conventional office hours and optimal hacking times. Additionally, it was learned that being the \"entry level\" option is advantageous as the low end tends to dominate the high end, and prestige can be a warning sign.', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'How does paul graham do at Interleaf', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Paul Graham did not perform well at Interleaf.', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': \"What's his relation to Viaweb? Did he do anything there?\", 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Paul Graham worked on Viaweb and was involved in various aspects of the company such as having a code editor, working with other individuals like Julian, Merchants, Yahoo, and Dan Giffin, being part of Y Combinator, and working at Viaweb.', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'who worked at Interleaf?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Paul Graham worked at Interleaf.', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'Does Paul gramham like eating burgers?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': \"There is no information provided in the context about Paul Graham's preference for eating burgers.\", 'role': 'user', 'name': 'paul_graham_agent'}], summary=\"There is no information provided in the context about Paul Graham's preference for eating burgers.\", cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=['what happened at Interleaf?', 'How does paul graham do at Interleaf', \"What's his relation to Viaweb? Did he do anything there?\", 'who worked at Interleaf?', 'Does Paul gramham like eating burgers?', 'exit'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_rag_capability import Neo4jGraphCapability\n",
    "\n",
    "# Create a ConversableAgent (no LLM configuration)\n",
    "graph_rag_agent = ConversableAgent(\n",
    "    name=\"paul_graham_agent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Associate the capability with the agent\n",
    "graph_rag_capability = Neo4jGraphCapability(query_engine)\n",
    "graph_rag_capability.add_to_agent(graph_rag_agent)\n",
    "\n",
    "# Create a user proxy agent to converse with our RAG agent\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(graph_rag_agent, message=\"What happened at Interleaf and Viaweb?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
