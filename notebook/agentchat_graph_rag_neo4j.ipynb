{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Neo4jGraphRagCapability with agents for GraphRAG Question & Answering\n",
    "\n",
    "AG2 provides GraphRAG integration using agent capabilities. This is an example to integrate Neo4j (a Property/Knowledge Graph database).\n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "llama-index dependencies, which is required to use Neo4j prpoerty graph\n",
    "\n",
    "```bash\n",
    "pip install llama-index==0.11.8 llama-index-graph-stores-neo4j==0.3.0 llama-index-core==0.11.8\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Configuration and OpenAI API Key\n",
    "\n",
    "By default, in order to use FalkorDB you need to have an OpenAI key in your environment variable `OPENAI_API_KEY`.\n",
    "\n",
    "You can utilise an OAI_CONFIG_LIST file and extract the OpenAI API key and put it in the environment, as will be shown in the following cell.\n",
    "\n",
    "Alternatively, you can load the environment variable yourself.\n",
    "\n",
    "````{=mdx}\n",
    ":::tip\n",
    "Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\", file_location=\"../\")\n",
    "\n",
    "# Put the OpenAI API key into the environment\n",
    "os.environ[\"OPENAI_API_KEY\"] = config_list[0][\"api_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to allow nested asyncio calls for Neo4j in Jupyter\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Information: Using Neo4j with LLM Models ðŸš€\n",
    "\n",
    "> **Important**  \n",
    "> - **Default Models**:\n",
    ">   - **Question Answering**: OpenAI's `GPT-3.5-turbo` with `temperature=0.0`.\n",
    ">   - **Embedding**: OpenAI's `text-embedding-3-small`.\n",
    "> \n",
    "> - **Customization**:\n",
    ">   You can change these defaults by setting the following parameters on the `Neo4jGraphQueryEngine`:\n",
    ">   - `llm`: Specify a LLM instance with a llm you like.\n",
    ">   - `embedding`: Specify a BaseEmbedding instance with a embedding model.\n",
    "> - **Refer to**\n",
    ">   - https://docs.llamaindex.ai/en/stable/module_guides/models/llms/\n",
    ">   - https://docs.llamaindex.ai/en/stable/examples/property_graph/graph_store/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Knowledge Graph with Your Own Data\n",
    "\n",
    "**Note:** You need to have a Neo4j database running. If you are running one in a Docker container, please ensure your Docker network is setup to allow access to it. \n",
    "\n",
    "In this example, the Neo4j endpoint is set to host=\"bolt://172.17.0.4\" and port=7687, please adjust accordingly. For how to spin up a Neo4j with Docker, you can refer to [this](https://docs.llamaindex.ai/en/stable/examples/property_graph/property_graph_neo4j/#:~:text=stores%2Dneo4j-,Docker%20Setup,%C2%B6,-To%20launch%20Neo4j)\n",
    "\n",
    "Below, we have some sample data from Paul Grahma's [essay](https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt).\n",
    "\n",
    "We then initialise the database with that text document, creating the graph in Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Example\n",
    "\n",
    "In this example, the graph schema is auto-generated. This allows you to load data without specifying the specific types of entities and relationships that will make up the database (however, this may not be optimal and not cost efficient). \n",
    "First, we create a Neo4j property graph (knowledge graph) with Paul Grahma's essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n",
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 15.78it/s]\n",
      "Extracting paths from text with schema:   0%|          | 0/22 [01:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     12\u001b[0m query_engine \u001b[38;5;241m=\u001b[39m Neo4jGraphQueryEngine(\n\u001b[1;32m     13\u001b[0m     username\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneo4j\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Change if you reset username\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Change if you reset password\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     database\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneo4j\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Change if you want to store the graphh in your custom database\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Ingest data and initialize the database\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_doc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_documents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ag2/autogen/agentchat/contrib/graph_rag/neo4j_graph_query_engine.py:113\u001b[0m, in \u001b[0;36mNeo4jGraphQueryEngine.init_db\u001b[0;34m(self, input_doc)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Extract paths following a strict schema of allowed entities, relationships, and which entities can be connected to which relationships.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# To add more extractors, please refer to https://docs.llamaindex.ai/en/latest/module_guides/indexing/lpg_index_guide/#construction\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkg_extractors \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    104\u001b[0m     SchemaLLMPathExtractor(\n\u001b[1;32m    105\u001b[0m         llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    111\u001b[0m ]\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m \u001b[43mPropertyGraphIndex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkg_extractors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkg_extractors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperty_graph_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/indices/base.py:119\u001b[0m, in \u001b[0;36mBaseIndex.from_documents\u001b[0;34m(cls, documents, storage_context, show_progress, callback_manager, transformations, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     docstore\u001b[38;5;241m.\u001b[39mset_document_hash(doc\u001b[38;5;241m.\u001b[39mget_doc_id(), doc\u001b[38;5;241m.\u001b[39mhash)\n\u001b[1;32m    112\u001b[0m nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    113\u001b[0m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     transformations,\n\u001b[1;32m    115\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    117\u001b[0m )\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/indices/property_graph/base.py:134\u001b[0m, in \u001b[0;36mPropertyGraphIndex.__init__\u001b[0;34m(self, nodes, llm, kg_extractors, property_graph_store, vector_store, use_async, embed_model, embed_kg_nodes, callback_manager, transformations, storage_context, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_kg_nodes \u001b[38;5;241m=\u001b[39m embed_kg_nodes\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_override_vector_store \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    130\u001b[0m     vector_store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m storage_context\u001b[38;5;241m.\u001b[39mproperty_graph_store\u001b[38;5;241m.\u001b[39msupports_vector_queries\n\u001b[1;32m    132\u001b[0m )\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/indices/base.py:77\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[0;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m---> 77\u001b[0m     index_struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct \u001b[38;5;241m=\u001b[39m index_struct\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage_context\u001b[38;5;241m.\u001b[39mindex_store\u001b[38;5;241m.\u001b[39madd_index_struct(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_struct)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/indices/base.py:185\u001b[0m, in \u001b[0;36mBaseIndex.build_index_from_nodes\u001b[0;34m(self, nodes, **build_kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build the index from nodes.\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_docstore\u001b[38;5;241m.\u001b[39madd_documents(nodes, allow_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuild_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/indices/property_graph/base.py:334\u001b[0m, in \u001b[0;36mPropertyGraphIndex._build_index_from_nodes\u001b[0;34m(self, nodes, **build_kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_index_from_nodes\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m, nodes: Optional[Sequence[BaseNode]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbuild_kwargs: Any\n\u001b[1;32m    332\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IndexLPG:\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build index from nodes.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_insert_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# this isn't really used or needed\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IndexLPG()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/indices/property_graph/base.py:200\u001b[0m, in \u001b[0;36mPropertyGraphIndex._insert_nodes\u001b[0;34m(self, nodes)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# run transformations on nodes to extract triplets\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async:\n\u001b[0;32m--> 200\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43marun_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kg_extractors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_progress\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m run_transformations(\n\u001b[1;32m    207\u001b[0m         nodes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kg_extractors, show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_progress\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/selectors.py:468\u001b[0m, in \u001b[0;36mEpollSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 468\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout, max_ev)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from autogen import ConversableAgent, UserProxyAgent\n",
    "from autogen.agentchat.contrib.graph_rag.document import Document, DocumentType\n",
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_query_engine import Neo4jGraphQueryEngine\n",
    "\n",
    "# Auto generate graph schema from unstructured data\n",
    "input_path = \"../test/agentchat/contrib/graph_rag/paul_graham_essay.txt\"\n",
    "input_documents = [Document(doctype=DocumentType.TEXT, path_or_url=input_path)]\n",
    "\n",
    "# Create FalkorGraphQueryEngine\n",
    "query_engine = Neo4jGraphQueryEngine(\n",
    "    username=\"neo4j\",  # Change if you reset username\n",
    "    password=\"password\",  # Change if you reset password\n",
    "    host=\"bolt://172.17.0.2\",  # Change\n",
    "    port=7687,  # if needed\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.0),  # Default, no need to specify\n",
    "    embedding=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),  # except you want to use a different model\n",
    "    database=\"neo4j\",  # Change if you want to store the graphh in your custom database\n",
    ")\n",
    "\n",
    "# Ingest data and initialize the database\n",
    "query_engine.init_db(input_doc=input_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add capability to a ConversableAgent and query them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "What happened at Interleaf and Viaweb?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Interleaf was a company that had smart people and built impressive technology but got crushed by Moore's Law in the 1990s. On the other hand, Viaweb was a company founded by the author and his partner to put art galleries online. However, they realized that art galleries didn't want to be online, and they pivoted to building online stores instead. They developed software to generate web stores and transitioned to creating web apps that allowed users to control the software through a browser, leading to the establishment of Viaweb as a pioneering web application company.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "What did Paul Graham do at Interleaf\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Paul Graham did freelance Lisp hacking work at Interleaf.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "Did he work well?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "He demonstrated a strong work ethic and a willingness to explore various projects and opportunities, which ultimately led to successful endeavors such as writing essays, developing spam filters, investing in startups, and founding an investment firm.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "Did he work well at Interleaf?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "He was nervous about money at Interleaf as he sensed the company was on the way down. He decided to write another book on Lisp to secure his financial situation. Ultimately, he chose to drop out of RISD and pursue other opportunities.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "Did Paul Graham like eating burger?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "There is no information provided in the context about Paul Graham's preference for eating burgers.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'What happened at Interleaf and Viaweb?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': \"Interleaf was a company that had smart people and built impressive technology but got crushed by Moore's Law in the 1990s. On the other hand, Viaweb was a company founded by the author and his partner to put art galleries online. However, they realized that art galleries didn't want to be online, and they pivoted to building online stores instead. They developed software to generate web stores and transitioned to creating web apps that allowed users to control the software through a browser, leading to the establishment of Viaweb as a pioneering web application company.\", 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'What did Paul Graham do at Interleaf', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Paul Graham did freelance Lisp hacking work at Interleaf.', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'Did he work well?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'He demonstrated a strong work ethic and a willingness to explore various projects and opportunities, which ultimately led to successful endeavors such as writing essays, developing spam filters, investing in startups, and founding an investment firm.', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'Did he work well at Interleaf?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'He was nervous about money at Interleaf as he sensed the company was on the way down. He decided to write another book on Lisp to secure his financial situation. Ultimately, he chose to drop out of RISD and pursue other opportunities.', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'Did Paul Graham like eating burger?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': \"There is no information provided in the context about Paul Graham's preference for eating burgers.\", 'role': 'user', 'name': 'paul_graham_agent'}], summary=\"There is no information provided in the context about Paul Graham's preference for eating burgers.\", cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=['What did Paul Graham do at Interleaf', 'Did he work well?', 'Did he work well at Interleaf?', 'Did Paul Graham like eating burger?', 'exit'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_rag_capability import Neo4jGraphCapability\n",
    "\n",
    "# Create a ConversableAgent (no LLM configuration)\n",
    "graph_rag_agent = ConversableAgent(\n",
    "    name=\"paul_graham_agent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Associate the capability with the agent\n",
    "graph_rag_capability = Neo4jGraphCapability(query_engine)\n",
    "graph_rag_capability.add_to_agent(graph_rag_agent)\n",
    "\n",
    "# Create a user proxy agent to converse with our RAG agent\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(graph_rag_agent, message=\"What happened at Interleaf and Viaweb?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit the example by defining custom entities, relations and schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n",
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.27it/s]\n",
      "Extracting paths from text with schema: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:56<00:00,  2.59s/it]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  4.02it/s]\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (e, row) { ... }} {position: line: 10, column: 21, offset: 397} for query: \"\\n                    UNWIND $data AS row\\n                    MERGE (e:__Node__ {id: row.id})\\n                    SET e += apoc.map.clean(row.properties, [], [])\\n                    SET e.name = row.name, e:`__Entity__`\\n                    WITH e, row\\n                    CALL apoc.create.addLabels(e, [row.label])\\n                    YIELD node\\n                    WITH e, row\\n                    CALL {\\n                        WITH e, row\\n                        WITH e, row\\n                        WHERE row.embedding IS NOT NULL\\n                        CALL db.create.setNodeVectorProperty(e, 'embedding', row.embedding)\\n                        RETURN count(*) AS count\\n                    }\\n                    WITH e, row WHERE row.properties.triplet_source_id IS NOT NULL\\n                    MERGE (c:__Node__ {id: row.properties.triplet_source_id})\\n                    MERGE (e)<-[:MENTIONS]-(c)\\n                    \"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from autogen import ConversableAgent, UserProxyAgent\n",
    "from autogen.agentchat.contrib.graph_rag.document import Document, DocumentType\n",
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_query_engine import Neo4jGraphQueryEngine\n",
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_rag_capability import Neo4jGraphCapability\n",
    "\n",
    "# load document\n",
    "input_path = \"../test/agentchat/contrib/graph_rag/paul_graham_essay.txt\"\n",
    "input_documents = [Document(doctype=DocumentType.TEXT, path_or_url=input_path)]\n",
    "\n",
    "\n",
    "# best practice to use upper-case\n",
    "entities = Literal[\"PERSON\", \"PLACE\", \"ORGANIZATION\"]  #\n",
    "relations = Literal[\"HAS\", \"PART_OF\", \"WORKED_ON\", \"WORKED_WITH\", \"WORKED_AT\"]\n",
    "\n",
    "# define which entities can have which relations\n",
    "validation_schema = {\n",
    "    \"PERSON\": [\"HAS\", \"PART_OF\", \"WORKED_ON\", \"WORKED_WITH\", \"WORKED_AT\"],\n",
    "    \"PLACE\": [\"HAS\", \"PART_OF\", \"WORKED_AT\"],\n",
    "    \"ORGANIZATION\": [\"HAS\", \"PART_OF\", \"WORKED_WITH\"],\n",
    "}\n",
    "\n",
    "# Create FalkorGraphQueryEngine\n",
    "query_engine = Neo4jGraphQueryEngine(\n",
    "    username=\"neo4j\",  # Change if you reset username\n",
    "    password=\"password\",  # Change if you reset password\n",
    "    host=\"bolt://172.17.0.2\",  # Change\n",
    "    port=7687,  # if needed\n",
    "    database=\"neo4j\",  # Change if you want to store the graphh in your custom database\n",
    "    entities=entities,  # possible entities\n",
    "    relations=relations,  # possible relations\n",
    "    validation_schema=validation_schema,  # schema to validate the extracted triplets\n",
    "    strict=True,  # enofrce the extracted triplets to be in the schema\n",
    ")\n",
    "\n",
    "# Ingest data and initialize the database\n",
    "query_engine.init_db(input_doc=input_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add capability to a ConversableAgent and query them again\n",
    "You should find the answers conform to your custom schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "Which companies did Paul Graham work for?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Paul Graham worked for Y Combinator (YC).\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "who did he worked with?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Jessica\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "Give me more people he worked with\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Trevor Blackwell, John Collison, Patrick Collison, Daniel Gackle, Ralph Hazell, Robert Morris, and Harj Taggar.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "Did he worked with Joe Biden?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "No, there is no mention or indication in the provided context information that he worked with Joe Biden.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'Which companies did Paul Graham work for?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Paul Graham worked for Y Combinator (YC).', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'who did he worked with?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Jessica', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'Give me more people he worked with', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Trevor Blackwell, John Collison, Patrick Collison, Daniel Gackle, Ralph Hazell, Robert Morris, and Harj Taggar.', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'Did he worked with Joe Biden?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'No, there is no mention or indication in the provided context information that he worked with Joe Biden.', 'role': 'user', 'name': 'paul_graham_agent'}], summary='No, there is no mention or indication in the provided context information that he worked with Joe Biden.', cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=['who did he worked with?', 'Give me more people he worked with', 'Did he worked with Joe Biden?', 'exit'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_rag_capability import Neo4jGraphCapability\n",
    "\n",
    "# Create a ConversableAgent (no LLM configuration)\n",
    "graph_rag_agent = ConversableAgent(\n",
    "    name=\"paul_graham_agent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Associate the capability with the agent\n",
    "graph_rag_capability = Neo4jGraphCapability(query_engine)\n",
    "graph_rag_capability.add_to_agent(graph_rag_agent)\n",
    "\n",
    "# Create a user proxy agent to converse with our RAG agent\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(graph_rag_agent, message=\"Which companies did Paul Graham work for?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can add new documents to the existing knoweldge graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 21.01it/s]\n",
      "Extracting paths from text with schema: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:12<00:00,  3.14s/it]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.14it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.29it/s]\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: CALL subquery without a variable scope clause is now deprecated. Use CALL (e, row) { ... }} {position: line: 10, column: 21, offset: 397} for query: \"\\n                    UNWIND $data AS row\\n                    MERGE (e:__Node__ {id: row.id})\\n                    SET e += apoc.map.clean(row.properties, [], [])\\n                    SET e.name = row.name, e:`__Entity__`\\n                    WITH e, row\\n                    CALL apoc.create.addLabels(e, [row.label])\\n                    YIELD node\\n                    WITH e, row\\n                    CALL {\\n                        WITH e, row\\n                        WITH e, row\\n                        WHERE row.embedding IS NOT NULL\\n                        CALL db.create.setNodeVectorProperty(e, 'embedding', row.embedding)\\n                        RETURN count(*) AS count\\n                    }\\n                    WITH e, row WHERE row.properties.triplet_source_id IS NOT NULL\\n                    MERGE (c:__Node__ {id: row.properties.triplet_source_id})\\n                    MERGE (e)<-[:MENTIONS]-(c)\\n                    \"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The procedure has a deprecated field. ('config' used by 'apoc.meta.graphSample' is deprecated.)} {position: line: 1, column: 1, offset: 0} for query: \"CALL apoc.meta.graphSample() YIELD nodes, relationships RETURN nodes, [rel in relationships | {name:apoc.any.property(rel, 'type'), count: apoc.any.property(rel, 'count')}] AS relationships\"\n"
     ]
    }
   ],
   "source": [
    "input_path = \"../test/agentchat/contrib/graph_rag/the_matrix.txt\"\n",
    "input_documents = [Document(doctype=DocumentType.TEXT, path_or_url=input_path)]\n",
    "\n",
    "_ = query_engine.add_records(input_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create a new graph rag agent and some quetions related to both 2 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "Who acted at 'The Matrix'?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, and Hugo Weaving acted in 'The Matrix'.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "Is there any addictional actors?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "No, there is no mention of additional actors in the provided context information.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to paul_graham_agent):\n",
      "\n",
      "How did Paul Graham work at 'The Matrix'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mpaul_graham_agent\u001b[0m (to user_proxy):\n",
      "\n",
      "Paul Graham did not work at 'The Matrix'.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': \"Who acted at 'The Matrix'?\", 'role': 'assistant', 'name': 'user_proxy'}, {'content': \"Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, and Hugo Weaving acted in 'The Matrix'.\", 'role': 'user', 'name': 'paul_graham_agent'}, {'content': 'Is there any addictional actors?', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'No, there is no mention of additional actors in the provided context information.', 'role': 'user', 'name': 'paul_graham_agent'}, {'content': \"How did Paul Graham work at 'The Matrix'\", 'role': 'assistant', 'name': 'user_proxy'}, {'content': \"Paul Graham did not work at 'The Matrix'.\", 'role': 'user', 'name': 'paul_graham_agent'}], summary=\"Paul Graham did not work at 'The Matrix'.\", cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=['Is there any addictional actors?', \"How did Paul Graham work at 'The Matrix'\", 'exit'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogen.agentchat.contrib.graph_rag.neo4j_graph_rag_capability import Neo4jGraphCapability\n",
    "\n",
    "# Create a ConversableAgent (no LLM configuration)\n",
    "graph_rag_agent = ConversableAgent(\n",
    "    name=\"paul_graham_agent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Associate the capability with the agent\n",
    "graph_rag_capability = Neo4jGraphCapability(query_engine)\n",
    "graph_rag_capability.add_to_agent(graph_rag_agent)\n",
    "\n",
    "# Create a user proxy agent to converse with our RAG agent\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"ALWAYS\",\n",
    ")\n",
    "\n",
    "user_proxy.initiate_chat(graph_rag_agent, message=\"Who acted at 'The Matrix'?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
