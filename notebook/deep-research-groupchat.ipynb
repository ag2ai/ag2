{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepseek implementstion using AG2 GroupChat\n",
    "\n",
    "You need to install `ag2[browser_use]` for the `WebSurferAgent` to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Annotated, Any, List, Optional\n",
    "\n",
    "import nest_asyncio\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import autogen\n",
    "from autogen import ConversableAgent\n",
    "from autogen.agents.experimental import WebSurferAgent\n",
    "from autogen.tools.dependency_injection import Depends, on\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\"tags\": [\"gpt-4o-mini\"]},\n",
    ")\n",
    "\n",
    "llm_config = {\"config_list\": config_list}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask function and initial analiser\n",
    "\n",
    "The initial_analiser agent will generate suquestions and use the answer question function to start internal team conversation between websurfer agent and a critic to create answers for suquestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    question: str,\n",
    "    llm_config: dict[str, Any],\n",
    "    max_web_steps: int = 30,\n",
    ") -> str:\n",
    "    class InformationCrumb(BaseModel):\n",
    "        source_url: str\n",
    "        source_title: str\n",
    "        source_summary: str\n",
    "        relevant_info: str\n",
    "\n",
    "    class GatheredInformation(BaseModel):\n",
    "        information: List[InformationCrumb]\n",
    "\n",
    "        def format(self) -> str:\n",
    "            return \"Here is the gathered information: \\n\" + \"\\n\".join(\n",
    "                f\"URL: {info.source_url}\\nTitle: {info.source_title}\\nSummary: {info.source_summary}\\nRelevant Information: {info.relevant_info}\\n\\n\"\n",
    "                for info in self.information\n",
    "            )\n",
    "\n",
    "    websurfer_config = copy.deepcopy(llm_config)\n",
    "\n",
    "    websurfer_config[\"config_list\"][0][\"response_format\"] = GatheredInformation\n",
    "\n",
    "    def is_termination_msg(x):\n",
    "        return x.get(\"content\", \"\") and x.get(\"content\", \"\").startswith(\"Answer confirmed:\")\n",
    "\n",
    "\n",
    "    websurfer = WebSurferAgent(\n",
    "        llm_config=websurfer_config,\n",
    "        name=\"WebSurferAgent\",\n",
    "        system_message=(\n",
    "            \"You are a web surfer agent responsible for gathering information from the web to provide information for answering a question\\n\"\n",
    "            \"You will be asked to find information related to the question and provide a summary of the information gathered.\\n\"\n",
    "            \"The summary should include the URL, title, summary, and relevant information for each piece of information gathered.\\n\"\n",
    "        ),\n",
    "        is_termination_msg=is_termination_msg,\n",
    "        human_input_mode=\"NEVER\",\n",
    "        web_tool_kwargs={\n",
    "            \"agent_kwargs\": {\"max_steps\": max_web_steps},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    websurfer_critic = ConversableAgent(\n",
    "        name=\"WebSurferCritic\",\n",
    "        system_message=(\n",
    "            \"You are a critic agent responsible for evaluating the answer provided by the web surfer agent.\\n\"\n",
    "            \"You need to confirm wehther the information provided by the websurfer is correct and sufficient to answer the question.\\n\"\n",
    "            \"You can ask the web surfer to provide more information or provide and confirm the answer.\\n\"\n",
    "        ),\n",
    "        llm_config=llm_config,\n",
    "        is_termination_msg=is_termination_msg,\n",
    "        human_input_mode=\"NEVER\",\n",
    "    )\n",
    "\n",
    "    @websurfer.register_for_execution()\n",
    "    @websurfer_critic.register_for_llm(\n",
    "        description=\"Call this method when you agree that the original question can be answered with the gathered information and provide the answer.\"\n",
    "    )\n",
    "    def confirm_answer(answer: str) -> str:\n",
    "        return \"Answer confirmed: \" + answer\n",
    "\n",
    "    websurfer_critic.register_for_execution()(websurfer.tool)\n",
    "\n",
    "    result = websurfer_critic.initiate_chat(\n",
    "        websurfer,\n",
    "        message=\"Please find the answer to this question: \" + question,\n",
    "    )\n",
    "\n",
    "    return result.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_question(\"What are the top most popular posts on hackernews?\", llm_config, max_web_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subquestion(BaseModel):\n",
    "    question: Annotated[str, \"The original question.\"]\n",
    "    answer: Annotated[Optional[str], \"The answer to the question.\"] = None\n",
    "\n",
    "    def format(self) -> str:\n",
    "        return f\"Question: {self.question}\\nAnswer: {self.answer}\\n\"\n",
    "\n",
    "\n",
    "class Task(BaseModel):\n",
    "    question: Annotated[str, \"The original question.\"]\n",
    "    subquestions: Annotated[List[Subquestion], \"The subquestions that need to be answered.\"]\n",
    "\n",
    "    def format(self) -> str:\n",
    "        return f\"Task: {self.question}\\n\\n\" + \"\\n\".join(\n",
    "            \"Subquestion \" + str(i + 1) + \":\\n\" + subquestion.format()\n",
    "            for i, subquestion in enumerate(self.subquestions)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_task = Task(\n",
    "    question=\"What is the capital of France?\", subquestions=[Subquestion(question=\"What is the capital of France?\")]\n",
    ")\n",
    "\n",
    "def split_question_and_answer_subquestions(\n",
    "        question: str, \n",
    "        llm_config: Annotated[dict[str, Any], Depends(on(llm_config))],\n",
    "    ) -> Task:\n",
    "    analyser = ConversableAgent(\n",
    "        name=\"InitialAnalysisAgent\",\n",
    "        system_message=(\n",
    "            \"You are an expert at breaking down complex questions into smaller, focused subquestions.\\n\"\n",
    "            \"Your task is to take any question provided and divide it into clear, actionable subquestions that can be individually answered.\\n\"\n",
    "            \"Ensure the subquestions are logical, non-redundant, and cover all key aspects of the original question.\\n\"\n",
    "            \"Avoid providing answers or interpretationsâ€”focus solely on decomposition.\\n\"\n",
    "            \"Do not include banal, general knowledge questions\\n\"\n",
    "            \"Do not include questions that go into unnecessary detail that is not relevant to the original question\\n\"\n",
    "            \"Do not include question that require knowledge of the original or other subquestions to answer\\n\"\n",
    "        ),\n",
    "        llm_config=llm_config,\n",
    "        is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").startswith(\"Subquestions generated:\"),\n",
    "        human_input_mode=\"NEVER\",\n",
    "    )\n",
    "\n",
    "    critic = ConversableAgent(\n",
    "        name=\"InitialAnalysisCritic\",\n",
    "        system_message=(\n",
    "            \"You are a critic agent responsible for evaluating the subquestions provided by the initial analysis agent.\\n\"\n",
    "            \"You need to confirm whether the subquestions are clear, actionable, and cover all key aspects of the original question.\\n\"\n",
    "            \"Do not accept redundant or unnecessary subquestions, focus solely on the minimal viable subset of subqestions necessary to answer the original question. \\n\"\n",
    "            \"Do not accept banal, general knowledge questions\\n\"\n",
    "            \"Do not accept questions that go into unnecessary detail that is not relevant to the original question\\n\"\n",
    "            \"Remove questions that can be answered with combinig knowledge from other questions\\n\"\n",
    "            \"After you are satisfied with the subquestions, call the 'generate_subquestions' method to answer each subquestion.\\n\"\n",
    "            \"This is an example of an argument that can be passed to the 'generate_subquestions' method:\\n\"\n",
    "            f\"{{'task': {example_task.model_dump()}}}\\n\"\n",
    "        ),\n",
    "        llm_config=llm_config,\n",
    "        is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").startswith(\"Subquestions answered:\"),\n",
    "        human_input_mode=\"NEVER\",\n",
    "    )\n",
    "\n",
    "    @analyser.register_for_execution()\n",
    "    @critic.register_for_llm(\n",
    "        name=\"generate_subquestions\",\n",
    "        description=\"Generates subquestions for a task.\",\n",
    "    )\n",
    "    def generate_subquestions(\n",
    "        task: Task, \n",
    "        llm_config: Annotated[dict[str, Any], Depends(on(llm_config))],\n",
    "    ) -> Task:\n",
    "        if not task.subquestions:\n",
    "            task.subquestions = [\n",
    "                Subquestion(question=task.question)\n",
    "            ]\n",
    "        \n",
    "        for subquestion in task.subquestions:\n",
    "            subquestion.answer = answer_question(subquestion.question, llm_config=llm_config)\n",
    "\n",
    "        return \"Subquestions answered: \\n\" + task.format()\n",
    "    \n",
    "    result = critic.initiate_chat(\n",
    "        analyser,\n",
    "        message=\"Analise and gather subqestions for the following question: \" + question,\n",
    "    )\n",
    "\n",
    "    return result.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split_question_and_answer_subquestions(\n",
    "    question = \"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\", \n",
    "    llm_config = llm_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizer agent\n",
    "\n",
    "Responsible for summarizing the subquestions into a complete answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_agent = ConversableAgent(\n",
    "    name=\"SummarizerAgent\",\n",
    "    system_message=(\n",
    "        \"You are an agent with a task of answering the question provided by the user.\"\n",
    "        \"First you need to split the question into subquestions by calling the 'split_question_and_answer_subquestions' method.\"\n",
    "        \"Then you need to sintesize the answers the original question by combining the answers to the subquestions.\"\n",
    "    ),\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").startswith(\"Answer confirmed:\"),\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic agent\n",
    "\n",
    "Responsible for evaluating the answer provided by the summarizer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_agent = ConversableAgent(\n",
    "    name=\"CriticAgent\",\n",
    "    system_message=(\n",
    "        \"You are a critic agent responsible for evaluating the answer provided by the summarizer agent.\\n\"\n",
    "        \"Your task is to assess the quality of the answer based on its coherence, relevance, and completeness.\\n\"\n",
    "        \"Provide constructive feedback on how the answer can be improved.\\n\"\n",
    "        \"If the answer is satisfactory, call the 'confirm_answer' method to end the task.\\n\"\n",
    "    ),\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").startswith(\"Answer confirmed:\"),\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "\n",
    "@summarizer_agent.register_for_execution()\n",
    "@critic_agent.register_for_llm(description=\"Call this method to confirm the final answer.\")\n",
    "def confirm_summary(\n",
    "    answer: str, \n",
    "    reasoning: str\n",
    ") -> str:\n",
    "    return \"Answer confirmed: \" + answer + \"\\nReasoning: \" + reasoning\n",
    "\n",
    "summarizer_agent.register_for_llm(\n",
    "    description = \"Split the question into subquestions and get answers.\"\n",
    ")(split_question_and_answer_subquestions)\n",
    "critic_agent.register_for_execution()(split_question_and_answer_subquestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = critic_agent.initiate_chat(\n",
    "        summarizer_agent,\n",
    "        message=\"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\", \n",
    "    )\n",
    "\n",
    "result.summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
