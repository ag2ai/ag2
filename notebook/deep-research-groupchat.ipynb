{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepseek implementstion using AG2 GroupChat\n",
    "\n",
    "You need to install `ag2[browser_use]` for the `WebSurferAgent` to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Any, List, Optional\n",
    "\n",
    "import nest_asyncio\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import autogen\n",
    "from autogen import ConversableAgent, UserProxyAgent\n",
    "from autogen.agents.experimental import WebSurferAgent\n",
    "from autogen.tools.dependency_injection import Depends, on\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\"tags\": [\"gpt-4o-mini\"]},\n",
    ")\n",
    "llm_config = {\"config_list\": config_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask function and initial analiser\n",
    "\n",
    "The initial_analiser agent will generate suquestions and use the answer question function to start internal team conversation between websurfer agent and a critic to create answers for suquestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    question: str,\n",
    "    llm_config: dict[str, Any],\n",
    ") -> str:\n",
    "    websurfer = WebSurferAgent(\n",
    "        llm_config=llm_config,\n",
    "        name=\"WebSurferAgent\",\n",
    "        system_message=(\n",
    "            \"You are a web surfer agent responsible for gathering information from the web to answer the provided question.\\n\"\n",
    "        ),\n",
    "        is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\") == \"Answer confirmed.\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "    )\n",
    "\n",
    "    websurfer_critic = ConversableAgent(\n",
    "        name=\"WebSurferCritic\",\n",
    "        system_message=\"You are a critic agent responsible for evaluating the answer provided by the web surfer agent.\\n\",\n",
    "        llm_config=llm_config,\n",
    "        is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\") == \"Answer confirmed.\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "    )\n",
    "\n",
    "    @websurfer.register_for_execution()\n",
    "    @websurfer_critic.register_for_llm(\n",
    "        description=\"Call this method when you agree that the original question has been answered correctly.\"\n",
    "    )\n",
    "    def confirm_answer() -> str:\n",
    "        return \"Answer confirmed.\"\n",
    "\n",
    "    websurfer_critic.register_for_execution()(websurfer.tool)\n",
    "\n",
    "    result = websurfer_critic.initiate_chat(\n",
    "        websurfer, message=\"Please find the answer to this question: \" + question, summary_method=\"reflection_with_llm\"\n",
    "    )\n",
    "\n",
    "    return result.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subquestion(BaseModel):\n",
    "    question: Annotated[str, \"The original question.\"]\n",
    "    answer: Annotated[Optional[str], \"The answer to the question.\"] = None\n",
    "\n",
    "\n",
    "class Task(BaseModel):\n",
    "    question: Annotated[str, \"The original question.\"]\n",
    "    subquestions: Annotated[List[Subquestion], \"The subquestions that need to be answered.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SUBQUESTIONS = \"two\"\n",
    "\n",
    "example_task = Task(\n",
    "    question=\"What is the capital of France?\", subquestions=[Subquestion(question=\"What is the capital of France?\")]\n",
    ")\n",
    "\n",
    "initial_analyser = ConversableAgent(\n",
    "    name=\"InitialAnalysisAgent\",\n",
    "    system_message=(\n",
    "        \"You are an expert at breaking down complex questions into smaller, focused subquestions.\\n\"\n",
    "        \"Your task is to take any question provided and divide it into clear, actionable subquestions that can be individually answered.\\n\"\n",
    "        \"Ensure the subquestions are logical, non-redundant, and cover all key aspects of the original question.\\n\"\n",
    "        \"Avoid providing answers or interpretations—focus solely on decomposition.\\n\"\n",
    "        \"After breaking down the question into subquestions, call the 'generate_subquestions' method to answer each subquestion.\\n\"\n",
    "        \"This is an example of an argument that can be passed to the 'generate_subquestions' method:\\n\"\n",
    "        f\"{{'task': {example_task.model_dump()}}}\\n\"\n",
    "        \"Do NOT forward the initial task without breaking it down into subquestions!!!\\n\"\n",
    "        \"Keep the subquestions to a maximum of {MAX_SUBQUESTIONS} subquestions.\\n\"\n",
    "    ),\n",
    "    llm_config=llm_config,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\") == \"Summary confirmed.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "\n",
    "@initial_analyser.register_for_llm(\n",
    "    name=\"generate_subquestions\",\n",
    "    description=\"Generates subquestions for a task. Keep it to a maximum of {MAX_SUBQUESTIONS} subquestions.\",\n",
    ")\n",
    "def generate_subquestions(task: Task, llm_config: Annotated[dict[str, Any], Depends(on(llm_config))]) -> Task:\n",
    "    for subquestion in task.subquestions:\n",
    "        subquestion.answer = answer_question(subquestion.question, llm_config=llm_config)\n",
    "\n",
    "    return task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizer agent\n",
    "\n",
    "Responsible for summarizing the subquestions into a complete answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_agent = ConversableAgent(\n",
    "    name=\"SummarizerAgent\",\n",
    "    system_message=(\n",
    "        \"You are a summarizer agent responsible for summarizing the results of the subquestions provided by the initial analysis agent.\\n\"\n",
    "        \"Your task is to take the subquestions and their answers and provide a concise summary of the overall question.\\n\"\n",
    "        \"Ensure the summary is clear, coherent, and covers all key aspects of the original question.\\n\"\n",
    "        \"Avoid providing new information or interpretations—focus solely on summarization.\\n\"\n",
    "    ),\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\") == \"Summary confirmed.\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic agent\n",
    "\n",
    "Responsible for evaluating the answer provided by the summarizer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_agent = ConversableAgent(\n",
    "    name=\"CriticAgent\",\n",
    "    system_message=(\n",
    "        \"You are a critic agent responsible for evaluating the summary provided by the summarizer agent.\\n\"\n",
    "        \"Your task is to assess the quality of the summary based on its coherence, relevance, and completeness.\\n\"\n",
    "        \"Provide constructive feedback on how the summary can be improved.\\n\"\n",
    "        \"If the summary is satisfactory, call the 'confirm_summary' method to end the task.\\n\"\n",
    "    ),\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\") == \"Summary confirmed.\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "\n",
    "@critic_agent.register_for_llm(description=\"Call this method to confirm the summary.\")\n",
    "def confirm_summary() -> str:\n",
    "    return \"Summary confirmed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User proxy agent and state transitions\n",
    "\n",
    "To enable function calling we need to create a UserProxyAgent that will act as a proxy for the user. This agent will be responsible for calling the functions of the other agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    system_message=\"A proxy to excute code\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "user_proxy.register_for_execution()(generate_subquestions)\n",
    "user_proxy.register_for_execution()(confirm_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_transition(last_speaker, groupchat):\n",
    "    messages = groupchat.messages\n",
    "\n",
    "    # always start with the user\n",
    "    if len(messages) <= 1:\n",
    "        return initial_analyser\n",
    "\n",
    "    # if the last message is a tool call, return the tool_execution agent\n",
    "    if \"tool_calls\" in messages[-1]:\n",
    "        return user_proxy\n",
    "\n",
    "    if last_speaker is user_proxy:\n",
    "        penultimate_speaker = groupchat.agent_by_name(name=messages[-2].get(\"name\", \"\"))\n",
    "\n",
    "        if penultimate_speaker is initial_analyser:\n",
    "            return summarizer_agent\n",
    "\n",
    "        if penultimate_speaker is critic_agent:\n",
    "            return initial_analyser\n",
    "\n",
    "    if last_speaker is summarizer_agent:\n",
    "        return critic_agent\n",
    "\n",
    "    if last_speaker is critic_agent:\n",
    "        return initial_analyser\n",
    "\n",
    "    if last_speaker is initial_analyser:\n",
    "        return summarizer_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "\n",
    "Run the groupchat and get the final answer by calling chat_result.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupchat = autogen.GroupChat(\n",
    "    agents=[initial_analyser, summarizer_agent, critic_agent, user_proxy],\n",
    "    messages=[],\n",
    "    speaker_selection_method=state_transition,\n",
    "    max_round=1000,\n",
    ")\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "chat_result = user_proxy.initiate_chat(\n",
    "    manager, message=\"Who are the founders of AG2\", summary_method=\"reflection_with_llm\"\n",
    ")\n",
    "\n",
    "chat_result.summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
