{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# MCP Multi-Agent Example: Multiagent One MCP Server.\n",
    "This Example Demonstrates a multiagent Orchestration to interact with a filesystem MCP server in order to perform code Analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup \n",
    "- install  ```ag2[openai,mcp]```\n",
    "- import Agent classes\n",
    "- import sse, stdio and streamable-http client methods to connect to server via different transport methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -q -U \"ag2[openai,mcp]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary agent and client libraries\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "import dotenv\n",
    "from mcp.client.session import ClientSession\n",
    "from mcp.client.sse import sse_client\n",
    "from mcp.client.stdio import StdioServerParameters, stdio_client\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "from autogen import LLMConfig\n",
    "from autogen.agentchat import AssistantAgent, UserProxyAgent, initiate_group_chat\n",
    "from autogen.agentchat.group import (\n",
    "    AgentTarget,\n",
    "    ReplyResult,\n",
    ")\n",
    "from autogen.agentchat.group.patterns import AutoPattern\n",
    "from autogen.mcp import create_toolkit\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "# Only needed for Jupyter notebooks\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Create an LLM Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a llm config\n",
    "llm_config = LLMConfig(model=\"o4-mini\", api_type=\"openai\", api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Initialize Agents and orchestrate the Autopattern\n",
    "- analysis agent : will select the tool to execute and perform analysis on file contents.\n",
    "- reporter agent : generate reports based on the analysis\n",
    "- userporxy agent: responsible for tool execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.conversable_agent import ConversableAgent\n",
    "\n",
    "with llm_config:\n",
    "    # Create a Analysis Agent\n",
    "    analysis_agent = ConversableAgent(\n",
    "        name=\"analysis_agent\",\n",
    "        system_message=\"You are a analysis agent that can perform analysis on the content of the file and return the result.\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "    )\n",
    "    # Create a report writer Agent\n",
    "    reporter = ConversableAgent(\n",
    "        name=\"report_writer\",\n",
    "        system_message=\"You are a reporter agent that can report the result of the analysis.\",\n",
    "        human_input_mode=\"NEVER\",\n",
    "    )\n",
    "    pass\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"Admin\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config=False,\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "# define orchestrate a pattern\n",
    "pattern = AutoPattern(\n",
    "    initial_agent=analysis_agent,\n",
    "    agents=[user_proxy, analysis_agent, reporter],\n",
    "    group_manager_args={\"llm_config\": llm_config},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## MCP Server Client tool with sse transport\n",
    "- this tool is an example of how an multiagent orcestration can interact with a mcp server with sse transport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run mcp ag2 client\n",
    "tool_prompt = \"\"\"\n",
    "use mcp tools to List files and directories under CONTEXT_PATH/relative_path and read the content of the file\n",
    "args:\n",
    "query: str (initial message to the agent)\n",
    "\"\"\"\n",
    "MCP_SERVER_URL = os.getenv(\"MCP_SERVER_URL\")\n",
    "\n",
    "\n",
    "@user_proxy.register_for_execution(description=tool_prompt)\n",
    "@analysis_agent.register_for_llm(description=tool_prompt)\n",
    "async def run_mcp_ag2_client_with_sse_client(query: str):\n",
    "    async with sse_client(url=\"http://127.0.0.1:8000/sse\") as (read, write), ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "        # create a MCP toolkit with create_toolkit method with MCP server session as param\n",
    "        toolkit = await create_toolkit(session=session)\n",
    "        agent = AssistantAgent(\n",
    "            name=\"assistant\",\n",
    "            llm_config=llm_config,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "        toolkit.register_for_llm(agent)\n",
    "        # Make a request using the MCP tool\n",
    "        result = await agent.a_run(\n",
    "            message=query,\n",
    "            tools=toolkit.tools,\n",
    "            max_turns=2,\n",
    "        )\n",
    "        res = await result.process()\n",
    "        return ReplyResult(message=str(res), target=AgentTarget(analysis_agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a group chat with the pattern\n",
    "result, context_variables, last_agent = initiate_group_chat(\n",
    "    messages=\"search wikipedia mcp the term 'upcoming movies 2026'\",\n",
    "    max_rounds=3,\n",
    "    pattern=pattern,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## MCP Client with streamable-http transport\n",
    "- This tool is an example of how an multiagent orchestration can help with streamable-http transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCP_SERVER_URL = os.getenv(\"MCP_SERVER_URL\")\n",
    "tool_prompt = \"\"\"\n",
    "use mcp tools to List files and directories under CONTEXT_PATH/relative_path and read the content of the file\n",
    "args:\n",
    "query: str (detail task for mcp server to execute)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@user_proxy.register_for_execution(description=tool_prompt)\n",
    "@analysis_agent.register_for_llm(description=tool_prompt)\n",
    "async def run_mcp_ag2_client_with_streamable_http_client(query: str):\n",
    "    async with (\n",
    "        streamablehttp_client(MCP_SERVER_URL) as (\n",
    "            read_stream,\n",
    "            write_stream,\n",
    "            _,\n",
    "        ),\n",
    "        ClientSession(read_stream, write_stream) as session,\n",
    "    ):\n",
    "        await session.initialize()\n",
    "        # create a MCP toolkit with create_toolkit method with MCP server session as param\n",
    "        toolkit = await create_toolkit(session=session)\n",
    "        agent = AssistantAgent(\n",
    "            name=\"assistant\",\n",
    "            llm_config=llm_config,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "        toolkit.register_for_llm(agent)\n",
    "        # Make a request using the MCP tool\n",
    "        result = await agent.a_run(\n",
    "            message=query,\n",
    "            tools=toolkit.tools,\n",
    "            max_turns=2,\n",
    "        )\n",
    "        res = await result.process()\n",
    "        return ReplyResult(message=str(res), target=AgentTarget(analysis_agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a group chat with the pattern\n",
    "result, context_variables, last_agent = initiate_group_chat(\n",
    "    messages=\"analyze the content of the file text.txt\",\n",
    "    max_rounds=5,\n",
    "    pattern=pattern,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## MCP Client with Stdio transport\n",
    "- This tool is an example of how an multiagent orchestration can help with stdio transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCP_SERVER_PATH = os.getenv(\"MCP_SERVER_PATH\")\n",
    "tool_prompt = \"\"\"\n",
    "\"use mcp tools to List files and directories under CONTEXT_PATH/relative_path and read the content of the file\n",
    "args:\n",
    "query: str (initial message to the agent)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create server parameters for stdio connection\n",
    "@user_proxy.register_for_execution(description=tool_prompt)\n",
    "@analysis_agent.register_for_llm(description=tool_prompt)\n",
    "async def run_mcp_ag2_client_with_stdio_client(query: str):\n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"python\",  # The command to run the server\n",
    "        args=[\n",
    "            MCP_SERVER_PATH,\n",
    "            \"stdio\",\n",
    "        ],  # Path to server script and transport mode\n",
    "    )\n",
    "    async with (\n",
    "        stdio_client(server_params) as (read, write),\n",
    "        ClientSession(read, write, read_timeout_seconds=timedelta(seconds=30)) as session,\n",
    "    ):\n",
    "        # Initialize the connection\n",
    "        await session.initialize()\n",
    "        toolkit = await create_toolkit(session=session)\n",
    "        agent = AssistantAgent(\n",
    "            name=\"assistant\",\n",
    "            llm_config=llm_config,\n",
    "            human_input_mode=\"NEVER\",\n",
    "        )\n",
    "        toolkit.register_for_llm(agent)\n",
    "        # Make a request using the MCP tool\n",
    "        result = await agent.a_run(\n",
    "            message=query,\n",
    "            tools=toolkit.tools,\n",
    "            max_turns=2,\n",
    "        )\n",
    "        res = await result.process()\n",
    "        return ReplyResult(message=str(res), target=AgentTarget(analysis_agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a group chat with the pattern\n",
    "result, context_variables, last_agent = initiate_group_chat(\n",
    "    messages=\"analyze the content of the file text.txt\",\n",
    "    max_rounds=5,\n",
    "    pattern=pattern,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
