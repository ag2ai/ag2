{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Chat\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://ag2ai.github.io/ag2/docs/Use-Cases/agent_chat).\n",
    "\n",
    "This notebook is modified based on https://github.com/ag2ai/FLAML/blob/4ea686af5c3e8ff24d9076a7a626c8b28ab5b1d7/notebook/autogen_multiagent_roleplay_chat.ipynb\n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Install `pyautogen`:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```\n",
    "\n",
    "For more information, please refer to the [installation guide](/docs/installation/).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://ag2ai.github.io/ag2/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4o\", \"gpt-4o-mini\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{=mdx}\n",
    ":::tip\n",
    "Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration).\n",
    ":::\n",
    "````\n",
    "\n",
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ResponseModel(BaseModel):\n",
    "    question: str\n",
    "    short_answer: str\n",
    "    reasoning: str\n",
    "    difficulty: float\n",
    "\n",
    "\n",
    "llm_config = {\"config_list\": config_list, \"cache_seed\": 42}\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"Assistant\",\n",
    "    llm_config=llm_config,\n",
    "    response_format=ResponseModel,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to Assistant):\n",
      "\n",
      "What's up\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAssistant\u001b[0m (to User_proxy):\n",
      "\n",
      "Hello! How can I assist you today? If you have any tasks or questions, feel free to let me know.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': \"What's up\", 'role': 'assistant', 'name': 'User_proxy'}, {'content': 'Hello! How can I assist you today? If you have any tasks or questions, feel free to let me know.', 'role': 'user', 'name': 'Assistant'}], summary='Hello! How can I assist you today? If you have any tasks or questions, feel free to let me know.', cost={'usage_including_cached_inference': {'total_cost': 0.0014125000000000001, 'gpt-4o-2024-08-06': {'cost': 0.0014125000000000001, 'prompt_tokens': 469, 'completion_tokens': 24, 'total_tokens': 493}}, 'usage_excluding_cached_inference': {'total_cost': 0.0014125000000000001, 'gpt-4o-2024-08-06': {'cost': 0.0014125000000000001, 'prompt_tokens': 469, 'completion_tokens': 24, 'total_tokens': 493}}}, human_input=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_proxy.initiate_chat(assistant, message=\"What's up\", max_turns=1, summary_method=\"last_msg\")\n",
    "# type exit to terminate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "front_matter": {
   "description": "Explore the utilization of large language models in automated group chat scenarios, where agents perform tasks collectively, demonstrating how they can be configured, interact with each other, and retrieve specific information from external resources.",
   "tags": [
    "orchestration",
    "group chat",
    "code generation"
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
