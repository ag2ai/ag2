{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured output\n",
    "\n",
    "OpenAI offers a functionality for defining a structure of the messages generated by LLMs, AutoGen enables this functionality by propagating `response_format` passed to your agents to the underlying client.\n",
    "\n",
    "For more info on structured output, please check [here](https://platform.openai.com/docs/guides/structured-outputs)\n",
    "\n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Install `pyautogen`:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```\n",
    "\n",
    "For more information, please refer to the [installation guide](/docs/installation/).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://ag2ai.github.io/ag2/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4o\", \"gpt-4o-mini\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{=mdx}\n",
    ":::tip\n",
    "Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration).\n",
    ":::\n",
    "````\n",
    "\n",
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class ResponseModel(BaseModel):\n",
    "    question: str\n",
    "    short_answer: str\n",
    "    reasoning: str\n",
    "    difficulty: float\n",
    "\n",
    "\n",
    "llm_config = {\"config_list\": config_list, \"cache_seed\": 42}\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"Assistant\",\n",
    "    llm_config=llm_config,\n",
    "    response_format=ResponseModel,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to Assistant):\n",
      "\n",
      "What's up\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 11-29 19:13:00] {921} INFO - Failed to cache response: Can't pickle <class 'openai.types.chat.parsed_chat_completion.ParsedChatCompletion[ResponseModel]'>: attribute lookup ParsedChatCompletion[ResponseModel] on openai.types.chat.parsed_chat_completion failed\n",
      "\u001b[33mAssistant\u001b[0m (to User_proxy):\n",
      "\n",
      "{\"question\":\"What's the current date and time?\",\"short_answer\":\"You can get the current date and time by executing the following code:\",\"reasoning\":\"To get the current date and time, we can use Python's built-in datetime module which provides functions to get the current date and time. By executing the code snippet, you will be able to see the current date and time.\",\"difficulty\":1}\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': \"What's up\", 'role': 'assistant', 'name': 'User_proxy'}, {'content': '{\"question\":\"What\\'s the current date and time?\",\"short_answer\":\"You can get the current date and time by executing the following code:\",\"reasoning\":\"To get the current date and time, we can use Python\\'s built-in datetime module which provides functions to get the current date and time. By executing the code snippet, you will be able to see the current date and time.\",\"difficulty\":1}', 'tool_calls': [], 'role': 'user', 'name': 'Assistant'}], summary='{\"question\":\"What\\'s the current date and time?\",\"short_answer\":\"You can get the current date and time by executing the following code:\",\"reasoning\":\"To get the current date and time, we can use Python\\'s built-in datetime module which provides functions to get the current date and time. By executing the code snippet, you will be able to see the current date and time.\",\"difficulty\":1}', cost={'usage_including_cached_inference': {'total_cost': 0.0021725000000000004, 'gpt-4o-2024-08-06': {'cost': 0.0021725000000000004, 'prompt_tokens': 541, 'completion_tokens': 82, 'total_tokens': 623}}, 'usage_excluding_cached_inference': {'total_cost': 0.0021725000000000004, 'gpt-4o-2024-08-06': {'cost': 0.0021725000000000004, 'prompt_tokens': 541, 'completion_tokens': 82, 'total_tokens': 623}}}, human_input=[])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_proxy.initiate_chat(assistant, message=\"What's up\", max_turns=1, summary_method=\"last_msg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "front_matter": {
   "description": "Explore the utilization of large language models in automated group chat scenarios, where agents perform tasks collectively, demonstrating how they can be configured, interact with each other, and retrieve specific information from external resources.",
   "tags": [
    "orchestration",
    "group chat",
    "code generation"
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
