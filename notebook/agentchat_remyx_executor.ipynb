{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RemyxCodeExecutor: Run Research Papers in 30 Seconds\n",
        "\n",
        "**Search â†’ Execute â†’ Explore** research papers with pre-built Docker environments and AI-guided code execution.\n",
        "\n",
        "## The Reproducibility Problem\n",
        "\n",
        "Running code from research papers is a dependency resolution and environment configuration problem:\n",
        "```python\n",
        "# Standard approach:\n",
        "git clone https://github.com/author/paper-code\n",
        "conda create -n paper python=3.8\n",
        "pip install -r requirements.txt  \n",
        "# 30+ minutes of downloading dependencies\n",
        "# CUDA 11.3 required, you have 12.1\n",
        "# PyTorch 1.9 conflicts with transformers>=4.0\n",
        "# Missing apt packages, system libraries\n",
        "# Papers don't specify exact environment\n",
        "# Code written for one GPU, you have different hardware\n",
        "```\n",
        "Even when authors provide installation instructions, you're resolving:\n",
        "\n",
        "* Conflicting transitive dependencies\n",
        "* CUDA/cuDNN version matching\n",
        "* System-level library requirements\n",
        "* Hardware-specific configurations\n",
        "* Undocumented environment assumptions\n",
        "\n",
        "**Hours of environment debugging before running a single experiment.**\n",
        "\n",
        "## What RemyxCodeExecutor Provides\n",
        "RemyxCodeExecutor gives you executable Docker images for 1000+ ArXiv papers where:\n",
        "\n",
        "* All dependencies are pre-installed and version-locked\n",
        "* CUDA/system libraries are pre-configured\n",
        "* Environments are tested and reproducible\n",
        "* Code execution is instrumented for programmatic access\n",
        "\n",
        "Integration with AG2 (AutoGen) adds:\n",
        "\n",
        "* Programmatic code execution in isolated containers\n",
        "* AI agents that can read, explain, and modify paper code\n",
        "* Interactive and batch execution modes\n",
        "* Structured exploration of research codebases\n",
        "\n",
        "This notebook demonstrates:\n",
        "\n",
        "* Searching Remyx's paper index for Docker-enabled papers\n",
        "* Instantiating RemyxCodeExecutor with a paper's environment\n",
        "* Using AG2 agents to explore and run experiments programmatically\n",
        "\n",
        "**Replace hours of dependency resolution with API calls to pre-built environments and accelerate your research and development.**\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Install AG2 with Remyx support:\n",
        "```bash\n",
        "pip install ag2[remyx]\n",
        "```\n",
        "\n",
        "Make sure you have the following dependencies:\n",
        "* [Remyx AI API key](https://docs.remyx.ai/cli)\n",
        "* [Docker](https://www.docker.com/get-started/)\n",
        "* [OpenAI API key for LLM agents](https://platform.openai.com/api-keys)\n",
        "\n",
        "Set your API tokens as environment variables:\n",
        "\n",
        "```bash\n",
        "export REMYXAI_API_KEY=your_remyxai_token\n",
        "export OPENAI_API_KEY=your_openai_key\n",
        "```"
      ],
      "metadata": {
        "id": "VL3_XWS1CBPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Ensure you have your API tokens set\n",
        "assert os.getenv(\"REMYXAI_API_KEY\"), \"Please set REMYXAI_API_KEY environment variable\"\n",
        "assert os.getenv(\"OPENAI_API_KEY\"), \"Please set OPENAI_API_KEY environment variable"
      ],
      "metadata": {
        "id": "fwSPHojGGlU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Discover Papers\n",
        "\n",
        "Search 1000+ research papers with pre-built Docker environments:"
      ],
      "metadata": {
        "id": "RCGmsoFxG0Y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from remyxai.client.search import SearchClient\n",
        "\n",
        "client = SearchClient()\n",
        "\n",
        "# Search for papers\n",
        "papers = client.search(\n",
        "    query=\"CLIP semantic alignment\",\n",
        "    has_docker=True,\n",
        "    max_results=5\n",
        ")\n",
        "\n",
        "# Browse results\n",
        "for paper in papers:\n",
        "    print(f\"ðŸ“– {paper.title[:50]}...\")\n",
        "    print(f\"   arXiv: {paper.arxiv_id}\")\n",
        "    print(f\"   image: {paper.docker_image}\")\n",
        "    print(f\"   abstract: {paper.abstract}\\n\")"
      ],
      "metadata": {
        "id": "VtruuzoIG7Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll see results like:\n",
        "```python\n",
        "ðŸ“– CLIPin: A Non-contrastive Plug-in to CLIP for Mult...\n",
        "   arXiv: 2508.06434v1\n",
        "   image: remyxai/2508.06434v1:latest\n",
        "   abstract: Large-scale natural image-text datasets, especially those automatically\n",
        "collected from the web, often suffer from loose semantic alignment due to weak\n",
        "supervision, while medical datasets tend to have high cross-modal correlation\n",
        "but low content diversity. These properties pose a common challenge for\n",
        "contrastive language-image pretraining (CLIP): they hinder the model's ability\n",
        "to learn robust and generalizable representations. In this work, we propose\n",
        "CLIPin, a unified non-contrastive plug-in th\n",
        "\n",
        "ðŸ“– COOkeD: Ensemble-based OOD detection in the era of...\n",
        "   arXiv: 2507.22576v1\n",
        "   image: remyxai/2507.22576v1:latest\n",
        "   abstract: Out-of-distribution (OOD) detection is an important building block in\n",
        "trustworthy image recognition systems as unknown classes may arise at\n",
        "test-time. OOD detection methods typically revolve around a single classifier,\n",
        "leading to a split in the research field between the classical supervised\n",
        "setting (e.g. ResNet18 classifier trained on CIFAR100) vs. the zero-shot\n",
        "setting (class names fed as prompts to CLIP). In both cases, an overarching\n",
        "challenge is that the OOD detection performance is implici\n",
        "\n",
        "ðŸ“– Mammo-CLIP Dissect: A Framework for Analysing Mamm...\n",
        "   arXiv: 2509.21102v1\n",
        "   image: remyxai/2509.21102v1:latest\n",
        "   abstract: Understanding what deep learning (DL) models learn is essential for the safe\n",
        "deployment of artificial intelligence (AI) in clinical settings. While previous\n",
        "work has focused on pixel-based explainability methods, less attention has been\n",
        "paid to the textual concepts learned by these models, which may better reflect\n",
        "the reasoning used by clinicians. We introduce Mammo-CLIP Dissect, the first\n",
        "concept-based explainability framework for systematically dissecting DL vision\n",
        "models trained for mammograp\n",
        "\n",
        "ðŸ“– CLASP: General-Purpose Clothes Manipulation with S...\n",
        "   arXiv: 2507.19983v1\n",
        "   image: remyxai/2507.19983v1:latest\n",
        "   abstract: Clothes manipulation, such as folding or hanging, is a critical capability\n",
        "for home service robots. Despite recent advances, most existing methods remain\n",
        "limited to specific tasks and clothes types, due to the complex,\n",
        "high-dimensional geometry of clothes. This paper presents CLothes mAnipulation\n",
        "with Semantic keyPoints (CLASP), which aims at general-purpose clothes\n",
        "manipulation over different clothes types, T-shirts, shorts, skirts, long\n",
        "dresses, ... , as well as different tasks, folding, flatt\n",
        "\n",
        "ðŸ“– Personalized Education with Ranking Alignment Reco...\n",
        "   arXiv: 2507.23664v1\n",
        "   image: remyxai/2507.23664v1:latest\n",
        "   abstract: Personalized question recommendation aims to guide individual students\n",
        "through questions to enhance their mastery of learning targets. Most previous\n",
        "methods model this task as a Markov Decision Process and use reinforcement\n",
        "learning to solve, but they struggle with efficient exploration, failing to\n",
        "identify the best questions for each student during training. To address this,\n",
        "we propose Ranking Alignment Recommendation (RAR), which incorporates\n",
        "collaborative ideas into the exploration mechanism,\n",
        "```"
      ],
      "metadata": {
        "id": "JD--eorYHTo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Fast Exploration\n",
        "\n",
        "You can quickly explore the contents of the codebase and environment using the `explore()` method of the `RemyxCodeExecutor`.\n",
        "\n",
        "How it works:\n",
        "\n",
        "1. Pulls Docker image with paper's code and dependencies\n",
        "2. Creates AI agents (one explores, one executes)\n",
        "3. Interactive session starts - you guide the exploration\n",
        "4. Ask free form questions about the code, create your own tests, and expand upon the research!\n",
        "\n",
        "You can launch an interactive session where you are able to chat with the system of agents or run automatically without pausing to run default tests and exploration."
      ],
      "metadata": {
        "id": "zz45Kyy7HbiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick Start (Default Exploration)"
      ],
      "metadata": {
        "id": "0Zr0KN4BIh1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen.coding import RemyxCodeExecutor\n",
        "\n",
        "executor = RemyxCodeExecutor(arxiv_id=\"2508.06434v1\")\n",
        "executor.explore()"
      ],
      "metadata": {
        "id": "0l-flTBWIerW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Mode (Automated)"
      ],
      "metadata": {
        "id": "Vw6bihSbIk9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Runs automatically without pausing\n",
        "executor = RemyxCodeExecutor(arxiv_id=\"2508.06434v1\")\n",
        "result = executor.explore(\n",
        "    goal=\"Run the default example quickstart\",\n",
        "    interactive=False,\n",
        "    max_turns=20\n",
        ")\n",
        "\n",
        "print(f\"âœ… Completed {len(result.chat_history)} steps\")"
      ],
      "metadata": {
        "id": "NVjIa_86IpwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Real-world Example: Exploring CLIPin\n",
        "Let's explore the [CLIPin paper](https://arxiv.org/pdf/2508.06434) - a method that improves CLIP's semantic alignment using non-contrastive learning.\n"
      ],
      "metadata": {
        "id": "31fNgB29cLfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen.coding import RemyxCodeExecutor\n",
        "\n",
        "# Create executor for CLIPin paper\n",
        "executor = RemyxCodeExecutor(arxiv_id=\"2508.06434v1\")\n",
        "\n",
        "# Start interactive exploration\n",
        "result = executor.explore(\n",
        "    goal=\"\"\"Explore CLIPin step-by-step:\n",
        "\n",
        "    Phase 1: Understanding\n",
        "    - Show repository structure\n",
        "    - Read the README\n",
        "    - Find the CLIPin model code\n",
        "\n",
        "    Phase 2: Architecture\n",
        "    - Explain the non-contrastive approach\n",
        "    - Show the loss function\n",
        "    - Compare with standard CLIP\n",
        "\n",
        "    Phase 3: Demo\n",
        "    - Load a model\n",
        "    - Run inference example\n",
        "\n",
        "    Work step-by-step. Explain clearly.\n",
        "    \"\"\",\n",
        "    interactive=False\n",
        ")"
      ],
      "metadata": {
        "id": "AQXp33LOGvXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can expect the output after multiple turns to look like:\n",
        "```python\n",
        "...\n",
        "\n",
        "exitcode: 0 (execution succeeded)\n",
        "Code output: Dummy Similarity Scores: tensor([[1.]])\n",
        "\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "research_explorer (to code_executor):\n",
        "\n",
        "The inference completed successfully, and we generated dummy similarity scores! The output indicates that the similarity between the randomly generated image features and text features resulted in a score of `1`. This output was produced using placeholder data since we didn't load actual model weights.\n",
        "\n",
        "### Summary of What We Completed:\n",
        "1. **Explored Repository**: We examined the repository structure, README, and model code.\n",
        "2. **Learned About CLIPin**: We discussed the non-contrastive nature of CLIPin and its loss functions.\n",
        "3. **Set Up Inference**: We created a placeholder setup to demonstrate the inference process, including generating a dummy image and running the model's inference logic.\n",
        "\n",
        "Would you like to analyze anything further related to CLIPin, or is there another question you have in mind?\n",
        "\n",
        "--------------------------------------------------------------------------------\n",
        "```"
      ],
      "metadata": {
        "id": "DAv0b9NwcoQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building on Research\n",
        "Use paper code as starting point for your own projects and research"
      ],
      "metadata": {
        "id": "Tfbd73mddG75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import ConversableAgent\n",
        "from autogen.coding import RemyxCodeExecutor\n",
        "\n",
        "# Start with paper's environment\n",
        "executor = RemyxCodeExecutor(arxiv_id=\"2508.06434v1\")\n",
        "\n",
        "# Create your own agent for custom experiments\n",
        "agent = ConversableAgent(\n",
        "    \"my_researcher\",\n",
        "    llm_config=False,\n",
        "    code_execution_config={\"executor\": executor},\n",
        "    human_input_mode=\"NEVER\"\n",
        ")\n",
        "\n",
        "# Run your custom code in paper's environment\n",
        "agent.generate_reply(messages=[{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"\"\"```python\n",
        "# Your custom experiment here\n",
        "from clip.model import CLIPin\n",
        "model = CLIPin.load_pretrained()\n",
        "# ... your modifications ...\n",
        "```\"\"\"\n",
        "}])"
      ],
      "metadata": {
        "id": "B8J1HoYtdRB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Features"
      ],
      "metadata": {
        "id": "VS49XQuPddX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Docker Args\n",
        "You can pass additional args to `container_create_kwargs` for further customization and configuration of containers like passing additional environment variables or switching to a GPU enabled container runtime."
      ],
      "metadata": {
        "id": "P_1GWm7sdgeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "executor = RemyxCodeExecutor(\n",
        "    arxiv_id=\"2508.06434v1\",\n",
        "    timeout=600,\n",
        "    container_create_kwargs={\n",
        "        \"environment\": {\n",
        "            \"HF_TOKEN\": os.getenv(\"HF_TOKEN\"),\n",
        "            \"WANDB_API_KEY\": os.getenv(\"WANDB_API_KEY\"),\n",
        "        },\n",
        "        \"mem_limit\": \"16g\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "lcRXrX2idvrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paper Metadata"
      ],
      "metadata": {
        "id": "ZgyrEnvsdyS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "executor = RemyxCodeExecutor(arxiv_id=\"2508.06434v1\")\n",
        "context = executor.get_paper_context()\n",
        "print(context)"
      ],
      "metadata": {
        "id": "M5cx_BSMd1Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Direct Use of Docker Images"
      ],
      "metadata": {
        "id": "84yEda5zd258"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you know the image name\n",
        "executor = RemyxCodeExecutor(\n",
        "    image=\"remyxai/2508.06434v1:latest\",\n",
        "    timeout=300\n",
        ")"
      ],
      "metadata": {
        "id": "huxugCkMd7C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Manual Agent Control"
      ],
      "metadata": {
        "id": "uCmYpFlGd_2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For advanced users who want full control\n",
        "executor = RemyxCodeExecutor(arxiv_id=\"2508.06434v1\")\n",
        "executor_agent, writer_agent = executor.create_agents(\n",
        "    goal=\"Custom exploration\",\n",
        "    llm_model=\"gpt-4o\"\n",
        ")\n",
        "\n",
        "# Customize the chat\n",
        "result = executor_agent.initiate_chat(\n",
        "    writer_agent,\n",
        "    message=\"Custom starting message\",\n",
        "    max_turns=10\n",
        ")"
      ],
      "metadata": {
        "id": "vJNip66YeC9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tips & Tricks\n"
      ],
      "metadata": {
        "id": "KyCvKrGEevXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Start with Search**\n",
        "\n",
        "You can quickly browse the catalog of pre-built images for papers you may want to experiment. Search papers and prebuilt Docker images using full text, keywords, or arXiv IDs."
      ],
      "metadata": {
        "id": "D4tjWWa-ezk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from remyxai.client.search import SearchClient\n",
        "\n",
        "papers = SearchClient().search(\n",
        "    query=\"data synthesis techniques\",\n",
        "    has_docker=True,\n",
        "    max_results=10\n",
        ")\n",
        "\n",
        "for p in papers:\n",
        "    print(f\"{p.arxiv_id}: {p.title[:50]}...\")"
      ],
      "metadata": {
        "id": "EkMp01FBhbV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Interactive Mode for Learning**\n",
        "\n",
        "Pause at each step to guide the agents in your exploration"
      ],
      "metadata": {
        "id": "lu1irZGfhpEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "executor.explore(\n",
        "    goal=\"Explain this paper's approach\",\n",
        "    interactive=True  # Lets you guide each step\n",
        ")"
      ],
      "metadata": {
        "id": "9VL70RYWhwqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Batch Mode for Experiments**\n",
        "\n",
        "Expand your experimentation by running multiple papers automatically:"
      ],
      "metadata": {
        "id": "nlPhOC0Bh076"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paper_ids = [\"2508.06434v1\", \"2103.00020v1\", \"2010.11929v2\"]\n",
        "\n",
        "results = {}\n",
        "for arxiv_id in paper_ids:\n",
        "    executor = RemyxCodeExecutor(arxiv_id=arxiv_id)\n",
        "    result = executor.explore(\n",
        "        goal=\"Run quickstart\",\n",
        "        interactive=False,\n",
        "        verbose=False\n",
        "    )\n",
        "    results[arxiv_id] = result\n",
        "\n",
        "# Compare results\n",
        "for arxiv_id, result in results.items():\n",
        "    print(f\"{arxiv_id}: {len(result.chat_history)} steps\")"
      ],
      "metadata": {
        "id": "IV74ihneh_1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check Metadata**\n",
        "\n",
        "Get a quick summary of all the available resources for a paper you may be interested in exploring further"
      ],
      "metadata": {
        "id": "nzpRpsotiOgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "executor = RemyxCodeExecutor(arxiv_id=\"2508.06434v1\")\n",
        "print(executor.get_paper_context())\n",
        "# Shows: title, GitHub, working directory, quickstart hints"
      ],
      "metadata": {
        "id": "z7PBB-02iOzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This notebook showed you how RemyxCodeExecutor transforms research paper execution:\n",
        "\n",
        "**Three Powerful Modes:**\n",
        "-  **Quick Start**: `executor.explore()` - AI-guided exploration with defaults\n",
        "-  **Learning Mode**: Interactive step-by-step with custom goals\n",
        "-  **Batch Mode**: Automated experiments across multiple papers\n",
        "\n",
        "**What Makes It Special:**\n",
        "- Pre-configured Docker environments for 1000+ papers\n",
        "- Zero dependency setup (everything pre-installed)\n",
        "- AI agents that explain as they explore\n",
        "- Reproducible execution every time\n",
        "\n",
        "\n",
        "### Quick Reference\n",
        "```python\n",
        "# 1. Search\n",
        "from remyxai.client.search import SearchClient\n",
        "papers = SearchClient().search(\"your topic\", has_docker=True)\n",
        "\n",
        "# 2. Create executor\n",
        "from autogen.coding import RemyxCodeExecutor\n",
        "executor = RemyxCodeExecutor(arxiv_id=papers[0].arxiv_id)\n",
        "\n",
        "# 3. Explore (pick one mode)\n",
        "executor.explore()                                   # Quick start\n",
        "executor.explore(goal=\"...\", interactive=True)       # Learning\n",
        "executor.explore(goal=\"...\", interactive=False)      # Batch"
      ],
      "metadata": {
        "id": "Gv79n3O5jwei"
      }
    }
  ]
}