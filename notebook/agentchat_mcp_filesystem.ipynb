{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a339a735-1dd5-48b4-9d3a-d050e5d54d77",
   "metadata": {},
   "source": [
    "# Conversational Workflows with MCP: A Marie Antoinette Take on The Eiffel Tower\n",
    "\n",
    "**Authors:** Licong Xu and Boris Bolliet (Cambridge)\n",
    "\n",
    "**Original Code:** [MCPAgents](https://github.com/CMBAgents/MCPAgents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5950d-698d-4619-abb9-6f1942820416",
   "metadata": {},
   "source": [
    "## Imports and Setup\n",
    "\n",
    "This cell sets up the environment and imports necessary libraries:\n",
    "\n",
    "- **Pathlib**, **os**, **json**, **asyncio**, etc. for file and system operations.\n",
    "- Imports from the `autogen` and `mcp` libraries, which are used to create conversational agents and connect to a filesystem-related tool server.\n",
    "- `nest_asyncio.apply()` ensures that asynchronous code runs properly in Jupyter notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ed7aaa-e6c9-42c4-bad6-b5c16176ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.sse import sse_client\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from autogen import LLMConfig\n",
    "from autogen.agentchat import AssistantAgent\n",
    "from autogen.mcp import create_toolkit\n",
    "import json\n",
    "import anyio\n",
    "import asyncio\n",
    "\n",
    "# Only needed for Jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from autogen.agentchat.group import (\n",
    "    AgentNameTarget,\n",
    "    AgentTarget,\n",
    "    AskUserTarget,\n",
    "    ContextExpression,\n",
    "    ContextStr,\n",
    "    ContextStrLLMCondition,\n",
    "    ContextVariables,\n",
    "    ExpressionAvailableCondition,\n",
    "    ExpressionContextCondition,\n",
    "    GroupChatConfig,\n",
    "    GroupChatTarget,\n",
    "    Handoffs,\n",
    "    NestedChatTarget,\n",
    "    OnCondition,\n",
    "    OnContextCondition,\n",
    "    ReplyResult,\n",
    "    RevertToUserTarget,\n",
    "    SpeakerSelectionResult,\n",
    "    StayTarget,\n",
    "    StringAvailableCondition,\n",
    "    StringContextCondition,\n",
    "    StringLLMCondition,\n",
    "    TerminateTarget,\n",
    ")\n",
    "\n",
    "from autogen.agentchat.group.patterns import (\n",
    "    DefaultPattern,\n",
    "    ManualPattern,\n",
    "    AutoPattern,\n",
    "    RandomPattern,\n",
    "    RoundRobinPattern,\n",
    ")\n",
    "\n",
    "\n",
    "from autogen import ConversableAgent, UpdateSystemMessage\n",
    "from autogen.agents.experimental import DocAgent\n",
    "import os\n",
    "import copy\n",
    "from typing import Any, Dict, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from autogen.agentchat import initiate_group_chat, a_initiate_group_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09044777-9b1b-47a9-abfb-dfebb19f323a",
   "metadata": {},
   "source": [
    "## Define MCP Server Path\n",
    "\n",
    "Set the path to the **MCP server script**, which will be used to handle tool execution related to the filesystem retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "522f8e10-d797-42f1-a3c5-ff0473f82573",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_server_path = Path(\"mcp/mcp_filesystem.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c8d4e-3c07-411a-aa02-ad195c927486",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "- Define the **joker agent**, whose role is to make jokes in the style of Marie Antoinette.\n",
    "- Use a `pydantic` model to structure the joke and explanation.\n",
    "- Set LLM configuration, including temperature, caching, and model.\n",
    "- Define a `ContextVariables` object to inject context (like joke constraints) into the agent's workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d5d303-7256-4507-8c2f-6eff59e4085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joker_message = \"\"\"\n",
    "You are the joker in the team. You make jokes. \n",
    "\n",
    "You must obey the following constraints:\n",
    "\n",
    "{joke_constraints}\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class JokeResponse(BaseModel):\n",
    "    joke_instructions: str = Field(..., description=\"instruction, not in the style of Marie Antoinette\")     \n",
    "    joke: str = Field(..., description=\"The joke. The joke must be in the style of Marie Antoinette and mention Louis the XIVth.\")\n",
    "    joke_explanation: str = Field(..., description=\"explanation, not in the style of Marie Antoinette\")\n",
    "    def format(self) -> str:\n",
    "        return \"\\n\".join([\n",
    "            \"**Joke instructions:**\",\n",
    "            \"\",\n",
    "            self.joke_instructions,\n",
    "            \"\",\n",
    "            \"**Joke:**\",\n",
    "            \"\",\n",
    "            self.joke,\n",
    "            \"\",\n",
    "            \"**Joke explanation:**\",\n",
    "            \"\",\n",
    "            self.joke_explanation\n",
    "        ])\n",
    "\n",
    "\n",
    "default_llm_config = {'cache_seed': 42,\n",
    "                     'temperature': 1.,\n",
    "                     'top_p': 0.05,\n",
    "                     'config_list': [{'model': 'gpt-4o',\n",
    "                                      'api_key': os.getenv('OPENAI_API_KEY'),\n",
    "                                      'api_type': 'openai'}],\n",
    "                     'timeout': 1200}\n",
    "\n",
    "joker_config_list = copy.deepcopy(default_llm_config)\n",
    "joker_config_list['config_list'][0]['response_format'] = JokeResponse\n",
    "\n",
    "\n",
    "joker =  ConversableAgent(\n",
    "    name=\"joker\",\n",
    "    system_message=joker_message,\n",
    "    llm_config = joker_config_list,\n",
    "    update_agent_state_before_reply=[UpdateSystemMessage(joker_message),],\n",
    ")\n",
    "\n",
    "workflow_context = ContextVariables(data={\n",
    "    \"joke_constraints\": \"the joke should make use of the contextual information passed on to you. It should be a paragraph long and use as much detailed information from the context as possible.\",\n",
    "})\n",
    "\n",
    "\n",
    "task = \"\"\"\n",
    "Read the file in context and Make a joke.\n",
    "\"\"\"\n",
    "\n",
    "initial_agent = joker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3846b82-3954-4615-8d51-92f43f25f614",
   "metadata": {},
   "source": [
    "## Create Toolkit and Run\n",
    "\n",
    "1. Asynchronously create a **toolkit** from the client session and registers it to a `mcp_agent` that will search and load local files.\n",
    "2. Set up a **handoff**: once `mcp_agent` finishes its task, it passes control to `joker`.\n",
    "3. Delete the `.cache/` folder to reset the environment.\n",
    "4. Initialize a **DefaultPattern** for how agents interact.\n",
    "5. Start the **group chat workflow** using `a_initiate_group_chat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1baf8175-01ce-48bf-b9f3-1f6bfcaa0b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all agents reset\n",
      ".cache folder deleted.\n",
      "\u001b[33m_User\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "Read the file in context and Make a joke.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_TbJ1xH4GbNfiIG3xi6z7wTMe): list_files *****\u001b[0m\n",
      "Arguments: \n",
      "{\"relative_path\":\"\"}\n",
      "\u001b[32m***************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION list_files...\n",
      "Call ID: call_TbJ1xH4GbNfiIG3xi6z7wTMe\n",
      "Input arguments: {'relative_path': ''}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_TbJ1xH4GbNfiIG3xi6z7wTMe) *****\u001b[0m\n",
      "(['.DS_Store', 'eiffel_tower.md', '.ipynb_checkpoints'], None)\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_roZjh62loxJNDf52L2Ld9lfa): read_file *****\u001b[0m\n",
      "Arguments: \n",
      "{\"relative_path\":\"eiffel_tower.md\"}\n",
      "\u001b[32m**************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION read_file...\n",
      "Call ID: call_roZjh62loxJNDf52L2Ld9lfa\n",
      "Input arguments: {'relative_path': 'eiffel_tower.md'}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_roZjh62loxJNDf52L2Ld9lfa) *****\u001b[0m\n",
      "('Rising above the left bank of the\\u202fRiver\\u202fSeine, the Eiffel\\u202fTower is both a product of late‑19th‑century engineering daring and a symbol of modern Paris.\\n\\n### Conception and Construction (1886\\u202f–\\u202f1889)\\n\\n* **Origins:** Paris needed a spectacular centerpiece for the 1889 *Exposition\\u202fUniverselle*, held to mark the centennial of the French Revolution. The government announced an open competition for a 300‑meter iron tower to be erected on the Champ‑de‑Mars, the broad field that slopes gently toward the Seine.\\n* **Gustave Eiffel’s design:** Engineer Gustave Eiffel and collaborators Maurice Koechlin, Émile Nouguier (structural), and architect Stephen Sauvestre submitted the winning proposal. Their plan combined wrought‑iron lattices—light yet incredibly strong—curving inwards to reduce wind loads while framing dramatic views of the nearby river.\\n* **Building the giant:** Foundations were sunk in January\\u202f1887 only a few dozen meters from the Seine’s embankment. Prefabricated iron pieces—18,038 of them—were riveted together like an enormous Meccano set. Two years, two months, and five days later, on 31\\u202fMarch\\u202f1889, Eiffel ascended the tower’s top to plant the French flag.\\n\\n### Early Reputation and Uses (1889\\u202f–\\u202f1914)\\n\\n* **Mixed reception:** Many artists and writers decried it as a “metal asparagus,” but fairgoers loved the panoramic view over Paris and its sinuous river. In its first year the tower drew almost two million visitors, proving its commercial worth.\\n* **Scientific platform:** Eiffel championed practical uses to prevent demolition after the exposition’s 20‑year lease. Meteorological and aerodynamic experiments began almost immediately; by 1903 the tower hosted one of the world’s first public radio transmitters, exploiting its height above the Seine basin for clear signals.\\n\\n### World Wars and Modernization (1914\\u202f–\\u202f1960s)\\n\\n* **Strategic mast:** During World\\u202fWar\\u202fI, wireless equipment on the tower jammed German communications and relayed orders to the front.\\n* **Aviation beacon:** Between the wars the tower’s red‑and‑white rotating beacon became a nighttime guide for pilots following the Seine corridor into Paris.\\n* **Post‑war renewal:** In 1957 a 24‑meter television antenna lifted the structure’s total height to 324\\u202fm (1,063\\u202fft). The elevators—originally hydraulic and powered by water pumped from the Seine—were gradually electrified and modernized.\\n\\n### The Late‑20th Century to Today\\n\\n* **Preservation efforts:** Extensive repainting (every seven years) guards against corrosion from the river’s humid breezes. Major refurbishments for the tower’s centenary (1989) and the millennium improved visitor flow, while a 2013 glass‑floor section on the first level lets guests look straight down to the esplanade bordering the Seine.\\n* **Cultural icon:** Featured in countless films and illuminated nightly, the Eiffel\\u202fTower now hosts about seven million visitors each year. Its silhouette—mirrored in the Seine below—remains shorthand worldwide for Parisian elegance and French technological flair.\\n\\n### Quick Facts\\n\\n| Key Date | Milestone                                                             |\\n| -------- | --------------------------------------------------------------------- |\\n| 1889     | Official opening for the Exposition Universelle                       |\\n| 1909     | City of Paris decides to keep the tower (radio value)                 |\\n| 1914     | First military radio interception from the tower                      |\\n| 1957     | TV antenna installed; height becomes 324\\u202fm                            |\\n| 1986‑87  | Elevators and lighting overhauled                                     |\\n| 2024     | Latest repaint begins, still using the signature “Eiffel\\u202fTower brown” |\\n\\nFrom its footings at the Seine’s edge to the beacon that still sweeps the Paris sky, the Eiffel\\u202fTower embodies the industrial age’s optimism and the enduring romance of the river that flows beside it.\\n', None)\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mmcp_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_2FsDD3QRCLVYs4BffL4qBOYW): transfer_to_joker_1 *****\u001b[0m\n",
      "Arguments: \n",
      "{}\n",
      "\u001b[32m************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION transfer_to_joker_1...\n",
      "Call ID: call_2FsDD3QRCLVYs4BffL4qBOYW\n",
      "Input arguments: {}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_2FsDD3QRCLVYs4BffL4qBOYW) *****\u001b[0m\n",
      "Transfer to joker\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mjoker\u001b[0m (to chat_manager):\n",
      "\n",
      "**Joke instructions:**\n",
      "\n",
      "Make a joke about the Eiffel Tower in the style of Marie Antoinette, mentioning Louis the XIVth.\n",
      "\n",
      "**Joke:**\n",
      "\n",
      "Ah, the Eiffel Tower, a marvel of iron and rivets, rising majestically above the Seine like a grandiose cake stand for the city of Paris! One can only imagine Louis the XIVth gazing upon it and exclaiming, \"Let them eat croissants!\" as he mistook it for a giant pastry display. After all, with its intricate lattice work, it does resemble a delicate confection, albeit one that would require a rather large oven. And just like Louis' penchant for grandeur, the tower stands tall, a testament to the French flair for turning even the most industrial of structures into a symbol of elegance and style. One might say, if Louis were alive today, he'd probably want to hang chandeliers from it!\n",
      "\n",
      "**Joke explanation:**\n",
      "\n",
      "The joke plays on the historical context of the Eiffel Tower as a symbol of French elegance and engineering, much like the opulence associated with Louis the XIVth's reign. The reference to \"Let them eat croissants\" is a humorous twist on the famous (though misattributed) quote \"Let them eat cake,\" often associated with Marie Antoinette, to tie in the theme of French pastries and grandeur. The joke imagines Louis XIV mistaking the Eiffel Tower for a giant pastry display, highlighting the tower's intricate design and the French love for turning functional structures into artistic statements.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (0cbcee05-9ece-45b1-be04-780178c9c681): No next speaker selected\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async def create_toolkit_and_run(session: ClientSession) -> None:\n",
    "    # Create a toolkit with available MCP tools\n",
    "    toolkit = await create_toolkit(session=session)\n",
    "    mcp_agent = ConversableAgent(name=\"mcp_agent\", \n",
    "                             system_message=r\"\"\"\n",
    "Read the file in your folder. \n",
    "                             \"\"\",\n",
    "                             llm_config=LLMConfig(model=\"gpt-4o\", \n",
    "                                                  api_type=\"openai\",\n",
    "                                                  tool_choice=\"required\"\n",
    "                                                 ))\n",
    "    # Register MCP tools with the agent\n",
    "    toolkit.register_for_llm(mcp_agent)\n",
    "    \n",
    "    toolkit.register_for_execution(mcp_agent)\n",
    "\n",
    "    # joker.handoffs.set_after_work(AgentTarget(mcp_agent))\n",
    "    joker.handoffs.set_after_work(TerminateTarget())\n",
    "    \n",
    "    mcp_agent.handoffs.set_after_work(AgentTarget(joker))\n",
    "\n",
    "\n",
    "    mcp_agent.handoffs.add_llm_conditions([\n",
    "            OnCondition(\n",
    "                target=AgentTarget(joker),\n",
    "                condition=StringLLMCondition(prompt=\"The file has been read.\"),            ),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    agents=[joker,\n",
    "            mcp_agent,\n",
    "               ]\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent.reset()\n",
    "    print(\"all agents reset\")\n",
    "\n",
    "    import shutil\n",
    "    import os\n",
    "    \n",
    "    def delete_cache_folder():\n",
    "        cache_path = os.path.join(os.getcwd(), \".cache\")\n",
    "        if os.path.isdir(cache_path):\n",
    "            shutil.rmtree(cache_path)\n",
    "            print(\".cache folder deleted.\")\n",
    "        else:\n",
    "            print(\"No .cache folder found in current directory.\")\n",
    "    \n",
    "    delete_cache_folder()\n",
    "\n",
    "    # Create the pattern\n",
    "    agent_pattern = DefaultPattern(\n",
    "      agents=[joker, mcp_agent],\n",
    "      initial_agent=mcp_agent,\n",
    "      context_variables=workflow_context,\n",
    "    )\n",
    "    \n",
    "\n",
    "    await a_initiate_group_chat(\n",
    "            pattern=agent_pattern,\n",
    "            messages=task,\n",
    "            max_rounds=20,\n",
    "        )\n",
    "\n",
    "\n",
    "# Create server parameters for stdio connection\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",  # The command to run the server\n",
    "    args=[\n",
    "        str(mcp_server_path),\n",
    "        \"stdio\",\n",
    "        \"--context-path\", \"mcp/context_docs\"\n",
    "    ],  # Path to server script and transport mode\n",
    ")\n",
    "\n",
    "async with stdio_client(server_params) as (read, write), ClientSession(read, write) as session:\n",
    "    # Initialize the connection\n",
    "    await session.initialize()\n",
    "    await create_toolkit_and_run(session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp_env",
   "language": "python",
   "name": "mcp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
