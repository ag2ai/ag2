{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic V2 Client with AG2 Agents\n",
    "\n",
    "Author: [Priyanshu Deshmukh](https://github.com/priyansh4320)\n",
    "\n",
    "This notebook demonstrates how to use the **Anthropic V2 Client** (`api_type: \"anthropic_v2\"`) with AG2's agent system. The V2 client provides:\n",
    "\n",
    "- **Rich UnifiedResponse objects**: Typed content blocks (TextContent, ReasoningContent, ToolCallContent, etc.)\n",
    "- **Structured Outputs**: Guaranteed schema-compliant JSON responses\n",
    "- **Strict Tool Use**: Type-safe function calls with guaranteed schema validation\n",
    "- **Vision Support**: Full multimodal capabilities with image input\n",
    "- **Forward Compatibility**: GenericContent handles unknown future content types\n",
    "\n",
    "## What is Anthropic V2 Client?\n",
    "\n",
    "The V2 client implements both `ModelClient` and `ModelClientV2` protocols, returning rich `UnifiedResponse` objects with:\n",
    "\n",
    "- **Typed content blocks**: `TextContent`, `ReasoningContent`, `ToolCallContent`, `CitationContent`\n",
    "- **Structured outputs**: Native support for Pydantic models and JSON schemas\n",
    "- **Strict tools**: Guaranteed schema validation for tool inputs\n",
    "- **Rich metadata**: Full reasoning blocks, citations, and tool execution details\n",
    "- **Type safety**: Pydantic validation for all response data\n",
    "- **Cost tracking**: Automatic per-response cost calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "h\n",
    "pip install ag2[anthropic]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 3\n",
      "python-dotenv could not parse statement starting at line 4\n",
      "python-dotenv could not parse statement starting at line 5\n",
      "python-dotenv could not parse statement starting at line 8\n",
      "python-dotenv could not parse statement starting at line 9\n",
      "python-dotenv could not parse statement starting at line 10\n",
      "python-dotenv could not parse statement starting at line 11\n",
      "python-dotenv could not parse statement starting at line 12\n",
      "python-dotenv could not parse statement starting at line 38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import textwrap\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "from autogen.io.run_response import Cost\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Helper function to extract total cost from ChatResult.cost dict\n",
    "def get_total_cost(cost_dict):\n",
    "    \"\"\"Extract total cost from ChatResult.cost dict structure.\"\"\"\n",
    "    total = 0.0\n",
    "    for usage_type in cost_dict.values():\n",
    "        if isinstance(usage_type, dict):\n",
    "            for model_usage in usage_type.values():\n",
    "                if isinstance(model_usage, dict) and \"cost\" in model_usage:\n",
    "                    total += model_usage[\"cost\"]\n",
    "    return total\n",
    "\n",
    "\n",
    "# Helper function to extract cost from run response\n",
    "def get_total_cost_from_run(run_response_cost):\n",
    "    \"\"\"Extract total cost from run response object.\"\"\"\n",
    "    if isinstance(run_response_cost, Cost):\n",
    "        return run_response_cost.usage_including_cached_inference.total_cost\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "print(\"✅ Environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Structured Outputs\n",
    "\n",
    "Anthropic's structured outputs feature provides two powerful modes:\n",
    "\n",
    "1. **JSON Outputs** (`response_format`): Get validated JSON responses matching a specific schema\n",
    "2. **Strict Tool Use** (`strict: true`): Guaranteed schema validation for tool inputs\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- **Always Valid**: No more `JSON.parse()` errors\n",
    "- **Type Safe**: Guaranteed field types and required fields\n",
    "- **Reliable**: No retries needed for schema violations\n",
    "- **Dual Modes**: JSON for data extraction, strict tools for agentic workflows\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Claude Sonnet 4.5 (`claude-sonnet-4-5`) or Claude Opus 4.1 (`claude-opus-4-1`)\n",
    "- Anthropic SDK >= 0.74.1\n",
    "- Beta header: `structured-outputs-2025-11-13` (automatically applied by AG2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: JSON Structured Outputs with Pydantic Models\n",
    "\n",
    "The most common use case is extracting structured data from unstructured text. We'll use Pydantic models to define our schema and get validated JSON responses.\n",
    "\n",
    "### Use Case: Mathematical Reasoning\n",
    "\n",
    "Let's create an agent that solves math problems and returns structured step-by-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structured output schema using Pydantic\n",
    "class Step(BaseModel):\n",
    "    \"\"\"A single step in mathematical reasoning.\"\"\"\n",
    "\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    \"\"\"Structured output for mathematical problem solving.\"\"\"\n",
    "\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "\n",
    "    def format(self) -> str:\n",
    "        \"\"\"Format the response for display.\"\"\"\n",
    "        steps_output = \"\\n\".join(\n",
    "            f\"Step {i + 1}: {step.explanation}\\n  Output: {step.output}\" for i, step in enumerate(self.steps)\n",
    "        )\n",
    "        return f\"{steps_output}\\n\\nFinal Answer: {self.final_answer}\"\n",
    "\n",
    "\n",
    "# Configure LLM with structured output using V2 client\n",
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"claude-sonnet-4-5\",\n",
    "            \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "            \"api_type\": \"anthropic_v2\",  # <-- Use V2 client\n",
    "            \"response_format\": MathReasoning,  # Enable structured outputs\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Create agents\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=0,\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "math_assistant = AssistantAgent(\n",
    "    name=\"MathAssistant\",\n",
    "    system_message=\"You are a math tutor. Solve problems step by step.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "print(\"✅ Example 1 configured: Math reasoning with structured outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the assistant to solve a math problem\n",
    "chat_result = user_proxy.run(\n",
    "    math_assistant,\n",
    "    message=\"Solve the equation: 3x + 7 = 22\",\n",
    "    max_turns=1,\n",
    ")\n",
    "\n",
    "chat_result.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How It Works\n",
    "\n",
    "1. **Schema Definition**: Pydantic models define the expected structure\n",
    "2. **Beta API**: AG2 automatically uses `beta.messages.parse()` for Pydantic models\n",
    "3. **Constrained Decoding**: Claude generates output that strictly follows the schema\n",
    "4. **FormatterProtocol**: If your model has a `format()` method, it's automatically called\n",
    "\n",
    "**Benefits**:\n",
    "- ✅ No JSON parsing errors\n",
    "- ✅ Guaranteed schema compliance\n",
    "- ✅ Type-safe field access\n",
    "- ✅ Custom formatting support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Strict Tool Use for Type-Safe Function Calls\n",
    "\n",
    "Strict tool use ensures that Claude's tool inputs exactly match your schema. This is critical for production agentic systems where invalid parameters can break workflows.\n",
    "\n",
    "### Use Case: Weather API with Validated Inputs\n",
    "\n",
    "Without strict mode, Claude might return `\"celsius\"` as a string when you expect an enum, or `\"2\"` instead of `2`. Strict mode guarantees correct types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool function\n",
    "def get_weather(location: str, unit: str = \"celsius\") -> str:\n",
    "    \"\"\"Get the weather for a location.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state, e.g. San Francisco, CA\n",
    "        unit: Temperature unit (celsius or fahrenheit)\n",
    "    \"\"\"\n",
    "    # In a real application, this would call a weather API\n",
    "    return f\"Weather in {location}: 22°{unit.upper()[0]}, partly cloudy\"\n",
    "\n",
    "\n",
    "# Configure LLM with strict tool using V2 client\n",
    "llm_config_strict = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"claude-sonnet-4-5\",\n",
    "            \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "            \"api_type\": \"anthropic_v2\",  # <-- Use V2 client\n",
    "        }\n",
    "    ],\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the weather for a location\",\n",
    "            \"strict\": True,  # Enable strict schema validation ✨\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"},\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"Temperature unit\"},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Create agents\n",
    "weather_assistant = AssistantAgent(\n",
    "    name=\"WeatherAssistant\",\n",
    "    system_message=\"You help users get weather information. Use the get_weather function.\",\n",
    "    llm_config=llm_config_strict,\n",
    ")\n",
    "\n",
    "user_proxy_2 = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=1,\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "# Register function on both agents\n",
    "weather_assistant.register_function({\"get_weather\": get_weather})\n",
    "user_proxy_2.register_function({\"get_weather\": get_weather})\n",
    "\n",
    "print(\"✅ Example 2 configured: Strict tool use for weather queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the weather\n",
    "chat_result = user_proxy_2.initiate_chat(\n",
    "    weather_assistant,\n",
    "    message=\"What's the weather in Boston, MA?\",\n",
    "    max_turns=2,\n",
    ")\n",
    "\n",
    "# Verify tool call had strict typing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOOL CALL VERIFICATION:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import json\n",
    "\n",
    "for message in chat_result.chat_history:\n",
    "    if message.get(\"tool_calls\"):\n",
    "        tool_call = message[\"tool_calls\"][0]\n",
    "        args = json.loads(tool_call[\"function\"][\"arguments\"])\n",
    "        print(f\"Function: {tool_call['function']['name']}\")\n",
    "        print(f\"Arguments: {args}\")\n",
    "        print(f\"✅ location type: {type(args['location']).__name__}\")\n",
    "        if \"unit\" in args:\n",
    "            print(f\"✅ unit value: {args['unit']} (valid enum)\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Combined JSON Outputs + Strict Tools\n",
    "\n",
    "The most powerful pattern is combining both features: use strict tools for calculations/actions, then return structured JSON for the final result.\n",
    "\n",
    "### Use Case: Math Calculator Agent\n",
    "\n",
    "The agent uses strict tools to perform calculations (guaranteed correct types), then provides a structured summary of the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define calculator tool\n",
    "def calculate(operation: str, a: float, b: float) -> float:\n",
    "    \"\"\"Perform a calculation.\n",
    "\n",
    "    Args:\n",
    "        operation: The operation to perform (add, subtract, multiply, divide)\n",
    "        a: First number\n",
    "        b: Second number\n",
    "    \"\"\"\n",
    "    if operation == \"add\":\n",
    "        return a + b\n",
    "    elif operation == \"subtract\":\n",
    "        return a - b\n",
    "    elif operation == \"multiply\":\n",
    "        return a * b\n",
    "    elif operation == \"divide\":\n",
    "        return a / b if b != 0 else 0\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Result model for structured output\n",
    "class CalculationResult(BaseModel):\n",
    "    \"\"\"Structured output for calculation results.\"\"\"\n",
    "\n",
    "    problem: str\n",
    "    steps: list[str]\n",
    "    result: float\n",
    "    verification: str\n",
    "\n",
    "\n",
    "# Configure with BOTH features using V2 client\n",
    "llm_config_combined = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"claude-sonnet-4-5\",\n",
    "            \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "            \"api_type\": \"anthropic_v2\",  # <-- Use V2 client\n",
    "            \"response_format\": CalculationResult,  # 1. Structured JSON output\n",
    "        }\n",
    "    ],\n",
    "    \"functions\": [\n",
    "        {  # 2. Strict tool validation\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Perform arithmetic calculation\",\n",
    "            \"strict\": True,  # Enable strict mode\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"operation\": {\"type\": \"string\", \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"]},\n",
    "                    \"a\": {\"type\": \"number\"},\n",
    "                    \"b\": {\"type\": \"number\"},\n",
    "                },\n",
    "                \"required\": [\"operation\", \"a\", \"b\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Create agents\n",
    "calc_assistant = AssistantAgent(\n",
    "    name=\"MathAssistant\",\n",
    "    system_message=\"You solve math problems using tools and provide structured results.\",\n",
    "    llm_config=llm_config_combined,\n",
    ")\n",
    "\n",
    "user_proxy_3 = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "# Register function on both agents\n",
    "calc_assistant.register_function({\"calculate\": calculate})\n",
    "user_proxy_3.register_function({\"calculate\": calculate})\n",
    "\n",
    "print(\"✅ Example 3 configured: Combined strict tools + structured output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result = user_proxy_3.run(\n",
    "    calc_assistant,\n",
    "    message=\"add 3 and 555\",\n",
    "    max_turns=2,\n",
    ")\n",
    "chat_result.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Vision and Image Input\n",
    "\n",
    "The V2 client also supports full multimodal capabilities with image input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Simple Image Description\n",
    "\n",
    "Using formal image input format to reduce hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vision assistant with V2 client created\n"
     ]
    }
   ],
   "source": [
    "# Configure LLM to use V2 client for vision\n",
    "llm_config_vision = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"api_type\": \"anthropic_v2\",  # <-- Key: use V2 client architecture\n",
    "            \"model\": \"claude-3-5-haiku-20241022\",  # Vision-capable model\n",
    "            \"api_key\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0.3,\n",
    "}\n",
    "\n",
    "# Create vision assistant\n",
    "vision_assistant = AssistantAgent(\n",
    "    name=\"VisionBot\",\n",
    "    llm_config=llm_config_vision,\n",
    "    system_message=textwrap.dedent(\"\"\"\n",
    "        You are an AI assistant with vision capabilities.\n",
    "        You can analyze images and provide detailed, accurate descriptions.\n",
    "    \"\"\").strip(),\n",
    ")\n",
    "\n",
    "# Create user proxy\n",
    "user_proxy_vision = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=0,\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "# Test image URL\n",
    "IMAGE_URL = \"https://upload.wikimedia.org/wikipedia/commons/3/3b/BlkStdSchnauzer2.jpg\"\n",
    "\n",
    "print(\"✅ Vision assistant with V2 client created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to VisionBot):\n",
      "\n",
      "Describe this image in one sentence.\n",
      "<image>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mVisionBot\u001b[0m (to User):\n",
      "\n",
      "A black, shaggy-coated Schnauzer dog stands alertly in a grassy field with an upright, sturdy posture.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (dcec32e4-c50c-4d89-a384-7857bdf09444): Maximum turns (1) reached\u001b[0m\n",
      "\n",
      "=== Response ===\n",
      "A black, shaggy-coated Schnauzer dog stands alertly in a grassy field with an upright, sturdy posture.\n",
      "\n",
      "Cost: $0.0007\n"
     ]
    }
   ],
   "source": [
    "# Formal image input format (recommended)\n",
    "message_with_image = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image in one sentence.\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": IMAGE_URL}},\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Initiate chat with image\n",
    "chat_result = user_proxy_vision.initiate_chat(\n",
    "    vision_assistant, message=message_with_image, max_turns=1, summary_method=\"last_msg\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Response ===\")\n",
    "print(chat_result.summary)\n",
    "print(f\"\\nCost: ${get_total_cost(chat_result.cost):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Detailed Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Analyze this image in detail. What breed is this dog? What are its characteristics?\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": IMAGE_URL}},\n",
    "    ],\n",
    "}\n",
    "\n",
    "chat_result = user_proxy_vision.initiate_chat(\n",
    "    vision_assistant,\n",
    "    message=detailed_message,\n",
    "    max_turns=1,\n",
    "    clear_history=True,  # Start fresh conversation\n",
    ")\n",
    "\n",
    "print(chat_result.summary)\n",
    "print(f\"\\nCost: ${get_total_cost(chat_result.cost):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Benefits of Anthropic V2 Client\n",
    "\n",
    "1. **Structured Outputs**: Guaranteed schema-compliant JSON responses\n",
    "2. **Strict Tool Use**: Type-safe function calls with guaranteed validation\n",
    "3. **Vision Support**: Full multimodal capabilities with image input\n",
    "4. **Rich Response Data**: Access to typed content blocks (reasoning, citations, etc.)\n",
    "5. **Cost Tracking**: Automatic per-response cost calculation\n",
    "6. **Type Safety**: Pydantic validation for all response data\n",
    "7. **Forward Compatible**: GenericContent handles unknown future types\n",
    "\n",
    "### Usage Pattern\n",
    "\n",
    "on\n",
    "# Simple: Just change api_type\n",
    "```python\n",
    "llm_config = {\n",
    "    \"config_list\": [{\n",
    "        \"api_type\": \"anthropic_v2\",  # <-- That's it!\n",
    "        \"model\": \"claude-sonnet-4-5\",\n",
    "        \"api_key\": \"...\",\n",
    "        \"response_format\": YourPydanticModel,  # Optional: for structured outputs\n",
    "    }]\n",
    "}\n",
    "\n",
    "assistant = AssistantAgent(llm_config=llm_config)\n",
    "# Everything works as before, but with rich UnifiedResponse internally\n",
    "```\n",
    "\n",
    "### When to Use V2 Client\n",
    "\n",
    "- ✅ Need structured outputs with guaranteed schema compliance\n",
    "- ✅ Want strict tool validation for type-safe function calls\n",
    "- ✅ Working with vision/multimodal models\n",
    "- ✅ Need access to reasoning blocks\n",
    "- ✅ Require rich metadata and citations\n",
    "- ✅ Building systems that need forward compatibility\n",
    "\n",
    "### Migration from Standard Client\n",
    "\n",
    "No code changes needed! Just update `api_type`:\n",
    "\n",
    "\n",
    "# Before\n",
    "{\"model\": \"claude-3-5-sonnet-20241022\", \"api_key\": \"...\"}\n",
    "\n",
    "# After  \n",
    "{\"api_type\": \"anthropic_v2\", \"model\": \"claude-sonnet-4-5\", \"api_key\": \"...\"}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
