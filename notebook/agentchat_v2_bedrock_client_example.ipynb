{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Amazon Bedrock V2 Client with AG2\n",
    "\n",
    "This notebook demonstrates how to use the **Bedrock V2 Client** (ModelClientV2 architecture) with AG2. The V2 client returns rich `UnifiedResponse` objects with typed content blocks, providing better access to multimodal content, tool calls, and provider-specific features.\n",
    "\n",
    "## What is Bedrock V2 Client?\n",
    "\n",
    "The Bedrock V2 client (`api_type: \"bedrock_v2\"`) is the next-generation client architecture that:\n",
    "\n",
    "- **Returns UnifiedResponse**: Rich, provider-agnostic response format with typed content blocks\n",
    "- **Preserves Rich Content**: Text, images, tool calls, and other content types are preserved as typed blocks\n",
    "- **Direct Property Access**: Use `response.text`, `response.get_content_by_type()` instead of parsing nested structures\n",
    "- **Forward Compatible**: Handles unknown content types via `GenericContent`\n",
    "- **Backward Compatible**: Works seamlessly with V1 clients in the same conversation\n",
    "\n",
    "## Key Differences: V1 vs V2\n",
    "\n",
    "| Feature | V1 Client (`api_type: \"bedrock\"`) | V2 Client (`api_type: \"bedrock_v2\"`) |\n",
    "|---------|-----------------------------------|--------------------------------------|\n",
    "| Response Format | `ChatCompletion` (flattened) | `UnifiedResponse` (rich, typed) |\n",
    "| Content Access | `client.message_retrieval(response)` | `response.text`, `response.messages` |\n",
    "| Rich Content | Lost or requires parsing | Preserved as typed blocks |\n",
    "| Tool Calls | Flattened to dict | `ToolCallContent` objects |\n",
    "| Images | Not easily accessible | `ImageContent` objects |\n",
    "| Forward Compatible | Limited | Yes (via `GenericContent`) |\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Python >= 3.10\n",
    "- AG2 installed: `pip install ag2`\n",
    "- `boto3` package: `pip install boto3`\n",
    "- AWS credentials configured (via environment variables, IAM role, or AWS credentials file)\n",
    "- A Bedrock model that supports the features you need (Tool Use, multimodal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ag2 boto3 pydantic --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Configure AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from autogen import ConversableAgent, LLMConfig\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Part 1: Basic Bedrock V2 Client Usage\n",
    "\n",
    "Let's start with a simple example using the Bedrock V2 client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bedrock V2 and V1 configurations created!\n"
     ]
    }
   ],
   "source": [
    "# Configure LLM to use Bedrock V2 client\n",
    "llm_config_v2 = LLMConfig(\n",
    "    config_list=[\n",
    "        {\n",
    "            \"api_type\": \"bedrock_v2\",  # <-- Key: use V2 client architecture\n",
    "            \"model\": \"qwen.qwen3-coder-480b-a35b-v1:0\",\n",
    "            \"aws_region\": os.getenv(\"AWS_REGION\", \"eu-north-1\"),\n",
    "            \"aws_access_key\": os.getenv(\"AWS_ACCESS_KEY\"),\n",
    "            \"aws_secret_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "            \"aws_profile_name\": os.getenv(\"AWS_PROFILE\"),\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Compare with V1 client configuration\n",
    "llm_config_v1 = LLMConfig(\n",
    "    config_list=[\n",
    "        {\n",
    "            \"api_type\": \"bedrock\",  # <-- V1 client architecture\n",
    "            \"model\": \"qwen.qwen3-coder-480b-a35b-v1:0\",\n",
    "            \"aws_region\": os.getenv(\"AWS_REGION\", \"eu-north-1\"),\n",
    "            \"aws_access_key\": os.getenv(\"AWS_ACCESS_KEY\"),\n",
    "            \"aws_secret_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "            \"aws_profile_name\": os.getenv(\"AWS_PROFILE\"),\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "print(\"Bedrock V2 and V1 configurations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Part 2: Direct Client Usage - Accessing Rich Responses\n",
    "\n",
    "Let's see how to use the Bedrock V2 client directly to access rich response content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response type: <class 'autogen.llm_clients.models.unified_response.UnifiedResponse'>\n",
      "Is UnifiedResponse: True\n",
      "\n",
      "Provider: bedrock\n",
      "Model: qwen.qwen3-coder-480b-a35b-v1:0\n",
      "\n",
      "Text content: Quantum computing uses the principles of quantum mechanics to process information with quantum bits (qubits) that can exist in multiple states simultaneously, allowing for exponentially faster computations than classical computers for certain problems. While still in early development, quantum computers promise to revolutionize fields like cryptography, drug discovery, and materials science by solving complex mathematical problems that would take classical computers thousands of years to complete.\n",
      "\n",
      "Usage: {'prompt_tokens': 17, 'completion_tokens': 78, 'total_tokens': 95}\n",
      "Cost: N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyanshu/Documents/GitHub/ag2/autogen/oai/bedrock.py:910: UserWarning: Cannot get the costs for qwen.qwen3-coder-480b-a35b-v1:0. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from autogen.llm_clients.bedrock_v2 import BedrockV2Client\n",
    "from autogen.llm_clients.models import UnifiedResponse\n",
    "\n",
    "# Create Bedrock V2 client directly\n",
    "client = BedrockV2Client(\n",
    "    aws_region=os.getenv(\"AWS_REGION\", \"eu-north-1\"),\n",
    "    aws_access_key=os.getenv(\"AWS_ACCESS_KEY\"),\n",
    "    aws_secret_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    ")\n",
    "\n",
    "# Make a request\n",
    "response = client.create({\n",
    "    \"model\": \"qwen.qwen3-coder-480b-a35b-v1:0\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in 2 sentences.\"}],\n",
    "})\n",
    "\n",
    "# Verify it's a UnifiedResponse\n",
    "print(f\"Response type: {type(response)}\")\n",
    "print(f\"Is UnifiedResponse: {isinstance(response, UnifiedResponse)}\")\n",
    "print(f\"\\nProvider: {response.provider}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"\\nText content: {response.text}\")\n",
    "print(f\"\\nUsage: {response.usage}\")\n",
    "print(f\"Cost: ${response.cost:.6f}\" if response.cost else \"Cost: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Part 3: Accessing Content Blocks\n",
    "\n",
    "The V2 client preserves all content as typed blocks. Let's explore the different content types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Message 1:\n",
      "  Role: UserRoleEnum.ASSISTANT\n",
      "  Content blocks: 1\n",
      "\n",
      "  Block 1:\n",
      "    Type: ContentType.TEXT\n",
      "    Class: TextContent\n",
      "    Text: Quantum computing uses the principles of quantum mechanics to process information with quantum bits ...\n",
      "\n",
      "============================================================\n",
      "Using helper methods:\n",
      "All text: Quantum computing uses the principles of quantum mechanics to process information with quantum bits (qubits) that can exist in multiple states simultaneously, allowing for exponentially faster computations than classical computers for certain problems. While still in early development, quantum computers promise to revolutionize fields like cryptography, drug discovery, and materials science by solving complex mathematical problems that would take classical computers thousands of years to complete.\n",
      "Tool calls: 0\n",
      "Text blocks: 1\n"
     ]
    }
   ],
   "source": [
    "# Access individual messages and content blocks\n",
    "for i, message in enumerate(response.messages):\n",
    "    print(f\"\\nMessage {i + 1}:\")\n",
    "    print(f\"  Role: {message.role}\")\n",
    "    print(f\"  Content blocks: {len(message.content)}\")\n",
    "\n",
    "    for j, block in enumerate(message.content):\n",
    "        print(f\"\\n  Block {j + 1}:\")\n",
    "        print(f\"    Type: {block.type}\")\n",
    "        print(f\"    Class: {type(block).__name__}\")\n",
    "\n",
    "        # Access text content\n",
    "        if hasattr(block, \"text\"):\n",
    "            print(f\"    Text: {block.text[:100]}...\" if len(block.text) > 100 else f\"    Text: {block.text}\")\n",
    "\n",
    "        # Access tool calls\n",
    "        if hasattr(block, \"name\"):\n",
    "            print(f\"    Tool: {block.name}\")\n",
    "            print(\n",
    "                f\"    Arguments: {block.arguments[:100]}...\"\n",
    "                if len(block.arguments) > 100\n",
    "                else f\"    Arguments: {block.arguments}\"\n",
    "            )\n",
    "\n",
    "# Use helper methods\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Using helper methods:\")\n",
    "print(f\"All text: {response.text}\")\n",
    "print(f\"Tool calls: {len(response.get_content_by_type('tool_call'))}\")\n",
    "print(f\"Text blocks: {len(response.get_content_by_type('text'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Part 4: Structured Outputs with Bedrock V2\n",
    "\n",
    "Bedrock V2 client supports structured outputs via `response_format`. Let's define a Pydantic model and use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic models defined:\n",
      "- Step: {'description': 'Represents a single step in solving a problem.', 'properties': {'explanation': {'title': 'Explanation', 'type': 'string'}, 'output': {'title': 'Output', 'type': 'string'}}, 'required': ['explanation', 'output'], 'title': 'Step', 'type': 'object'}\n",
      "- ProblemSolution: {'$defs': {'Step': {'description': 'Represents a single step in solving a problem.', 'properties': {'explanation': {'title': 'Explanation', 'type': 'string'}, 'output': {'title': 'Output', 'type': 'string'}}, 'required': ['explanation', 'output'], 'title': 'Step', 'type': 'object'}}, 'description': 'Complete structured response for a problem solution.', 'properties': {'problem': {'title': 'Problem', 'type': 'string'}, 'steps': {'items': {'$ref': '#/$defs/Step'}, 'title': 'Steps', 'type': 'array'}, 'final_answer': {'title': 'Final Answer', 'type': 'string'}, 'confidence': {'anyOf': [{'type': 'number'}, {'type': 'null'}], 'default': None, 'title': 'Confidence'}}, 'required': ['problem', 'steps', 'final_answer'], 'title': 'ProblemSolution', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "# Define structured output model\n",
    "class Step(BaseModel):\n",
    "    \"\"\"Represents a single step in solving a problem.\"\"\"\n",
    "\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "class ProblemSolution(BaseModel):\n",
    "    \"\"\"Complete structured response for a problem solution.\"\"\"\n",
    "\n",
    "    problem: str\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "    confidence: float | None = None\n",
    "\n",
    "    def format(self) -> str:\n",
    "        \"\"\"Format the structured output for human-readable display.\"\"\"\n",
    "        steps_output = \"\\n\".join(\n",
    "            f\"Step {i + 1}: {step.explanation}\\n  Output: {step.output}\" for i, step in enumerate(self.steps)\n",
    "        )\n",
    "        confidence_str = f\" (Confidence: {self.confidence})\" if self.confidence else \"\"\n",
    "        return f\"Problem: {self.problem}\\n\\n{steps_output}\\n\\nFinal Answer: {self.final_answer}{confidence_str}\"\n",
    "\n",
    "\n",
    "print(\"Pydantic models defined:\")\n",
    "print(f\"- Step: {Step.model_json_schema()}\")\n",
    "print(f\"- ProblemSolution: {ProblemSolution.model_json_schema()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent created with Bedrock V2 and structured outputs!\n"
     ]
    }
   ],
   "source": [
    "# Configure Bedrock V2 with structured outputs\n",
    "\n",
    "\n",
    "llm_config_v2_structured = LLMConfig(\n",
    "    config_list={\n",
    "        \"api_type\": \"bedrock_v2\",\n",
    "        \"model\": \"qwen.qwen3-coder-480b-a35b-v1:0\",\n",
    "        \"aws_region\": os.getenv(\"AWS_REGION\", \"eu-north-1\"),\n",
    "        \"aws_access_key\": os.getenv(\"AWS_ACCESS_KEY\"),\n",
    "        \"aws_secret_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        \"total_max_attempts\": 8,\n",
    "        \"mode\": \"adaptive\",  # Retries with client-side throttling\n",
    "        \"response_format\": ProblemSolution,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create agent with structured outputs\n",
    "math_agent = ConversableAgent(\n",
    "    name=\"math_assistant\",\n",
    "    llm_config=llm_config_v2_structured,\n",
    "    system_message=\"You are a helpful math assistant that solves problems step by step. Always show your reasoning process clearly.\",\n",
    "    max_consecutive_auto_reply=1,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "print(\"Agent created with Bedrock V2 and structured outputs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Solving Math Problem with Structured Output ===\n",
      "\u001b[33muser\u001b[0m (to math_assistant):\n",
      "\n",
      "Solve the equation: 3x + 7 = 22. Show all steps.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mmath_assistant\u001b[0m (to user):\n",
      "\n",
      "{\"problem\": \"Solve the equation: 3x + 7 = 22\", \"final_answer\": \"x = 5\", \"steps\": [{\"explanation\": \"Start with the given equation\", \"output\": \"3x + 7 = 22\"}, {\"explanation\": \"Subtract 7 from both sides to isolate the term with x\", \"output\": \"3x + 7 - 7 = 22 - 7\"}, {\"explanation\": \"Simplify both sides\", \"output\": \"3x = 15\"}, {\"explanation\": \"Divide both sides by 3 to solve for x\", \"output\": \"3x \\u00f7 3 = 15 \\u00f7 3\"}, {\"explanation\": \"Simplify to find the value of x\", \"output\": \"x = 5\"}]}\n",
      "\u001b[32m***** Suggested tool call (tooluse_8BkGB1rcpgiua1PictswpK): __structured_output *****\u001b[0m\n",
      "Arguments: \n",
      "{\"problem\": \"Solve the equation: 3x + 7 = 22\", \"final_answer\": \"x = 5\", \"steps\": [{\"explanation\": \"Start with the given equation\", \"output\": \"3x + 7 = 22\"}, {\"explanation\": \"Subtract 7 from both sides to isolate the term with x\", \"output\": \"3x + 7 - 7 = 22 - 7\"}, {\"explanation\": \"Simplify both sides\", \"output\": \"3x = 15\"}, {\"explanation\": \"Divide both sides by 3 to solve for x\", \"output\": \"3x \\u00f7 3 = 15 \\u00f7 3\"}, {\"explanation\": \"Simplify to find the value of x\", \"output\": \"x = 5\"}]}\n",
      "\u001b[32m*************************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (221723eb-01e8-4445-af8c-8dfa9e17982c): Maximum turns (1) reached\u001b[0m\n",
      "\n",
      "Response received!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priyanshu/Documents/GitHub/ag2/autogen/oai/bedrock.py:910: UserWarning: Cannot get the costs for qwen.qwen3-coder-480b-a35b-v1:0. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Test the agent with a math problem\n",
    "print(\"=== Solving Math Problem with Structured Output ===\")\n",
    "\n",
    "result = math_agent.run(\n",
    "    message=\"Solve the equation: 3x + 7 = 22. Show all steps.\",\n",
    "    max_turns=1,\n",
    ").process()\n",
    "\n",
    "print(\"\\nResponse received!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Part 5: V1 vs V2 Client Comparison\n",
    "\n",
    "Let's create agents with both V1 and V2 clients to see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents created with V1 and V2 clients!\n"
     ]
    }
   ],
   "source": [
    "# Create agents with different client versions\n",
    "agent_v2 = ConversableAgent(\n",
    "    name=\"agent_v2\",\n",
    "    llm_config=llm_config_v2,\n",
    "    system_message=\"You are a helpful assistant using V2 client architecture.\",\n",
    "    max_consecutive_auto_reply=1,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "agent_v1 = ConversableAgent(\n",
    "    name=\"agent_v1\",\n",
    "    llm_config=llm_config_v1,\n",
    "    system_message=\"You are a helpful assistant using V1 client architecture.\",\n",
    "    max_consecutive_auto_reply=1,\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "print(\"Agents created with V1 and V2 clients!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test both agents with the same question\n",
    "question = \"What are the three main benefits of renewable energy?\"\n",
    "\n",
    "print(\"=== V2 Client Response ===\")\n",
    "result_v2 = agent_v2.run(message=question, max_turns=1).process()\n",
    "\n",
    "print(\"\\n=== V1 Client Response ===\")\n",
    "result_v1 = agent_v1.run(message=question, max_turns=1).process()\n",
    "\n",
    "print(\"\\nBoth clients work seamlessly with the same interface!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Part 6: Group Chat with Mixed V1/V2 Bedrock Clients\n",
    "\n",
    "Now let's create a group chat where agents use different Bedrock client versions. This demonstrates backward compatibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import GroupChat, GroupChatManager\n",
    "\n",
    "# Planner agent - uses V2 client\n",
    "planner = ConversableAgent(\n",
    "    name=\"planner_agent\",\n",
    "    llm_config=llm_config_v2,\n",
    "    system_message=\"Create detailed project plans. Break down tasks into clear steps.\",\n",
    "    description=\"Creates project plans\",\n",
    ")\n",
    "\n",
    "# Reviewer agent - uses V1 client (demonstrates compatibility)\n",
    "reviewer = ConversableAgent(\n",
    "    name=\"reviewer_agent\",\n",
    "    llm_config=llm_config_v1,\n",
    "    system_message=\"Review plans and provide constructive feedback. Keep reviews concise.\",\n",
    "    description=\"Reviews plans\",\n",
    ")\n",
    "\n",
    "# Coordinator agent - uses V2 client\n",
    "coordinator = ConversableAgent(\n",
    "    name=\"coordinator_agent\",\n",
    "    llm_config=llm_config_v2,\n",
    "    system_message=\"Coordinate between planner and reviewer. Say DONE! when the plan is finalized.\",\n",
    "    description=\"Coordinates the planning process\",\n",
    ")\n",
    "\n",
    "# Setup group chat\n",
    "groupchat = GroupChat(\n",
    "    agents=[coordinator, planner, reviewer],\n",
    "    speaker_selection_method=\"auto\",\n",
    "    messages=[],\n",
    ")\n",
    "\n",
    "# Create manager with V2 client\n",
    "manager = GroupChatManager(\n",
    "    name=\"group_manager\",\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config_v2,\n",
    "    is_termination_msg=lambda x: \"DONE!\" in (x.get(\"content\", \"\") or \"\").upper(),\n",
    ")\n",
    "\n",
    "print(\"Group chat created with mixed V1/V2 Bedrock clients!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the conversation\n",
    "print(\"=== Starting Group Chat ===\")\n",
    "chat_result = coordinator.initiate_chat(\n",
    "    recipient=manager,\n",
    "    message=\"Let's create a plan for organizing a tech conference.\",\n",
    ")\n",
    "\n",
    "print(\"\\n=== Chat History ===\")\n",
    "for msg in chat_result.chat_history:\n",
    "    print(\n",
    "        f\"\\n[{msg.get('role', 'unknown')}]: {msg.get('content', '')[:200]}...\"\n",
    "        if len(msg.get(\"content\", \"\")) > 200\n",
    "        else f\"\\n[{msg.get('role', 'unknown')}]: {msg.get('content', '')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Part 7: Advanced Group Chat with Structured Outputs\n",
    "\n",
    "Let's create a more sophisticated group chat where the orchestrator uses structured outputs for routing decisions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define structured output models for orchestration\n",
    "class TaskDetails(BaseModel):\n",
    "    \"\"\"Details about the task being processed.\"\"\"\n",
    "\n",
    "    task_type: str\n",
    "    description: str\n",
    "    priority: str | None = None\n",
    "    requirements: list[str] = []\n",
    "\n",
    "\n",
    "class RoutingDecision(BaseModel):\n",
    "    \"\"\"Structured routing decision from the orchestrator.\"\"\"\n",
    "\n",
    "    request_analysis: str\n",
    "    task_details: TaskDetails\n",
    "    selected_agent: str\n",
    "    routing_reason: str\n",
    "    expected_outcome: str\n",
    "    next_steps: list[str] = []\n",
    "\n",
    "    def format(self) -> str:\n",
    "        \"\"\"Format the structured output for human-readable display.\"\"\"\n",
    "        output = \"üéØ Routing Decision\\n\"\n",
    "        output += f\"{'=' * 60}\\n\\n\"\n",
    "        output += f\"Request Analysis:\\n{self.request_analysis}\\n\\n\"\n",
    "        output += f\"Task Type: {self.task_details.task_type}\\n\"\n",
    "        output += f\"Description: {self.task_details.description}\\n\\n\"\n",
    "        output += \"Routing Decision:\\n\"\n",
    "        output += f\"  ‚Üí Selected Agent: {self.selected_agent}\\n\"\n",
    "        output += f\"  ‚Üí Reason: {self.routing_reason}\\n\"\n",
    "        output += f\"  ‚Üí Expected Outcome: {self.expected_outcome}\\n\"\n",
    "        if self.next_steps:\n",
    "            output += \"\\nNext Steps:\\n\"\n",
    "            for i, step in enumerate(self.next_steps, 1):\n",
    "                output += f\"  {i}. {step}\\n\"\n",
    "        return output\n",
    "\n",
    "\n",
    "print(\"Structured output models defined for orchestration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure orchestrator with structured outputs\n",
    "orchestrator_llm_config = LLMConfig(\n",
    "    config_list=[\n",
    "        {\n",
    "            \"api_type\": \"bedrock_v2\",  # V2 client with structured outputs\n",
    "            \"model\": \"aqwen.qwen3-coder-480b-a35b-v1:0\",\n",
    "            \"aws_region\": os.getenv(\"AWS_REGION\", \"eu-north-1\"),\n",
    "            \"aws_access_key\": os.getenv(\"AWS_ACCESS_KEY\"),\n",
    "            \"aws_secret_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "            \"response_format\": RoutingDecision,  # Structured output for routing\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "# Regular config for other agents\n",
    "regular_llm_config = LLMConfig(\n",
    "    config_list=[\n",
    "        {\n",
    "            \"api_type\": \"bedrock_v2\",  # V2 client without structured outputs\n",
    "            \"model\": \"aqwen.qwen3-coder-480b-a35b-v1:0\",\n",
    "            \"aws_region\": os.getenv(\"AWS_REGION\", \"eu-north-1\"),\n",
    "            \"aws_access_key\": os.getenv(\"AWS_ACCESS_KEY\"),\n",
    "            \"aws_secret_key\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "        }\n",
    "    ],\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "print(\"LLM configurations created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import UserProxyAgent\n",
    "from autogen.agentchat import initiate_group_chat\n",
    "from autogen.agentchat.group.patterns.auto import AutoPattern\n",
    "\n",
    "# Create orchestrator agent with structured outputs\n",
    "orchestrator = ConversableAgent(\n",
    "    name=\"pipeline_orchestrator\",\n",
    "    system_message=\"\"\"üéØ You are the Pipeline Orchestrator. Your role is to:\n",
    "    ‚Ä¢ Analyze user requests and determine the workflow path\n",
    "    ‚Ä¢ Route tasks to appropriate specialized agents\n",
    "    ‚Ä¢ Monitor pipeline progress and coordinate handoffs\n",
    "    ‚Ä¢ Report final results to the user\n",
    "\n",
    "    You MUST provide structured routing decisions that include:\n",
    "    - Analysis of the user's request\n",
    "    - Task type and details\n",
    "    - Selected agent and reasoning\n",
    "    - Expected outcome and next steps\n",
    "\n",
    "    Workflow Decision Logic:\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ 1. Research task? ‚Üí Route to researcher                  ‚îÇ\n",
    "    ‚îÇ 2. Writing task? ‚Üí Route to writer                       ‚îÇ\n",
    "    ‚îÇ 3. Analysis task? ‚Üí Route to analyst                     ‚îÇ\n",
    "    ‚îÇ 4. Planning task? ‚Üí Route to planner                     ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    Always provide clear routing decisions with reasoning.\"\"\",\n",
    "    llm_config=orchestrator_llm_config,\n",
    ")\n",
    "\n",
    "# Create specialized agents\n",
    "researcher = ConversableAgent(\n",
    "    name=\"researcher\",\n",
    "    system_message=\"\"\"üîç You are a Researcher. Your role is to:\n",
    "    ‚Ä¢ Conduct thorough research on given topics\n",
    "    ‚Ä¢ Gather relevant information from multiple sources\n",
    "    ‚Ä¢ Provide well-sourced findings\n",
    "    ‚Ä¢ Summarize key points clearly\n",
    "    Always cite your sources and provide comprehensive research.\"\"\",\n",
    "    llm_config=regular_llm_config,\n",
    ")\n",
    "\n",
    "writer = ConversableAgent(\n",
    "    name=\"writer\",\n",
    "    system_message=\"\"\"‚úçÔ∏è You are a Writer. Your role is to:\n",
    "    ‚Ä¢ Create well-structured written content\n",
    "    ‚Ä¢ Follow style guidelines and best practices\n",
    "    ‚Ä¢ Ensure clarity and coherence\n",
    "    ‚Ä¢ Adapt tone and style to the audience\n",
    "    Write engaging, clear, and professional content.\"\"\",\n",
    "    llm_config=regular_llm_config,\n",
    ")\n",
    "\n",
    "analyst = ConversableAgent(\n",
    "    name=\"analyst\",\n",
    "    system_message=\"\"\"üìä You are an Analyst. Your role is to:\n",
    "    ‚Ä¢ Analyze data and information systematically\n",
    "    ‚Ä¢ Identify patterns and trends\n",
    "    ‚Ä¢ Provide insights and recommendations\n",
    "    ‚Ä¢ Support conclusions with evidence\n",
    "    Provide thorough, data-driven analysis.\"\"\",\n",
    "    llm_config=regular_llm_config,\n",
    ")\n",
    "\n",
    "planner_agent = ConversableAgent(\n",
    "    name=\"planner\",\n",
    "    system_message=\"\"\"üìã You are a Planner. Your role is to:\n",
    "    ‚Ä¢ Create detailed plans and roadmaps\n",
    "    ‚Ä¢ Break down complex tasks into steps\n",
    "    ‚Ä¢ Identify dependencies and timelines\n",
    "    ‚Ä¢ Anticipate potential issues\n",
    "    Create comprehensive, actionable plans.\"\"\",\n",
    "    llm_config=regular_llm_config,\n",
    ")\n",
    "\n",
    "# Create user proxy\n",
    "user = UserProxyAgent(\n",
    "    name=\"user\",\n",
    "    human_input_mode=\"TERMINATE\",\n",
    "    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False},\n",
    ")\n",
    "\n",
    "print(\"All agents created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AutoPattern for groupchat\n",
    "pattern = AutoPattern(\n",
    "    initial_agent=orchestrator,\n",
    "    agents=[\n",
    "        orchestrator,\n",
    "        researcher,\n",
    "        writer,\n",
    "        analyst,\n",
    "        planner_agent,\n",
    "    ],\n",
    "    user_agent=user,\n",
    "    group_manager_args={\"llm_config\": orchestrator_llm_config},\n",
    ")\n",
    "\n",
    "print(\"AutoPattern created with Bedrock V2 agents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the group chat\n",
    "print(\"=== Starting Group Chat with Bedrock V2 and Structured Outputs ===\")\n",
    "\n",
    "result, context, last_agent = initiate_group_chat(\n",
    "    pattern=pattern,\n",
    "    messages=\"I need help creating a marketing strategy for a new coffee shop. Research the market, analyze competitors, and create a comprehensive plan.\",\n",
    "    max_rounds=8,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Final Result ===\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Part 8: Accessing Rich Content from V2 Responses\n",
    "\n",
    "Let's demonstrate how to access rich content from V2 client responses in a custom workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom workflow that processes V2 responses\n",
    "def process_v2_response(response: UnifiedResponse):\n",
    "    \"\"\"Process a UnifiedResponse and extract all relevant information.\"\"\"\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Processing Response from {response.provider.upper()}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    print(f\"\\nüìù Model: {response.model}\")\n",
    "    print(f\"üÜî ID: {response.id}\")\n",
    "    print(f\"‚úÖ Status: {response.status}\")\n",
    "    print(f\"üèÅ Finish Reason: {response.finish_reason}\")\n",
    "\n",
    "    print(f\"\\nüí¨ Messages ({len(response.messages)}):\")\n",
    "    for i, msg in enumerate(response.messages, 1):\n",
    "        print(f\"\\n  Message {i}:\")\n",
    "        print(f\"    Role: {msg.role}\")\n",
    "        print(f\"    Content Blocks: {len(msg.content)}\")\n",
    "\n",
    "        # Count content types\n",
    "        text_blocks = [b for b in msg.content if b.type == \"text\"]\n",
    "        tool_blocks = [b for b in msg.content if b.type == \"tool_call\"]\n",
    "        image_blocks = [b for b in msg.content if b.type == \"image\"]\n",
    "\n",
    "        if text_blocks:\n",
    "            print(f\"    üìÑ Text blocks: {len(text_blocks)}\")\n",
    "            print(\n",
    "                f\"       Text: {text_blocks[0].text[:100]}...\"\n",
    "                if len(text_blocks[0].text) > 100\n",
    "                else f\"       Text: {text_blocks[0].text}\"\n",
    "            )\n",
    "\n",
    "        if tool_blocks:\n",
    "            print(f\"    üîß Tool calls: {len(tool_blocks)}\")\n",
    "            for tool in tool_blocks:\n",
    "                print(f\"       - {tool.name}\")\n",
    "\n",
    "        if image_blocks:\n",
    "            print(f\"    üñºÔ∏è  Images: {len(image_blocks)}\")\n",
    "\n",
    "    print(\"\\nüìä Usage:\")\n",
    "    print(f\"    Prompt tokens: {response.usage.get('prompt_tokens', 0)}\")\n",
    "    print(f\"    Completion tokens: {response.usage.get('completion_tokens', 0)}\")\n",
    "    print(f\"    Total tokens: {response.usage.get('total_tokens', 0)}\")\n",
    "\n",
    "    if response.cost:\n",
    "        print(f\"    üí∞ Cost: ${response.cost:.6f}\")\n",
    "\n",
    "    print(\"\\nüîç Provider Metadata:\")\n",
    "    for key, value in response.provider_metadata.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_response = client.create({\n",
    "    \"model\": \"aqwen.qwen3-coder-480b-a35b-v1:0\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"List 3 benefits of cloud computing.\"}],\n",
    "})\n",
    "\n",
    "process_v2_response(test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've learned:\n",
    "\n",
    "1. ‚úÖ How to configure and use Bedrock V2 client (`api_type: \"bedrock_v2\"`)\n",
    "2. ‚úÖ How to access rich `UnifiedResponse` objects with typed content blocks\n",
    "3. ‚úÖ How to use structured outputs with Bedrock V2 client\n",
    "4. ‚úÖ How V1 and V2 Bedrock clients work together seamlessly\n",
    "5. ‚úÖ How to create group chats with mixed client versions\n",
    "6. ‚úÖ How to process and extract information from V2 responses\n",
    "7. ‚úÖ Advanced patterns like orchestration with structured outputs\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **V2 Client Benefits**: Rich content preservation, direct property access, forward compatibility\n",
    "- **Backward Compatible**: V1 and V2 clients can work together in the same conversation\n",
    "- **Structured Outputs**: Combine V2 architecture with structured outputs for powerful workflows\n",
    "- **Group Chats**: Use V2 clients in multi-agent scenarios for better content handling\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Experiment with different Bedrock models using V2 client\n",
    "- Try multimodal content (images) with Bedrock V2\n",
    "- Create custom workflows that leverage rich content blocks\n",
    "- Combine V2 clients with other AG2 features like tools and function calling\n",
    "\n",
    "## References\n",
    "\n",
    "- [AG2 Documentation](https://docs.ag2.ai)\n",
    "- [Bedrock Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html)\n",
    "- [ModelClientV2 Migration Guide](/autogen/llm_clients/MIGRATION_TO_V2.md)\n",
    "- [Bedrock Model IDs](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html)"
   ]
  }
 ],
 "metadata": {
  "front_matter": {
   "description": "Amazon Bedrock V2 Client with AG2 - Rich UnifiedResponse and Structured Outputs",
   "tags": [
    "bedrock",
    "amazon",
    "v2",
    "unified-response",
    "structured-output",
    "groupchat"
   ]
  },
  "kernelspec": {
   "display_name": "ag2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
