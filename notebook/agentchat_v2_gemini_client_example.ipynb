{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AG2 + Gemini V2 Client Example\n",
    "\n",
    "Author: [Your Name]\n",
    "\n",
    "This notebook demonstrates the **Gemini V2 Client** (`api_type: \"gemini_v2\"`), which implements the ModelClientV2 protocol and returns rich `UnifiedResponse` objects with typed content blocks.\n",
    "\n",
    "## Key Features of Gemini V2 Client\n",
    "\n",
    "- **Rich Content Support**: Access reasoning blocks, multimodal content (images, audio, video), and tool calls\n",
    "- **Provider Agnostic**: Unified format compatible with other V2 clients (OpenAI, Anthropic, Bedrock)\n",
    "- **Type Safety**: Typed content blocks with Pydantic validation\n",
    "- **Direct Property Access**: Use `response.text`, `response.reasoning`, etc. instead of parsing nested structures\n",
    "- **Thinking Config Support**: Full support for Gemini's thinking features (`thinking_budget`, `thinking_level`, `include_thoughts`)\n",
    "- **Structured Outputs**: Support for Pydantic models and JSON schemas\n",
    "\n",
    "## Installation\n",
    "\n",
    "sh\n",
    "pip install ag2[gemini]\n",
    "```\n",
    "\n",
    "## Why Use V2 Client?\n",
    "\n",
    "The V2 client (`api_type: \"gemini_v2\"`) provides several advantages over the V1 client (`api_type: \"google\"`):\n",
    "\n",
    "1. **Rich Content Access**: Direct access to reasoning blocks, multimodal content, and citations\n",
    "2. **Unified Interface**: Same response format across all providers (OpenAI, Anthropic, Gemini, Bedrock)\n",
    "3. **Forward Compatible**: Handles new content types without code changes\n",
    "4. **Better Developer Experience**: Type-safe content blocks with direct property access\n",
    "5. **Full Thinking Support**: Complete support for Gemini 3 thinking features\n",
    "\n",
    "Reference: [ModelClientV2 Migration Guide](https://github.com/microsoft/autogen/blob/main/autogen/llm_clients/MIGRATION_TO_V2.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 3\n",
      "python-dotenv could not parse statement starting at line 4\n",
      "python-dotenv could not parse statement starting at line 5\n",
      "python-dotenv could not parse statement starting at line 8\n",
      "python-dotenv could not parse statement starting at line 9\n",
      "python-dotenv could not parse statement starting at line 10\n",
      "python-dotenv could not parse statement starting at line 11\n",
      "python-dotenv could not parse statement starting at line 12\n",
      "python-dotenv could not parse statement starting at line 38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from autogen import ConversableAgent, GroupChat, GroupChatManager, LLMConfig\n",
    "from autogen.llm_clients import GeminiV2Client, UnifiedResponse\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY is not set. Please set it in your environment or .env file.\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Basic Usage: Direct Client Access\n",
    "\n",
    "The Gemini V2 client can be used directly to get rich `UnifiedResponse` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response type: <class 'autogen.llm_clients.models.unified_response.UnifiedResponse'>\n",
      "Is UnifiedResponse: True\n",
      "Provider: gemini\n",
      "Model: gemini-2.5-flash\n",
      "Text: Hello\n",
      "Usage: {'prompt_tokens': 9, 'completion_tokens': 1, 'total_tokens': 10}\n",
      "Cost: $0.000005\n"
     ]
    }
   ],
   "source": [
    "# Create Gemini V2 client directly\n",
    "client = GeminiV2Client(api_key=api_key)\n",
    "\n",
    "# Create a completion\n",
    "response = client.create({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Say 'Hello' in one word.\"}],\n",
    "})\n",
    "\n",
    "# Verify it's a UnifiedResponse\n",
    "print(f\"Response type: {type(response)}\")\n",
    "print(f\"Is UnifiedResponse: {isinstance(response, UnifiedResponse)}\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Text: {response.text}\")\n",
    "print(f\"Usage: {response.usage}\")\n",
    "print(f\"Cost: ${response.cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Accessing Rich Content Blocks\n",
    "\n",
    "The V2 client preserves all content types in structured blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Text Content ===\n",
      "Quantum computing harnesses quantum-mechanical phenomena like superposition and entanglement to allow \"qubits\" to exist in multiple states simultaneously, unlike classical bits. This enables them to process vast amounts of information in parallel, tackling certain complex problems impossible for traditional computers.\n",
      "\n",
      "=== Messages ===\n",
      "Message 0 (role: UserRoleEnum.ASSISTANT):\n",
      "  - Type: ContentType.TEXT, Content: type=<ContentType.TEXT: 'text'> extra={} text='Quantum computing harnesses quantum-mechanical phenom...\n",
      "\n",
      "Number of text blocks: 1\n"
     ]
    }
   ],
   "source": [
    "# Get response with rich content\n",
    "response = client.create({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in 2 sentences.\"}],\n",
    "})\n",
    "\n",
    "# Access text content directly\n",
    "print(\"=== Text Content ===\")\n",
    "print(response.text)\n",
    "print()\n",
    "\n",
    "# Access individual messages\n",
    "print(\"=== Messages ===\")\n",
    "for idx, message in enumerate(response.messages):\n",
    "    print(f\"Message {idx} (role: {message.role}):\")\n",
    "    for content_block in message.content:\n",
    "        print(f\"  - Type: {content_block.type}, Content: {str(content_block)[:100]}...\")\n",
    "    print()\n",
    "\n",
    "# Get content by type\n",
    "text_blocks = response.get_content_by_type(\"text\")\n",
    "print(f\"Number of text blocks: {len(text_blocks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Thinking Config Support (Gemini 3 Models)\n",
    "\n",
    "The V2 client fully supports Gemini's thinking features. This is especially powerful with Gemini 3 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using thinking_level with Gemini 3 Pro\n",
    "prompt = \"\"\"You are playing the 20 question game. You know that what you are looking for\n",
    "is an aquatic mammal that doesn't live in the sea, is venomous and that's\n",
    "smaller than a cat. What could that be and how could you make sure?\n",
    "\"\"\"\n",
    "\n",
    "response = client.create({\n",
    "    \"model\": \"gemini-3-pro-preview\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    \"thinking_level\": \"High\",  # Use thinking_level for Gemini 3 Pro\n",
    "    \"include_thoughts\": True,  # Include thought summaries in response\n",
    "})\n",
    "\n",
    "print(\"=== Response with Thinking ===\")\n",
    "print(response.text)\n",
    "print()\n",
    "\n",
    "# Access reasoning blocks if present\n",
    "reasoning_blocks = response.get_content_by_type(\"reasoning\")\n",
    "if reasoning_blocks:\n",
    "    print(\"=== Reasoning Blocks ===\")\n",
    "    for reasoning in reasoning_blocks:\n",
    "        print(f\"Reasoning: {reasoning.reasoning[:200]}...\")\n",
    "else:\n",
    "    print(\"No reasoning blocks found (thoughts may be included in text)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using thinking_budget with Gemini 2.5 Flash\n",
    "response = client.create({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    \"thinking_budget\": 4096,  # Token budget for thinking\n",
    "})\n",
    "\n",
    "print(\"=== Response with Thinking Budget ===\")\n",
    "print(response.text)\n",
    "print(f\"\\nTokens used: {response.usage.get('total_tokens', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Structured Outputs with Pydantic Models\n",
    "\n",
    "The V2 client supports structured outputs using Pydantic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic model for structured output\n",
    "class Answer(BaseModel):\n",
    "    answer: str\n",
    "    confidence: float\n",
    "    reasoning: str | None = None\n",
    "\n",
    "\n",
    "# Create client with response format\n",
    "structured_client = GeminiV2Client(api_key=api_key, response_format=Answer)\n",
    "\n",
    "# Get structured response\n",
    "response = structured_client.create({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is 2+2? Answer with confidence score.\"}],\n",
    "})\n",
    "\n",
    "print(\"=== Structured Response ===\")\n",
    "print(response.text)\n",
    "print()\n",
    "\n",
    "# The response text contains JSON matching the schema\n",
    "import json\n",
    "\n",
    "try:\n",
    "    structured_data = json.loads(response.text)\n",
    "    print(\"Parsed structured data:\")\n",
    "    print(json.dumps(structured_data, indent=2))\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Response is not valid JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Tool/Function Calling\n",
    "\n",
    "The V2 client preserves tool calls as `ToolCallContent` blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"Temperature unit\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create request with tools\n",
    "response = client.create({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}],\n",
    "    \"tools\": tools,\n",
    "})\n",
    "\n",
    "print(\"=== Response with Tool Calls ===\")\n",
    "print(f\"Text: {response.text}\")\n",
    "print()\n",
    "\n",
    "# Access tool calls\n",
    "tool_calls = response.messages[0].get_tool_calls()\n",
    "if tool_calls:\n",
    "    print(\"=== Tool Calls ===\")\n",
    "    for tool_call in tool_calls:\n",
    "        print(f\"Tool: {tool_call.name}\")\n",
    "        print(f\"ID: {tool_call.id}\")\n",
    "        print(f\"Arguments: {tool_call.arguments}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No tool calls in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6. Using with LLMConfig (Recommended for Agents)\n",
    "\n",
    "The V2 client can be used with `LLMConfig` for agent integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "=== Agent Response ===\n",
      "Machine learning is a field where computers learn from data to identify patterns, make decisions, or predictions without\n"
     ]
    }
   ],
   "source": [
    "# Configure LLM with V2 client\n",
    "llm_config_v2 = LLMConfig(\n",
    "    config_list=[\n",
    "        {\n",
    "            \"api_type\": \"gemini_v2\",  # Use gemini_v2 for V2 client\n",
    "            \"model\": \"gemini-2.5-flash\",\n",
    "            \"api_key\": api_key,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 500,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create agent with V2 client\n",
    "agent_v2 = ConversableAgent(\n",
    "    name=\"assistant_v2\",\n",
    "    llm_config=llm_config_v2,\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    ")\n",
    "\n",
    "# Use the agent\n",
    "response = agent_v2.generate_reply(messages=[{\"role\": \"user\", \"content\": \"Explain machine learning in one sentence.\"}])\n",
    "\n",
    "print(\"=== Agent Response ===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 7. V1 vs V2 Comparison\n",
    "\n",
    "Compare the V1 and V2 client responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 Client (legacy)\n",
    "llm_config_v1 = LLMConfig(\n",
    "    config_list=[\n",
    "        {\n",
    "            \"api_type\": \"google\",  # V1 client\n",
    "            \"model\": \"gemini-2.5-flash\",\n",
    "            \"api_key\": api_key,\n",
    "            \"response_format\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent_v1 = ConversableAgent(\n",
    "    name=\"assistant_v1\",\n",
    "    llm_config=llm_config_v1,\n",
    ")\n",
    "\n",
    "# V2 Client\n",
    "agent_v2 = ConversableAgent(\n",
    "    name=\"assistant_v2\",\n",
    "    llm_config=llm_config_v2,\n",
    ")\n",
    "\n",
    "test_message = \"What is the capital of France?\"\n",
    "\n",
    "print(\"=== V1 Client Response ===\")\n",
    "response_v1 = agent_v1.generate_reply(messages=[{\"role\": \"user\", \"content\": test_message}])\n",
    "print(f\"Type: {type(response_v1)}\")\n",
    "print(f\"Response: {response_v1}\")\n",
    "print()\n",
    "\n",
    "print(\"=== V2 Client Response ===\")\n",
    "response_v2 = agent_v2.generate_reply(messages=[{\"role\": \"user\", \"content\": test_message}])\n",
    "print(f\"Type: {type(response_v2)}\")\n",
    "print(f\"Response: {response_v2}\")\n",
    "print()\n",
    "\n",
    "print(\"Note: Both responses work with agents, but V2 provides richer content access when using the client directly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 8. Group Chat with V2 Client\n",
    "\n",
    "The V2 client works seamlessly in group chats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agents with V2 client\n",
    "planner = ConversableAgent(\n",
    "    name=\"planner\",\n",
    "    llm_config=LLMConfig(\n",
    "        config_list=[\n",
    "            {\n",
    "                \"api_type\": \"gemini_v2\",\n",
    "                \"model\": \"gemini-2.5-flash\",\n",
    "                \"api_key\": api_key,\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    system_message=\"You are a planning assistant. Create detailed plans.\",\n",
    "    description=\"Creates plans\",\n",
    ")\n",
    "\n",
    "reviewer = ConversableAgent(\n",
    "    name=\"reviewer\",\n",
    "    llm_config=LLMConfig(\n",
    "        config_list=[\n",
    "            {\n",
    "                \"api_type\": \"gemini_v2\",\n",
    "                \"model\": \"gemini-2.5-flash\",\n",
    "                \"api_key\": api_key,\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    system_message=\"You are a review assistant. Provide constructive feedback.\",\n",
    "    description=\"Reviews plans\",\n",
    ")\n",
    "\n",
    "executor = ConversableAgent(\n",
    "    name=\"executor\",\n",
    "    llm_config=LLMConfig(\n",
    "        config_list=[\n",
    "            {\n",
    "                \"api_type\": \"gemini_v2\",\n",
    "                \"model\": \"gemini-2.5-flash\",\n",
    "                \"api_key\": api_key,\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    system_message=\"You are an execution assistant. Implement plans.\",\n",
    "    description=\"Executes plans\",\n",
    ")\n",
    "\n",
    "# Create group chat\n",
    "groupchat = GroupChat(\n",
    "    agents=[planner, reviewer, executor],\n",
    "    speaker_selection_method=\"auto\",\n",
    "    messages=[],\n",
    ")\n",
    "\n",
    "manager = GroupChatManager(\n",
    "    name=\"manager\",\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config_v2,\n",
    "    # is_termination_msg=lambda x: \"DONE\" in (x.get(\"content\", \"\") or \"\").upper(),\n",
    ")\n",
    "\n",
    "# Start conversation\n",
    "chat_result = planner.initiate_chat(\n",
    "    recipient=manager,\n",
    "    message=\"Create a plan for organizing a small team meeting. Say DONE when finished.\",\n",
    "    max_turns=3,\n",
    ")\n",
    "\n",
    "print(\"=== Group Chat History ===\")\n",
    "for msg in chat_result.chat_history:\n",
    "    print(f\"{msg.get('name', 'unknown')}: {msg.get('content', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 9. Group Chat with Structured Outputs\n",
    "\n",
    "Combine V2 client with structured outputs in a group chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Structured Plan ===\n",
      "<autogen.io.run_response.RunResponse object at 0x15fedcec0>\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not RunResponse",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     plan_data = \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Parsed Plan ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTitle: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplan_data.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.2/Frameworks/Python.framework/Versions/3.14/lib/python3.14/json/__init__.py:345\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    346\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    347\u001b[39m     s = s.decode(detect_encoding(s), \u001b[33m'\u001b[39m\u001b[33msurrogatepass\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    350\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    351\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[31mTypeError\u001b[39m: the JSON object must be str, bytes or bytearray, not RunResponse"
     ]
    }
   ],
   "source": [
    "# Define structured output model\n",
    "class Plan(BaseModel):\n",
    "    title: str\n",
    "    steps: list[str]\n",
    "    estimated_time: str\n",
    "    resources_needed: list[str]\n",
    "\n",
    "\n",
    "# Create agent with structured output\n",
    "structured_agent = ConversableAgent(\n",
    "    name=\"structured_planner\",\n",
    "    llm_config=LLMConfig(\n",
    "    config_list=[{\n",
    "        \"api_type\": \"gemini_v2\",\n",
    "        \"model\": \"gemini-2.5-flash\",\n",
    "        \"api_key\": os.environ.get(\"GEMINI_API_KEY\"),\n",
    "        # \"temperature\": 0,\n",
    "    \"response_format\":Plan\n",
    "    }],\n",
    "),\n",
    "    system_message=\"You are a planning assistant. Always respond with structured plans.\",\n",
    ")\n",
    "\n",
    "# Get structured response\n",
    "response = structured_agent.run(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Create a plan for learning Python in 30 days.\"}],\n",
    "    max_turns=3,\n",
    ")\n",
    "\n",
    "print(\"=== Structured Plan ===\")\n",
    "print(response)\n",
    "print()\n",
    "\n",
    "# Parse the structured response\n",
    "import json\n",
    "\n",
    "try:\n",
    "    plan_data = json.loads(response)\n",
    "    print(\"=== Parsed Plan ===\")\n",
    "    print(f\"Title: {plan_data.get('title')}\")\n",
    "    print(f\"Steps: {len(plan_data.get('steps', []))}\")\n",
    "    print(f\"Estimated Time: {plan_data.get('estimated_time')}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Response is not valid JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 10. Cost and Usage Tracking\n",
    "\n",
    "The V2 client provides detailed cost and usage information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response\n",
    "response = client.create({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain the theory of relativity.\"}],\n",
    "    \"max_tokens\": 500,\n",
    "})\n",
    "\n",
    "# Access usage information\n",
    "usage = GeminiV2Client.get_usage(response)\n",
    "cost = client.cost(response)\n",
    "\n",
    "print(\"=== Usage Information ===\")\n",
    "print(f\"Prompt tokens: {usage['prompt_tokens']}\")\n",
    "print(f\"Completion tokens: {usage['completion_tokens']}\")\n",
    "print(f\"Total tokens: {usage['total_tokens']}\")\n",
    "print(f\"Cost: ${cost:.6f}\")\n",
    "print(f\"Model: {usage['model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 11. Backward Compatibility: create_v1_compatible()\n",
    "\n",
    "The V2 client provides backward compatibility with V1 code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get V1-compatible response\n",
    "v1_response = client.create_v1_compatible({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "})\n",
    "\n",
    "print(\"=== V1-Compatible Response ===\")\n",
    "print(f\"Type: {type(v1_response)}\")\n",
    "print(f\"Keys: {list(v1_response.keys())}\")\n",
    "print(f\"Model: {v1_response.get('model')}\")\n",
    "print(f\"Choices: {len(v1_response.get('choices', []))}\")\n",
    "print(f\"Usage: {v1_response.get('usage')}\")\n",
    "print(f\"Cost: {v1_response.get('cost')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Gemini V2 client (`api_type: \"gemini_v2\"`) provides:\n",
    "\n",
    "✅ **Rich Content Access**: Direct access to reasoning blocks, multimodal content, and tool calls\n",
    "✅ **Unified Interface**: Same response format across all providers\n",
    "✅ **Full Thinking Support**: Complete support for Gemini 3 thinking features\n",
    "✅ **Structured Outputs**: Support for Pydantic models and JSON schemas\n",
    "✅ **Backward Compatible**: Works with existing agent code via `create_v1_compatible()`\n",
    "✅ **Type Safe**: Typed content blocks with Pydantic validation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore multimodal content (images, audio, video)\n",
    "- Try different Gemini models (2.5 Flash, 2.5 Pro, 3 Pro Preview)\n",
    "- Experiment with thinking configurations\n",
    "- Integrate with other V2 clients in group chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6886f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser\u001b[0m (to bot):\n",
      "\n",
      "Think about the weather in paris, and return any information you find.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "additionalProperties is not supported in the Gemini API.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     29\u001b[39m bot = ConversableAgent(\n\u001b[32m     30\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mbot\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     31\u001b[39m     llm_config=llm_config,\n\u001b[32m     32\u001b[39m     system_message=\u001b[33m\"\u001b[39m\u001b[33mYou are a smart assistant.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     33\u001b[39m )\n\u001b[32m     35\u001b[39m response = bot.run(\n\u001b[32m     36\u001b[39m     message=\u001b[33m\"\u001b[39m\u001b[33mThink about the weather in paris, and return any information you find.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     37\u001b[39m     max_turns=\u001b[32m1\u001b[39m\n\u001b[32m     38\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m result = \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/autogen/io/run_response.py:209\u001b[39m, in \u001b[36mRunResponse.process\u001b[39m\u001b[34m(self, processor)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, processor: EventProcessorProtocol | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    208\u001b[39m     processor = processor \u001b[38;5;129;01mor\u001b[39;00m ConsoleEventProcessor()\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/autogen/io/processors/console_event_processor.py:20\u001b[39m, in \u001b[36mConsoleEventProcessor.process\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, response: \u001b[33m\"\u001b[39m\u001b[33mRunResponseProtocol\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocess_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/autogen/io/run_response.py:164\u001b[39m, in \u001b[36mRunResponse._queue_generator\u001b[39m\u001b[34m(self, q)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, ErrorEvent):\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m event.content.error  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m queue.Empty:\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/autogen/agentchat/conversable_agent.py:1566\u001b[39m, in \u001b[36mConversableAgent.run.<locals>.initiate_chat\u001b[39m\u001b[34m(self, iostream, response)\u001b[39m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m msg_to == \u001b[33m\"\u001b[39m\u001b[33magent\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1566\u001b[39m         chat_result = \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1567\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclear_history\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclear_history\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_turns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m            \u001b[49m\u001b[43msummary_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43msummary_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1573\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1574\u001b[39m         chat_result = \u001b[38;5;28mself\u001b[39m.initiate_chat(\n\u001b[32m   1575\u001b[39m             executor,\n\u001b[32m   1576\u001b[39m             message=message,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1579\u001b[39m             summary_method=summary_method,\n\u001b[32m   1580\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/autogen/agentchat/conversable_agent.py:1468\u001b[39m, in \u001b[36mConversableAgent.initiate_chat\u001b[39m\u001b[34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m msg2send \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1467\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1468\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# No breaks in the for loop, so we have reached max turns\u001b[39;00m\n\u001b[32m   1470\u001b[39m     iostream.send(\n\u001b[32m   1471\u001b[39m         TerminationEvent(\n\u001b[32m   1472\u001b[39m             termination_reason=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMaximum turns (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_turns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) reached\u001b[39m\u001b[33m\"\u001b[39m, sender=\u001b[38;5;28mself\u001b[39m, recipient=recipient\n\u001b[32m   1473\u001b[39m         )\n\u001b[32m   1474\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/autogen/agentchat/conversable_agent.py:1134\u001b[39m, in \u001b[36mConversableAgent.send\u001b[39m\u001b[34m(self, message, recipient, request_reply, silent)\u001b[39m\n\u001b[32m   1132\u001b[39m valid = \u001b[38;5;28mself\u001b[39m._append_oai_message(message, recipient, role=\u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, name=\u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[32m-> \u001b[39m\u001b[32m1134\u001b[39m     \u001b[43mrecipient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1137\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMessage can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1138\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/autogen/agentchat/conversable_agent.py:1242\u001b[39m, in \u001b[36mConversableAgent.receive\u001b[39m\u001b[34m(self, message, sender, request_reply, silent)\u001b[39m\n\u001b[32m   1240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m reply = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1244\u001b[39m     \u001b[38;5;28mself\u001b[39m.send(reply, sender, silent=silent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/autogen/agentchat/conversable_agent.py:3204\u001b[39m, in \u001b[36mConversableAgent.generate_reply\u001b[39m\u001b[34m(self, messages, sender, exclude)\u001b[39m\n\u001b[32m   3202\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   3203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._match_trigger(reply_func_tuple[\u001b[33m\"\u001b[39m\u001b[33mtrigger\u001b[39m\u001b[33m\"\u001b[39m], sender):\n\u001b[32m-> \u001b[39m\u001b[32m3204\u001b[39m     final, reply = \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m=\u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3205\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logging_enabled():\n\u001b[32m   3206\u001b[39m         log_event(\n\u001b[32m   3207\u001b[39m             \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3208\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreply_func_executed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3212\u001b[39m             reply=reply,\n\u001b[32m   3213\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/autogen/agentchat/conversable_agent.py:2485\u001b[39m, in \u001b[36mConversableAgent.generate_oai_reply\u001b[39m\u001b[34m(self, messages, sender, config, **kwargs)\u001b[39m\n\u001b[32m   2482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m processed_messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, {\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLLM call blocked by safeguard\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m-> \u001b[39m\u001b[32m2485\u001b[39m extracted_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2487\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2488\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2489\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2492\u001b[39m \u001b[38;5;66;03m# Process LLM response\u001b[39;00m\n\u001b[32m   2493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/autogen/agentchat/conversable_agent.py:2520\u001b[39m, in \u001b[36mConversableAgent._generate_oai_reply_from_client\u001b[39m\u001b[34m(self, llm_client, messages, cache, **kwargs)\u001b[39m\n\u001b[32m   2517\u001b[39m         all_messages.append(message)\n\u001b[32m   2519\u001b[39m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2520\u001b[39m response = \u001b[43mllm_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2524\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2525\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2526\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2527\u001b[39m extracted_response = llm_client.extract_text_or_completion_object(response)[\u001b[32m0\u001b[39m]\n\u001b[32m   2529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/autogen/oai/client.py:1270\u001b[39m, in \u001b[36mOpenAIWrapper.create\u001b[39m\u001b[34m(self, **config)\u001b[39m\n\u001b[32m   1268\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1269\u001b[39m     request_ts = get_current_ts()\n\u001b[32m-> \u001b[39m\u001b[32m1270\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1272\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m openai_result.is_successful:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/autogen/llm_clients/gemini_v2.py:371\u001b[39m, in \u001b[36mGeminiV2Client.create\u001b[39m\u001b[34m(self, params)\u001b[39m\n\u001b[32m    363\u001b[39m     generate_content_config = GenerateContentConfig(\n\u001b[32m    364\u001b[39m         safety_settings=safety_settings,\n\u001b[32m    365\u001b[39m         system_instruction=system_instruction,\n\u001b[32m   (...)\u001b[39m\u001b[32m    368\u001b[39m         **generation_config,\n\u001b[32m    369\u001b[39m     )\n\u001b[32m    370\u001b[39m     chat = client.chats.create(model=model_name, config=generate_content_config, history=gemini_messages[:-\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     response = \u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgemini_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# Transform to UnifiedResponse\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_response(response, model_name, has_response_format)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/.venv/lib/python3.14/site-packages/google/genai/chats.py:252\u001b[39m, in \u001b[36mChat.send_message\u001b[39m\u001b[34m(self, message, config)\u001b[39m\n\u001b[32m    247\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    248\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMessage must be a valid part type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes.PartUnion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    249\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes.PartUnionDict\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    250\u001b[39m   )\n\u001b[32m    251\u001b[39m input_content = t.t_content(message)\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_modules\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_curated_history\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_content\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m model_output = (\n\u001b[32m    258\u001b[39m     [response.candidates[\u001b[32m0\u001b[39m].content]\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.candidates \u001b[38;5;129;01mand\u001b[39;00m response.candidates[\u001b[32m0\u001b[39m].content\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    261\u001b[39m )\n\u001b[32m    262\u001b[39m automatic_function_calling_history = (\n\u001b[32m    263\u001b[39m     response.automatic_function_calling_history\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m response.automatic_function_calling_history\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    266\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/.venv/lib/python3.14/site-packages/google/genai/models.py:5227\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5225\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5226\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5227\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5228\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5229\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5231\u001b[39m   function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5232\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/.venv/lib/python3.14/site-packages/google/genai/models.py:3985\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3983\u001b[39m     path = \u001b[33m'\u001b[39m\u001b[38;5;132;01m{model}\u001b[39;00m\u001b[33m:generateContent\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   3984\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3985\u001b[39m   request_dict = \u001b[43m_GenerateContentParameters_to_mldev\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3986\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_model\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3988\u001b[39m   request_url_dict = request_dict.get(\u001b[33m'\u001b[39m\u001b[33m_url\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   3989\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m request_url_dict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/.venv/lib/python3.14/site-packages/google/genai/models.py:1324\u001b[39m, in \u001b[36m_GenerateContentParameters_to_mldev\u001b[39m\u001b[34m(api_client, from_object, parent_object)\u001b[39m\n\u001b[32m   1311\u001b[39m   setv(\n\u001b[32m   1312\u001b[39m       to_object,\n\u001b[32m   1313\u001b[39m       [\u001b[33m'\u001b[39m\u001b[33mcontents\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1317\u001b[39m       ],\n\u001b[32m   1318\u001b[39m   )\n\u001b[32m   1320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m getv(from_object, [\u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1321\u001b[39m   setv(\n\u001b[32m   1322\u001b[39m       to_object,\n\u001b[32m   1323\u001b[39m       [\u001b[33m'\u001b[39m\u001b[33mgenerationConfig\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m-> \u001b[39m\u001b[32m1324\u001b[39m       \u001b[43m_GenerateContentConfig_to_mldev\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m          \u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mconfig\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_object\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1327\u001b[39m   )\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m to_object\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/.venv/lib/python3.14/site-packages/google/genai/models.py:1022\u001b[39m, in \u001b[36m_GenerateContentConfig_to_mldev\u001b[39m\u001b[34m(api_client, from_object, parent_object)\u001b[39m\n\u001b[32m   1012\u001b[39m   setv(\n\u001b[32m   1013\u001b[39m       to_object,\n\u001b[32m   1014\u001b[39m       [\u001b[33m'\u001b[39m\u001b[33mresponseMimeType\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   1015\u001b[39m       getv(from_object, [\u001b[33m'\u001b[39m\u001b[33mresponse_mime_type\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m   1016\u001b[39m   )\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m getv(from_object, [\u001b[33m'\u001b[39m\u001b[33mresponse_schema\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1019\u001b[39m   setv(\n\u001b[32m   1020\u001b[39m       to_object,\n\u001b[32m   1021\u001b[39m       [\u001b[33m'\u001b[39m\u001b[33mresponseSchema\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m       \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mt_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mresponse_schema\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1023\u001b[39m   )\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m getv(from_object, [\u001b[33m'\u001b[39m\u001b[33mresponse_json_schema\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1026\u001b[39m   setv(\n\u001b[32m   1027\u001b[39m       to_object,\n\u001b[32m   1028\u001b[39m       [\u001b[33m'\u001b[39m\u001b[33mresponseJsonSchema\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   1029\u001b[39m       getv(from_object, [\u001b[33m'\u001b[39m\u001b[33mresponse_json_schema\u001b[39m\u001b[33m'\u001b[39m]),\n\u001b[32m   1030\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/.venv/lib/python3.14/site-packages/google/genai/_transformers.py:888\u001b[39m, in \u001b[36mt_schema\u001b[39m\u001b[34m(client, origin)\u001b[39m\n\u001b[32m    886\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(origin, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m _is_type_dict_str_any(origin):\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m   \u001b[43mprocess_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m types.Schema.model_validate(origin)\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(origin, EnumMeta):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/.venv/lib/python3.14/site-packages/google/genai/_transformers.py:824\u001b[39m, in \u001b[36mprocess_schema\u001b[39m\u001b[34m(schema, client, defs, order_properties)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (properties := schema.get(\u001b[33m'\u001b[39m\u001b[33mproperties\u001b[39m\u001b[33m'\u001b[39m)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    823\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m name, sub_schema \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(properties.items()):\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     properties[name] = \u001b[43m_recurse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    825\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    826\u001b[39m       \u001b[38;5;28mlen\u001b[39m(properties.items()) > \u001b[32m1\u001b[39m\n\u001b[32m    827\u001b[39m       \u001b[38;5;129;01mand\u001b[39;00m order_properties\n\u001b[32m    828\u001b[39m       \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mpropertyOrdering\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m schema\n\u001b[32m    829\u001b[39m   ):\n\u001b[32m    830\u001b[39m     schema[\u001b[33m'\u001b[39m\u001b[33mproperty_ordering\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(properties.keys())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/.venv/lib/python3.14/site-packages/google/genai/_transformers.py:798\u001b[39m, in \u001b[36mprocess_schema.<locals>._recurse\u001b[39m\u001b[34m(sub_schema)\u001b[39m\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (ref := sub_schema.pop(\u001b[33m'\u001b[39m\u001b[33m$ref\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    797\u001b[39m   sub_schema = defs[ref.split(\u001b[33m'\u001b[39m\u001b[33mdefs/\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]]\n\u001b[32m--> \u001b[39m\u001b[32m798\u001b[39m \u001b[43mprocess_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_schema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder_properties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sub_schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/.venv/lib/python3.14/site-packages/google/genai/_transformers.py:762\u001b[39m, in \u001b[36mprocess_schema\u001b[39m\u001b[34m(schema, client, defs, order_properties)\u001b[39m\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema.get(\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m) == \u001b[33m'\u001b[39m\u001b[33mPlaceholderLiteralEnum\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    760\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m schema[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m762\u001b[39m \u001b[43m_raise_for_unsupported_mldev_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[38;5;66;03m# Standardize spelling for relevant schema fields.  For example, if a dict is\u001b[39;00m\n\u001b[32m    765\u001b[39m \u001b[38;5;66;03m# provided directly to response_schema, it may use `any_of` instead of `anyOf.\u001b[39;00m\n\u001b[32m    766\u001b[39m \u001b[38;5;66;03m# Otherwise, model_json_schema() uses `anyOf`.\u001b[39;00m\n\u001b[32m    767\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m from_name, to_name \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    768\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33madditional_properties\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33madditionalProperties\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    769\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33many_of\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33manyOf\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    770\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mprefix_items\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mprefixItems\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    771\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mproperty_ordering\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpropertyOrdering\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    772\u001b[39m ]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/ag2/.venv/lib/python3.14/site-packages/google/genai/_transformers.py:689\u001b[39m, in \u001b[36m_raise_for_unsupported_mldev_properties\u001b[39m\u001b[34m(schema, client)\u001b[39m\n\u001b[32m    678\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_raise_for_unsupported_mldev_properties\u001b[39m(\n\u001b[32m    679\u001b[39m     schema: Any, client: Optional[_api_client.BaseApiClient]\n\u001b[32m    680\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    681\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    682\u001b[39m       client\n\u001b[32m    683\u001b[39m       \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m client.vertexai\n\u001b[32m   (...)\u001b[39m\u001b[32m    687\u001b[39m       )\n\u001b[32m    688\u001b[39m   ):\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33madditionalProperties is not supported in the Gemini API.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: additionalProperties is not supported in the Gemini API."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from autogen import ConversableAgent, LLMConfig\n",
    "\n",
    "\n",
    "class Extra(BaseModel):\n",
    "    notes: str\n",
    "\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    is_good: bool\n",
    "\n",
    "    extra: dict[str, Extra]\n",
    "\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    config_list={\n",
    "        \"api_type\": \"gemini_v2\",\n",
    "        \"model\": \"gemini-2.5-flash\",\n",
    "        \"api_key\": os.environ.get(\"GEMINI_API_KEY\"),\n",
    "        \"temperature\": 0,\n",
    "        \"response_format\":Output\n",
    "    },\n",
    ")\n",
    "\n",
    "bot = ConversableAgent(\n",
    "    name=\"bot\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a smart assistant.\",\n",
    ")\n",
    "\n",
    "response = bot.run(\n",
    "    message=\"Think about the weather in paris, and return any information you find.\",\n",
    "    max_turns=1\n",
    ")\n",
    "result = response.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6aebb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: google-genai\n",
      "Version: 1.60.0\n",
      "Summary: GenAI Python SDK\n",
      "Home-page: https://github.com/googleapis/python-genai\n",
      "Author: \n",
      "Author-email: Google LLC <googleapis-packages@google.com>\n",
      "License-Expression: Apache-2.0\n",
      "Location: /Users/priyanshu/Documents/GitHub/ag2/.venv/lib/python3.14/site-packages\n",
      "Requires: anyio, distro, google-auth, httpx, pydantic, requests, sniffio, tenacity, typing-extensions, websockets\n",
      "Required-by: google-cloud-aiplatform\n"
     ]
    }
   ],
   "source": [
    "! pip show google-genai"
   ]
  }
 ],
 "metadata": {
  "front_matter": {
   "description": "Google Gemini V2 Client Example",
   "tags": [
    "google",
    "gemini",
    "ModelClientV2",
    "client",
    "v2"
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
