{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# AG2 + Gemini V2 Client Example\n",
    "\n",
    "Author: [Your Name]\n",
    "\n",
    "This notebook demonstrates the **Gemini V2 Client** (`api_type: \"gemini_v2\"`), which implements the ModelClientV2 protocol and returns rich `UnifiedResponse` objects with typed content blocks.\n",
    "\n",
    "## Key Features of Gemini V2 Client\n",
    "\n",
    "- **Rich Content Support**: Access reasoning blocks, multimodal content (images, audio, video), and tool calls\n",
    "- **Provider Agnostic**: Unified format compatible with other V2 clients (OpenAI, Anthropic, Bedrock)\n",
    "- **Type Safety**: Typed content blocks with Pydantic validation\n",
    "- **Direct Property Access**: Use `response.text`, `response.reasoning`, etc. instead of parsing nested structures\n",
    "- **Thinking Config Support**: Full support for Gemini's thinking features (`thinking_budget`, `thinking_level`, `include_thoughts`)\n",
    "- **Structured Outputs**: Support for Pydantic models and JSON schemas\n",
    "\n",
    "## Installation\n",
    "\n",
    "sh\n",
    "pip install ag2[gemini]\n",
    "```\n",
    "\n",
    "## Why Use V2 Client?\n",
    "\n",
    "The V2 client (`api_type: \"gemini_v2\"`) provides several advantages over the V1 client (`api_type: \"google\"`):\n",
    "\n",
    "1. **Rich Content Access**: Direct access to reasoning blocks, multimodal content, and citations\n",
    "2. **Unified Interface**: Same response format across all providers (OpenAI, Anthropic, Gemini, Bedrock)\n",
    "3. **Forward Compatible**: Handles new content types without code changes\n",
    "4. **Better Developer Experience**: Type-safe content blocks with direct property access\n",
    "5. **Full Thinking Support**: Complete support for Gemini 3 thinking features\n",
    "\n",
    "Reference: [ModelClientV2 Migration Guide](https://github.com/microsoft/autogen/blob/main/autogen/llm_clients/MIGRATION_TO_V2.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from autogen import ConversableAgent, GroupChat, GroupChatManager, LLMConfig\n",
    "from autogen.llm_clients import GeminiV2Client, UnifiedResponse\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"GEMINI_API_KEY is not set. Please set it in your environment or .env file.\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Basic Usage: Direct Client Access\n",
    "\n",
    "The Gemini V2 client can be used directly to get rich `UnifiedResponse` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gemini V2 client directly\n",
    "client = GeminiV2Client(api_key=api_key)\n",
    "\n",
    "# Create a completion\n",
    "response = client.create({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Say 'Hello' in one word.\"}],\n",
    "})\n",
    "\n",
    "# Verify it's a UnifiedResponse\n",
    "print(f\"Response type: {type(response)}\")\n",
    "print(f\"Is UnifiedResponse: {isinstance(response, UnifiedResponse)}\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Text: {response.text}\")\n",
    "print(f\"Usage: {response.usage}\")\n",
    "print(f\"Cost: ${response.cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Accessing Rich Content Blocks\n",
    "\n",
    "The V2 client preserves all content types in structured blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response with rich content\n",
    "response = client.create({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in 2 sentences.\"}],\n",
    "})\n",
    "\n",
    "# Access text content directly\n",
    "print(\"=== Text Content ===\")\n",
    "print(response.text)\n",
    "print()\n",
    "\n",
    "# Access individual messages\n",
    "print(\"=== Messages ===\")\n",
    "for idx, message in enumerate(response.messages):\n",
    "    print(f\"Message {idx} (role: {message.role}):\")\n",
    "    for content_block in message.content:\n",
    "        print(f\"  - Type: {content_block.type}, Content: {str(content_block)[:100]}...\")\n",
    "    print()\n",
    "\n",
    "# Get content by type\n",
    "text_blocks = response.get_content_by_type(\"text\")\n",
    "print(f\"Number of text blocks: {len(text_blocks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Thinking Config Support (Gemini 3 Models)\n",
    "\n",
    "The V2 client fully supports Gemini's thinking features. This is especially powerful with Gemini 3 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using thinking_level with Gemini 3 Pro\n",
    "prompt = \"\"\"You are playing the 20 question game. You know that what you are looking for\n",
    "is an aquatic mammal that doesn't live in the sea, is venomous and that's\n",
    "smaller than a cat. What could that be and how could you make sure?\n",
    "\"\"\"\n",
    "\n",
    "response = client.create({\n",
    "    \"model\": \"gemini-3-pro-preview\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    \"thinking_level\": \"High\",  # Use thinking_level for Gemini 3 Pro\n",
    "    \"include_thoughts\": True,  # Include thought summaries in response\n",
    "})\n",
    "\n",
    "print(\"=== Response with Thinking ===\")\n",
    "print(response.text)\n",
    "print()\n",
    "\n",
    "# Access reasoning blocks if present\n",
    "reasoning_blocks = response.get_content_by_type(\"reasoning\")\n",
    "if reasoning_blocks:\n",
    "    print(\"=== Reasoning Blocks ===\")\n",
    "    for reasoning in reasoning_blocks:\n",
    "        print(f\"Reasoning: {reasoning.reasoning[:200]}...\")\n",
    "else:\n",
    "    print(\"No reasoning blocks found (thoughts may be included in text)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using thinking_budget with Gemini 2.5 Flash\n",
    "response = client.create({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "    \"thinking_budget\": 4096,  # Token budget for thinking\n",
    "})\n",
    "\n",
    "print(\"=== Response with Thinking Budget ===\")\n",
    "print(response.text)\n",
    "print(f\"\\nTokens used: {response.usage.get('total_tokens', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Structured Outputs with Pydantic Models\n",
    "\n",
    "The V2 client supports structured outputs using Pydantic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Pydantic model for structured output\n",
    "class Answer(BaseModel):\n",
    "    answer: str\n",
    "    confidence: float\n",
    "    reasoning: str | None = None\n",
    "\n",
    "\n",
    "# Create client with response format\n",
    "structured_client = GeminiV2Client(api_key=api_key, response_format=Answer)\n",
    "\n",
    "# Get structured response\n",
    "response = structured_client.create({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is 2+2? Answer with confidence score.\"}],\n",
    "})\n",
    "\n",
    "print(\"=== Structured Response ===\")\n",
    "print(response.text)\n",
    "print()\n",
    "\n",
    "# The response text contains JSON matching the schema\n",
    "import json\n",
    "\n",
    "try:\n",
    "    structured_data = json.loads(response.text)\n",
    "    print(\"Parsed structured data:\")\n",
    "    print(json.dumps(structured_data, indent=2))\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Response is not valid JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Tool/Function Calling\n",
    "\n",
    "The V2 client preserves tool calls as `ToolCallContent` blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"Temperature unit\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create request with tools\n",
    "response = client.create({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}],\n",
    "    \"tools\": tools,\n",
    "})\n",
    "\n",
    "print(\"=== Response with Tool Calls ===\")\n",
    "print(f\"Text: {response.text}\")\n",
    "print()\n",
    "\n",
    "# Access tool calls\n",
    "tool_calls = response.messages[0].get_tool_calls()\n",
    "if tool_calls:\n",
    "    print(\"=== Tool Calls ===\")\n",
    "    for tool_call in tool_calls:\n",
    "        print(f\"Tool: {tool_call.name}\")\n",
    "        print(f\"ID: {tool_call.id}\")\n",
    "        print(f\"Arguments: {tool_call.arguments}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No tool calls in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6. Using with LLMConfig (Recommended for Agents)\n",
    "\n",
    "The V2 client can be used with `LLMConfig` for agent integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LLM with V2 client\n",
    "llm_config_v2 = LLMConfig(\n",
    "    config_list=[\n",
    "        {\n",
    "            \"api_type\": \"gemini_v2\",  # Use gemini_v2 for V2 client\n",
    "            \"model\": \"gemini-2.5-flash\",\n",
    "            \"api_key\": api_key,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 500,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create agent with V2 client\n",
    "agent_v2 = ConversableAgent(\n",
    "    name=\"assistant_v2\",\n",
    "    llm_config=llm_config_v2,\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    ")\n",
    "\n",
    "# Use the agent\n",
    "response = agent_v2.generate_reply(messages=[{\"role\": \"user\", \"content\": \"Explain machine learning in one sentence.\"}])\n",
    "\n",
    "print(\"=== Agent Response ===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 7. V1 vs V2 Comparison\n",
    "\n",
    "Compare the V1 and V2 client responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 Client (legacy)\n",
    "llm_config_v1 = LLMConfig(\n",
    "    config_list=[\n",
    "        {\n",
    "            \"api_type\": \"google\",  # V1 client\n",
    "            \"model\": \"gemini-2.5-flash\",\n",
    "            \"api_key\": api_key,\n",
    "            \"response_format\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "agent_v1 = ConversableAgent(\n",
    "    name=\"assistant_v1\",\n",
    "    llm_config=llm_config_v1,\n",
    ")\n",
    "\n",
    "# V2 Client\n",
    "agent_v2 = ConversableAgent(\n",
    "    name=\"assistant_v2\",\n",
    "    llm_config=llm_config_v2,\n",
    ")\n",
    "\n",
    "test_message = \"What is the capital of France?\"\n",
    "\n",
    "print(\"=== V1 Client Response ===\")\n",
    "response_v1 = agent_v1.generate_reply(messages=[{\"role\": \"user\", \"content\": test_message}])\n",
    "print(f\"Type: {type(response_v1)}\")\n",
    "print(f\"Response: {response_v1}\")\n",
    "print()\n",
    "\n",
    "print(\"=== V2 Client Response ===\")\n",
    "response_v2 = agent_v2.generate_reply(messages=[{\"role\": \"user\", \"content\": test_message}])\n",
    "print(f\"Type: {type(response_v2)}\")\n",
    "print(f\"Response: {response_v2}\")\n",
    "print()\n",
    "\n",
    "print(\"Note: Both responses work with agents, but V2 provides richer content access when using the client directly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 8. Group Chat with V2 Client\n",
    "\n",
    "The V2 client works seamlessly in group chats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agents with V2 client\n",
    "planner = ConversableAgent(\n",
    "    name=\"planner\",\n",
    "    llm_config=LLMConfig(\n",
    "        config_list=[\n",
    "            {\n",
    "                \"api_type\": \"gemini_v2\",\n",
    "                \"model\": \"gemini-2.5-flash\",\n",
    "                \"api_key\": api_key,\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    system_message=\"You are a planning assistant. Create detailed plans.\",\n",
    "    description=\"Creates plans\",\n",
    ")\n",
    "\n",
    "reviewer = ConversableAgent(\n",
    "    name=\"reviewer\",\n",
    "    llm_config=LLMConfig(\n",
    "        config_list=[\n",
    "            {\n",
    "                \"api_type\": \"gemini_v2\",\n",
    "                \"model\": \"gemini-2.5-flash\",\n",
    "                \"api_key\": api_key,\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    system_message=\"You are a review assistant. Provide constructive feedback.\",\n",
    "    description=\"Reviews plans\",\n",
    ")\n",
    "\n",
    "executor = ConversableAgent(\n",
    "    name=\"executor\",\n",
    "    llm_config=LLMConfig(\n",
    "        config_list=[\n",
    "            {\n",
    "                \"api_type\": \"gemini_v2\",\n",
    "                \"model\": \"gemini-2.5-flash\",\n",
    "                \"api_key\": api_key,\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    system_message=\"You are an execution assistant. Implement plans.\",\n",
    "    description=\"Executes plans\",\n",
    ")\n",
    "\n",
    "# Create group chat\n",
    "groupchat = GroupChat(\n",
    "    agents=[planner, reviewer, executor],\n",
    "    speaker_selection_method=\"auto\",\n",
    "    messages=[],\n",
    ")\n",
    "\n",
    "manager = GroupChatManager(\n",
    "    name=\"manager\",\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config_v2,\n",
    "    # is_termination_msg=lambda x: \"DONE\" in (x.get(\"content\", \"\") or \"\").upper(),\n",
    ")\n",
    "\n",
    "# Start conversation\n",
    "chat_result = planner.initiate_chat(\n",
    "    recipient=manager,\n",
    "    message=\"Create a plan for organizing a small team meeting. Say DONE when finished.\",\n",
    "    max_turns=3,\n",
    ")\n",
    "\n",
    "print(\"=== Group Chat History ===\")\n",
    "for msg in chat_result.chat_history:\n",
    "    print(f\"{msg.get('name', 'unknown')}: {msg.get('content', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 9. Group Chat with Structured Outputs\n",
    "\n",
    "Combine V2 client with structured outputs in a group chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define structured output model\n",
    "class Plan(BaseModel):\n",
    "    title: str\n",
    "    steps: list[str]\n",
    "    estimated_time: str\n",
    "    resources_needed: list[str]\n",
    "\n",
    "\n",
    "# Create agent with structured output\n",
    "structured_agent = ConversableAgent(\n",
    "    name=\"structured_planner\",\n",
    "    llm_config=LLMConfig(\n",
    "    config_list=[{\n",
    "        \"api_type\": \"gemini_v2\",\n",
    "        \"model\": \"gemini-2.5-flash\",\n",
    "        \"api_key\": os.environ.get(\"GEMINI_API_KEY\"),\n",
    "        # \"temperature\": 0,\n",
    "    \"response_format\":Plan\n",
    "    }],\n",
    "),\n",
    "    system_message=\"You are a planning assistant. Always respond with structured plans.\",\n",
    ")\n",
    "\n",
    "# Get structured response\n",
    "response = structured_agent.run(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Create a plan for learning Python in 30 days.\"}],\n",
    "    max_turns=3,\n",
    ")\n",
    "\n",
    "print(\"=== Structured Plan ===\")\n",
    "print(response)\n",
    "print()\n",
    "\n",
    "# Parse the structured response\n",
    "import json\n",
    "\n",
    "try:\n",
    "    plan_data = json.loads(response)\n",
    "    print(\"=== Parsed Plan ===\")\n",
    "    print(f\"Title: {plan_data.get('title')}\")\n",
    "    print(f\"Steps: {len(plan_data.get('steps', []))}\")\n",
    "    print(f\"Estimated Time: {plan_data.get('estimated_time')}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Response is not valid JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 10. Cost and Usage Tracking\n",
    "\n",
    "The V2 client provides detailed cost and usage information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response\n",
    "response = client.create({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain the theory of relativity.\"}],\n",
    "    \"max_tokens\": 500,\n",
    "})\n",
    "\n",
    "# Access usage information\n",
    "usage = GeminiV2Client.get_usage(response)\n",
    "cost = client.cost(response)\n",
    "\n",
    "print(\"=== Usage Information ===\")\n",
    "print(f\"Prompt tokens: {usage['prompt_tokens']}\")\n",
    "print(f\"Completion tokens: {usage['completion_tokens']}\")\n",
    "print(f\"Total tokens: {usage['total_tokens']}\")\n",
    "print(f\"Cost: ${cost:.6f}\")\n",
    "print(f\"Model: {usage['model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 11. Backward Compatibility: create_v1_compatible()\n",
    "\n",
    "The V2 client provides backward compatibility with V1 code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get V1-compatible response\n",
    "v1_response = client.create_v1_compatible({\n",
    "    \"model\": \"gemini-2.5-flash\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "})\n",
    "\n",
    "print(\"=== V1-Compatible Response ===\")\n",
    "print(f\"Type: {type(v1_response)}\")\n",
    "print(f\"Keys: {list(v1_response.keys())}\")\n",
    "print(f\"Model: {v1_response.get('model')}\")\n",
    "print(f\"Choices: {len(v1_response.get('choices', []))}\")\n",
    "print(f\"Usage: {v1_response.get('usage')}\")\n",
    "print(f\"Cost: {v1_response.get('cost')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Gemini V2 client (`api_type: \"gemini_v2\"`) provides:\n",
    "\n",
    "✅ **Rich Content Access**: Direct access to reasoning blocks, multimodal content, and tool calls\n",
    "✅ **Unified Interface**: Same response format across all providers\n",
    "✅ **Full Thinking Support**: Complete support for Gemini 3 thinking features\n",
    "✅ **Structured Outputs**: Support for Pydantic models and JSON schemas\n",
    "✅ **Backward Compatible**: Works with existing agent code via `create_v1_compatible()`\n",
    "✅ **Type Safe**: Typed content blocks with Pydantic validation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore multimodal content (images, audio, video)\n",
    "- Try different Gemini models (2.5 Flash, 2.5 Pro, 3 Pro Preview)\n",
    "- Experiment with thinking configurations\n",
    "- Integrate with other V2 clients in group chats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from autogen import ConversableAgent, LLMConfig\n",
    "\n",
    "\n",
    "class Extra(BaseModel):\n",
    "    notes: str\n",
    "\n",
    "\n",
    "\n",
    "class Output(BaseModel):\n",
    "    is_good: bool\n",
    "\n",
    "    extra: dict[str, Extra]\n",
    "\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    config_list={\n",
    "        \"api_type\": \"google\",\n",
    "        \"model\": \"gemini-2.5-flash\",\n",
    "        \"api_key\": os.environ.get(\"GEMINI_API_KEY\"),\n",
    "        \"temperature\": 0,\n",
    "        \"response_format\":Output\n",
    "    },\n",
    ")\n",
    "\n",
    "bot = ConversableAgent(\n",
    "    name=\"bot\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a smart assistant.\",\n",
    ")\n",
    "\n",
    "response = bot.run(\n",
    "    message=\"Think about the weather in paris, and return any information you find.\",\n",
    "    max_turns=1\n",
    ")\n",
    "result = response.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip show google-genai"
   ]
  }
 ],
 "metadata": {
  "front_matter": {
   "description": "Google Gemini V2 Client Example",
   "tags": [
    "google",
    "gemini",
    "ModelClientV2",
    "client",
    "v2"
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
