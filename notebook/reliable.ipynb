{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88cd3b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 18:24:50,466 - autogen.tools.experimental.reliable.reliable - INFO - --- Starting ReliableTool 'SubQuestionGenerator' Internal Group Chat (Sync) (Attempt 1 / 3 Max) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported ReliableTool, AgentConfig, ReliableToolError, ContextVariables from autogen.tools.experimental.reliable\n",
      "Configured to use local model at: http://192.168.0.44:1234/v1, Model name: gemma-3-12b-it-qat\n",
      "\n",
      "Attempting to reliably generate 3 sub-questions for: 'How many people live in the busiest city in the US?'\n",
      "------------------------------\n",
      "\u001b[33m_User\u001b[0m (to chat_manager):\n",
      "\n",
      "Start Reliable Task: Generate exactly 3 relevant sub-questions for the main question: 'How many people live in the busiest city in the US?'\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SubQuestionGenerator_Runner\n",
      "\u001b[0m\n",
      "\u001b[33mSubQuestionGenerator_Runner\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (616817442): execute_generate_sub_questions_attempt *****\u001b[0m\n",
      "Arguments: \n",
      "{\"hypothesis\":\"The function will return a list of 3 sub-questions related to determining the population of the busiest city in the US.\",\"sub_questions\":[\"What is considered the 'busiest' city in the United States?\",\"Which data sources can be used to determine the population of that city?\",\"How does the population number change over time?\"]}\n",
      "\u001b[32m***********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: _Group_Tool_Executor\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION execute_generate_sub_questions_attempt...\n",
      "Call ID: 616817442\n",
      "Input arguments: {'hypothesis': 'The function will return a list of 3 sub-questions related to determining the population of the busiest city in the US.', 'sub_questions': [\"What is considered the 'busiest' city in the United States?\", 'Which data sources can be used to determine the population of that city?', 'How does the population number change over time?']}\u001b[0m\n",
      "\u001b[33m_Group_Tool_Executor\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (616817442) *****\u001b[0m\n",
      "[\"What is considered the 'busiest' city in the United States?\", 'Which data sources can be used to determine the population of that city?', 'How does the population number change over time?']\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SubQuestionGenerator_Validator\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 18:24:50,513 - autogen.tools.experimental.reliable.reliable - INFO - Validator Hook: PASSED. Justification: The output is a Python list of strings. It contains exactly 3 sub-questions, and each question is relevant to determining the population of the busiest city in the US.\n",
      "2025-04-28 18:24:50,514 - autogen.tools.experimental.reliable.reliable - INFO - Validator hook: Updated latest attempt (1) validation status.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mSubQuestionGenerator_Validator\u001b[0m (to chat_manager):\n",
      "\n",
      "{\"validation_result\":true,\"justification\":\"The output is a Python list of strings. It contains exactly 3 sub-questions, and each question is relevant to determining the population of the busiest city in the US.\"}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (cd2bbd79-97e2-4af0-b0ca-21b5706dfc8d): No next speaker selected\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 18:24:50,515 - autogen.tools.experimental.reliable.reliable - INFO - --- ReliableTool 'SubQuestionGenerator' Internal Group Chat Finished ---\n",
      "2025-04-28 18:24:50,516 - autogen.tools.experimental.reliable.reliable - INFO - ReliableTool 'SubQuestionGenerator' completed successfully and validated after 1 attempt(s).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "\n",
      "✅ Successfully generated sub-questions:\n",
      "   1. What is considered the 'busiest' city in the United States?\n",
      "   2. Which data sources can be used to determine the population of that city?\n",
      "   3. How does the population number change over time?\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Using ReliableTool to Generate Sub-Questions\n",
    "#\n",
    "# This notebook demonstrates how to use the `ReliableTool` to take a user's question and reliably generate exactly 3 sub-questions related to it.\n",
    "#\n",
    "# We will:\n",
    "# 1. Define a simple (potentially unreliable) function to generate sub-questions.\n",
    "# 2. Configure a `ReliableTool` instance to wrap this function.\n",
    "#    - The *runner* agent will try to call the function.\n",
    "#    - The *validator* agent will check if the output is a list of exactly 3 relevant sub-questions.\n",
    "# 3. Run the tool with a sample question.\n",
    "\n",
    "# %%\n",
    "import asyncio\n",
    "import autogen\n",
    "from typing import List, Any, Optional\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "# Assuming reliable.py is now in the same directory, we can use a direct import.\n",
    "try:\n",
    "    from autogen.tools.experimental.reliable import ReliableTool, AgentConfig, ReliableToolError\n",
    "    from autogen.agentchat.group import (\n",
    "            ContextVariables,\n",
    "        )\n",
    "    print(\"Successfully imported ReliableTool, AgentConfig, ReliableToolError, ContextVariables from autogen.tools.experimental.reliable\")\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")\n",
    "# %% [markdown]\n",
    "# ## 1. Define the Core Function (Potentially Unreliable)\n",
    "#\n",
    "# This function takes a question and tries to return a list of sub-questions. It might not always succeed or return the correct number.\n",
    "\n",
    "# %%\n",
    "def generate_sub_questions_attempt(sub_questions:List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Attempts to sub-questions based on the main question.\n",
    "\n",
    "    Args:\n",
    "        sub_questions: The desired number of sub-questions.\n",
    "\n",
    "    Returns:\n",
    "        A list of sub-questions.\n",
    "    \"\"\"\n",
    "    return sub_questions\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Configure LLM and ReliableTool\n",
    "#\n",
    "# We need LLM configurations for the internal runner and validator agents. The validator's system message is crucial for ensuring the output meets our criteria (exactly 3 questions).\n",
    "\n",
    "# %%\n",
    "local_model_endpoint = \"http://192.168.0.44:1234/v1\"  # Standard OpenAI API path is /v1\n",
    "# You might need to adjust the model name depending on what your local server expects.\n",
    "local_model_name = \"gemma-3-12b-it-qat\"  # Updated model name\n",
    "\n",
    "try:\n",
    "    config_list = [\n",
    "        {\n",
    "            \"model\": local_model_name,\n",
    "            \"base_url\": local_model_endpoint,\n",
    "            \"api_key\": \"NotNeeded\",  # Set to None or a placeholder string if no key is required\n",
    "            # \"api_type\": \"openai\", # Explicitly set type if needed, often inferred\n",
    "        }\n",
    "    ]\n",
    "    llm_config = {\n",
    "        \"config_list\": config_list,\n",
    "        \"cache_seed\": 42,  # Use caching\n",
    "        \"temperature\": 0.5,  # Adjust temperature as needed for your local model\n",
    "    }\n",
    "    print(f\"Configured to use local model at: {local_model_endpoint}, Model name: {local_model_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # This catch block might be less relevant now but kept for safety\n",
    "    print(f\"Error creating local config: {e}\")\n",
    "    llm_config = None\n",
    "# --- Agent Configurations ---\n",
    "\n",
    "# Runner: Tries to call the function\n",
    "runner_agent_config = AgentConfig(\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are an assistant that helps break down questions. Use the provided tool to generate sub-questions.\",\n",
    ")\n",
    "\n",
    "# Validator: Checks if the result is a list of exactly 3 relevant sub-questions\n",
    "validator_agent_config = AgentConfig(\n",
    "    llm_config=llm_config, # Validator needs an LLM\n",
    "    system_message=f\"\"\"You are a quality control assistant. Your task is to validate the output of a function that should generate sub-questions.\n",
    "\n",
    "    **Validation Criteria:**\n",
    "    1.  **Correct Format:** The output MUST be a Python list of strings.\n",
    "    2.  **Correct Quantity:** The list MUST contain exactly 3 sub-questions.\n",
    "    3.  **Relevance:** Each sub-question MUST be relevant to the original main question provided in the context.\n",
    "\n",
    "    Analyze the function result provided in the user message. Respond with your validation assessment in the required JSON format (ValidationResult).\n",
    "    - If all criteria are met, set `validation_result` to `true`.\n",
    "    - If any criterion is not met, set `validation_result` to `false` and clearly state the reason in the `justification`.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# --- Instantiate ReliableTool ---\n",
    "\n",
    "if llm_config: # Only proceed if LLM config is loaded\n",
    "    sub_question_tool = ReliableTool(\n",
    "        name=\"SubQuestionGenerator\",\n",
    "        func_or_tool=generate_sub_questions_attempt, # The function to wrap\n",
    "        description=\"Reliably generates exactly 3 sub-questions for a given main question.\",\n",
    "        runner_config=runner_agent_config,\n",
    "        validator_config=validator_agent_config,\n",
    "        max_retries=2, # Allow 2 retries (total 3 attempts)\n",
    "    )\n",
    "else:\n",
    "    print(\"LLM Configuration not loaded. Cannot create ReliableTool.\")\n",
    "    sub_question_tool = None\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Get User Input and Run the Tool\n",
    "\n",
    "# %%\n",
    "async def run_sub_question_generation():\n",
    "    if not sub_question_tool:\n",
    "        print(\"ReliableTool was not initialized due to missing LLM config.\")\n",
    "        return\n",
    "\n",
    "    # main_question = input(\"Please enter the main question you want to break down: \")\n",
    "    main_question = \"How many people live in the busiest city in the US?\"\n",
    "\n",
    "    if not main_question:\n",
    "        print(\"No question entered.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nAttempting to reliably generate 3 sub-questions for: '{main_question}'\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    try:\n",
    "        # We don't need complex context here, but pass an empty one\n",
    "        initial_context = ContextVariables()\n",
    "\n",
    "        # Run the tool - the 'task' description helps guide the internal agents\n",
    "        result = await sub_question_tool.a_run(\n",
    "            task=f\"Generate exactly 3 relevant sub-questions for the main question: '{main_question}'\",\n",
    "            context_variables=initial_context\n",
    "        )\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "        print(\"\\n✅ Successfully generated sub-questions:\")\n",
    "        if isinstance(result, list):\n",
    "            for i, sq in enumerate(result):\n",
    "                print(f\"   {i+1}. {sq}\")\n",
    "        else:\n",
    "            print(f\"   Unexpected result format: {result}\")\n",
    "\n",
    "    except ReliableToolError as e:\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"\\n❌ Failed to generate sub-questions after multiple attempts.\")\n",
    "        print(f\"Error: {e}\")\n",
    "        # You can inspect e.final_context for detailed history if needed\n",
    "        # print(\"\\n--- Final Context ---\")\n",
    "        # print(e.final_context.model_dump_json(indent=2))\n",
    "    except Exception as e:\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"\\n❌ An unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function\n",
    "# In a Jupyter notebook, you might need to use nest_asyncio or ensure an event loop is running\n",
    "# For simplicity, we use asyncio.run here. If you get a RuntimeError about nested loops,\n",
    "# you might need `nest_asyncio.apply()` at the start of your notebook.\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "asyncio.run(run_sub_question_generation())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Explanation\n",
    "#\n",
    "# 1.  **`generate_sub_questions_attempt`**: This function simulates the core logic. We made it intentionally unreliable (sometimes returning 2 or 4 questions) to show how `ReliableTool` handles retries.\n",
    "# 2.  **`AgentConfig`**: We define configurations for the internal runner and validator. The validator's system prompt is key – it explicitly tells the LLM to check for a list of *exactly 3* relevant questions.\n",
    "# 3.  **`ReliableTool`**: We instantiate the tool, passing the function to wrap, the agent configs, and setting `max_retries`.\n",
    "# 4.  **`sub_question_tool.run()`**: We call the `run` method.\n",
    "#     *   The `task` parameter provides high-level instructions for the overall goal.\n",
    "#     *   The `ReliableTool` orchestrates the internal group chat:\n",
    "#         *   The runner agent calls `generate_sub_questions_attempt`.\n",
    "#         *   The validator agent receives the result (or error) and checks it against its system prompt criteria (list? 3 items? relevant?).\n",
    "#         *   If validation fails, the `ReliableTool` triggers another attempt (up to `max_retries`), potentially guiding the runner with the validation failure reason.\n",
    "#         *   If validation passes, `run` returns the validated result.\n",
    "#         *   If all attempts fail validation, `run` raises a `ReliableToolError`.\n",
    "# 5.  **Output**: The notebook prints the final list of 3 sub-questions if successful, or an error message if the tool failed after retries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73529cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported ReliableTool, AgentConfig, ReliableToolError, ContextVariables from autogen.tools.experimental.reliable\n",
      "Configured to use local model at: http://192.168.0.44:1234/v1, Model name: gemma-3-12b-it-qat\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LLMConfig' object has no attribute 'setdefault'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ag2/autogen/llm_config.py:191\u001b[39m, in \u001b[36mLLMConfig.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getattr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ag2/autogen/llm_config.py:170\u001b[39m, in \u001b[36mLLMConfig._getattr\u001b[39m\u001b[34m(self, o, name)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_getattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, o: \u001b[38;5;28mobject\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     val = \u001b[38;5;28mgetattr\u001b[39m(o, name)\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ag2/.venv/lib/python3.11/site-packages/pydantic/main.py:891\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    890\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m891\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: '_LLMConfig' object has no attribute 'setdefault'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# --- Instantiate ReliableTool ---\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m llm_config: \u001b[38;5;66;03m# Only proceed if LLM config is loaded\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     sub_question_tool = \u001b[43mReliableTool\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSubQuestionGenerator\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc_or_tool\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerate_sub_questions_attempt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The function to wrap\u001b[39;49;00m\n\u001b[32m    105\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReliably generates exactly 3 sub-questions for a given main question.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrunner_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrunner_agent_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidator_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidator_agent_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Allow 2 retries (total 3 attempts)\u001b[39;49;00m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLLM Configuration not loaded. Cannot create ReliableTool.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ag2/autogen/tools/experimental/reliable/reliable.py:398\u001b[39m, in \u001b[36mReliableTool.__init__\u001b[39m\u001b[34m(self, name, func_or_tool, runner_config, validator_config, description, max_retries, agent_kwargs, enable_dynamic_validation)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28mself\u001b[39m._validator = \u001b[38;5;28mself\u001b[39m._setup_validator_agent()\n\u001b[32m    395\u001b[39m \u001b[38;5;28mself\u001b[39m._reliable_func_wrapper = reliable_function_wrapper(\n\u001b[32m    396\u001b[39m     \u001b[38;5;28mself\u001b[39m._original_func, \u001b[38;5;28mself\u001b[39m._validator, \u001b[38;5;28mself\u001b[39m._context_variables_key \u001b[38;5;66;03m# Pass validator instance\u001b[39;00m\n\u001b[32m    397\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28mself\u001b[39m._runner = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_runner_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28mself\u001b[39m._register_internal_hooks()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ag2/autogen/tools/experimental/reliable/reliable.py:452\u001b[39m, in \u001b[36mReliableTool._setup_runner_agent\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# Ensure runner's LLM config includes tool schema and removes conflicting response_format\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m runner.llm_config:\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetdefault\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m, []).append(internal_tool.tool_schema)\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# Ensure no response_format conflicts\u001b[39;00m\n\u001b[32m    454\u001b[39m     runner.llm_config.pop(\u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/ag2/autogen/llm_config.py:193\u001b[39m, in \u001b[36mLLMConfig.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getattr(\u001b[38;5;28mself\u001b[39m._model, name)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'LLMConfig' object has no attribute 'setdefault'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Using ReliableTool to Generate Sub-Questions\n",
    "#\n",
    "# This notebook demonstrates how to use the `ReliableTool` to take a user's question and reliably generate exactly 3 sub-questions related to it.\n",
    "#\n",
    "# We will:\n",
    "# 1. Define a simple (potentially unreliable) function to generate sub-questions.\n",
    "# 2. Configure a `ReliableTool` instance to wrap this function.\n",
    "#    - The *runner* agent will try to call the function.\n",
    "#    - The *validator* agent will check if the output is a list of exactly 3 relevant sub-questions.\n",
    "# 3. Run the tool with a sample question.\n",
    "\n",
    "# %%\n",
    "import asyncio\n",
    "import autogen\n",
    "from typing import List, Any, Optional\n",
    "\n",
    "# Assuming reliable.py is now in the same directory, we can use a direct import.\n",
    "try:\n",
    "    from autogen.tools.experimental.reliable import ReliableTool, AgentConfig, ReliableToolError\n",
    "    from autogen.agentchat.group import (\n",
    "            ContextVariables,\n",
    "        )\n",
    "    print(\"Successfully imported ReliableTool, AgentConfig, ReliableToolError, ContextVariables from autogen.tools.experimental.reliable\")\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError: {e}\")\n",
    "# %% [markdown]\n",
    "# ## 1. Define the Core Function (Potentially Unreliable)\n",
    "#\n",
    "# This function takes a question and tries to return a list of sub-questions. It might not always succeed or return the correct number.\n",
    "\n",
    "# %%\n",
    "def generate_sub_questions_attempt(sub_questions:List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Attempts to sub-questions based on the main question.\n",
    "\n",
    "    Args:\n",
    "        sub_questions: The desired number of sub-questions.\n",
    "\n",
    "    Returns:\n",
    "        A list of sub-questions.\n",
    "    \"\"\"\n",
    "    return sub_questions\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Configure LLM and ReliableTool\n",
    "#\n",
    "# We need LLM configurations for the internal runner and validator agents. The validator's system message is crucial for ensuring the output meets our criteria (exactly 3 questions).\n",
    "\n",
    "# %%\n",
    "local_model_endpoint = \"http://192.168.0.44:1234/v1\"  # Standard OpenAI API path is /v1\n",
    "# You might need to adjust the model name depending on what your local server expects.\n",
    "local_model_name = \"gemma-3-12b-it-qat\"  # Updated model name\n",
    "\n",
    "try:\n",
    "    config_list = [\n",
    "        {\n",
    "            \"model\": local_model_name,\n",
    "            \"base_url\": local_model_endpoint,\n",
    "            \"api_key\": \"NotNeeded\",  # Set to None or a placeholder string if no key is required\n",
    "            # \"api_type\": \"openai\", # Explicitly set type if needed, often inferred\n",
    "        }\n",
    "    ]\n",
    "    llm_config = {\n",
    "        \"config_list\": config_list,\n",
    "        \"cache_seed\": 42,  # Use caching\n",
    "        \"temperature\": 0.5,  # Adjust temperature as needed for your local model\n",
    "    }\n",
    "    print(f\"Configured to use local model at: {local_model_endpoint}, Model name: {local_model_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    # This catch block might be less relevant now but kept for safety\n",
    "    print(f\"Error creating local config: {e}\")\n",
    "    llm_config = None\n",
    "# --- Agent Configurations ---\n",
    "\n",
    "# Runner: Tries to call the function\n",
    "runner_agent_config = AgentConfig(\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are an assistant that helps break down questions. Use the provided tool to generate sub-questions.\",\n",
    ")\n",
    "\n",
    "# Validator: Checks if the result is a list of exactly 3 relevant sub-questions\n",
    "validator_agent_config = AgentConfig(\n",
    "    llm_config=llm_config, # Validator needs an LLM\n",
    "    system_message=f\"\"\"You are a quality control assistant. Your task is to validate the output of a function that should generate sub-questions.\n",
    "\n",
    "    **Validation Criteria:**\n",
    "    1.  **Correct Format:** The output MUST be a Python list of strings.\n",
    "    2.  **Correct Quantity:** The list MUST contain exactly 3 sub-questions.\n",
    "    3.  **Relevance:** Each sub-question MUST be relevant to the original main question provided in the context.\n",
    "\n",
    "    Analyze the function result provided in the user message. Respond with your validation assessment in the required JSON format (ValidationResult).\n",
    "    - If all criteria are met, set `validation_result` to `true`.\n",
    "    - If any criterion is not met, set `validation_result` to `false` and clearly state the reason in the `justification`.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# --- Instantiate ReliableTool ---\n",
    "\n",
    "if llm_config: # Only proceed if LLM config is loaded\n",
    "    sub_question_tool = ReliableTool(\n",
    "        name=\"SubQuestionGenerator\",\n",
    "        func_or_tool=generate_sub_questions_attempt, # The function to wrap\n",
    "        description=\"Reliably generates exactly 3 sub-questions for a given main question.\",\n",
    "        runner_config=runner_agent_config,\n",
    "        validator_config=validator_agent_config,\n",
    "        max_retries=2, # Allow 2 retries (total 3 attempts)\n",
    "    )\n",
    "else:\n",
    "    print(\"LLM Configuration not loaded. Cannot create ReliableTool.\")\n",
    "    sub_question_tool = None\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Get User Input and Run the Tool\n",
    "\n",
    "# %%\n",
    "def run_sub_question_generation():\n",
    "    if not sub_question_tool:\n",
    "        print(\"ReliableTool was not initialized due to missing LLM config.\")\n",
    "        return\n",
    "\n",
    "    # main_question = input(\"Please enter the main question you want to break down: \")\n",
    "    main_question = \"How many people live in the busiest city in the US?\"\n",
    "\n",
    "    if not main_question:\n",
    "        print(\"No question entered.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nAttempting to reliably generate 3 sub-questions for: '{main_question}'\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    try:\n",
    "        # We don't need complex context here, but pass an empty one\n",
    "        initial_context = ContextVariables()\n",
    "\n",
    "        # Run the tool - the 'task' description helps guide the internal agents\n",
    "        result = sub_question_tool.run(\n",
    "            task=f\"Generate exactly 3 relevant sub-questions for the main question: '{main_question}'\",\n",
    "            context_variables=initial_context\n",
    "        )\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "        print(\"\\n✅ Successfully generated sub-questions:\")\n",
    "        if isinstance(result, list):\n",
    "            for i, sq in enumerate(result):\n",
    "                print(f\"   {i+1}. {sq}\")\n",
    "        else:\n",
    "            print(f\"   Unexpected result format: {result}\")\n",
    "\n",
    "    except ReliableToolError as e:\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"\\n❌ Failed to generate sub-questions after multiple attempts.\")\n",
    "        print(f\"Error: {e}\")\n",
    "        # You can inspect e.final_context for detailed history if needed\n",
    "        # print(\"\\n--- Final Context ---\")\n",
    "        # print(e.final_context.model_dump_json(indent=2))\n",
    "    except Exception as e:\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"\\n❌ An unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the async function\n",
    "# In a Jupyter notebook, you might need to use nest_asyncio or ensure an event loop is running\n",
    "# For simplicity, we use asyncio.run here. If you get a RuntimeError about nested loops,\n",
    "# you might need `nest_asyncio.apply()` at the start of your notebook.\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "run_sub_question_generation()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Explanation\n",
    "#\n",
    "# 1.  **`generate_sub_questions_attempt`**: This function simulates the core logic. We made it intentionally unreliable (sometimes returning 2 or 4 questions) to show how `ReliableTool` handles retries.\n",
    "# 2.  **`AgentConfig`**: We define configurations for the internal runner and validator. The validator's system prompt is key – it explicitly tells the LLM to check for a list of *exactly 3* relevant questions.\n",
    "# 3.  **`ReliableTool`**: We instantiate the tool, passing the function to wrap, the agent configs, and setting `max_retries`.\n",
    "# 4.  **`sub_question_tool.run()`**: We call the `run` method.\n",
    "#     *   The `task` parameter provides high-level instructions for the overall goal.\n",
    "#     *   The `ReliableTool` orchestrates the internal group chat:\n",
    "#         *   The runner agent calls `generate_sub_questions_attempt`.\n",
    "#         *   The validator agent receives the result (or error) and checks it against its system prompt criteria (list? 3 items? relevant?).\n",
    "#         *   If validation fails, the `ReliableTool` triggers another attempt (up to `max_retries`), potentially guiding the runner with the validation failure reason.\n",
    "#         *   If validation passes, `run` returns the validated result.\n",
    "#         *   If all attempts fail validation, `run` raises a `ReliableToolError`.\n",
    "# 5.  **Output**: The notebook prints the final list of 3 sub-questions if successful, or an error message if the tool failed after retries.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
