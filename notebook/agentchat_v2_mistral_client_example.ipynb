{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Mistral V2 Client Example\n",
    "\n",
    "This notebook demonstrates how to use the Mistral V2 client (ModelClientV2 architecture) with AG2 agents.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Rich UnifiedResponse**: Returns `UnifiedResponse` with typed content blocks\n",
    "- **Direct Property Access**: Use `response.text`, `response.get_content_by_type()` instead of parsing nested structures\n",
    "- **Tool Call Support**: Preserves tool calls as `ToolCallContent` blocks\n",
    "- **Backward Compatible**: Works seamlessly with existing AG2 agents\n",
    "\n",
    "## Setup\n",
    "\n",
    "Set your `MISTRAL_API_KEY` environment variable or provide it in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Configure Mistral V2 client\n",
    "llm_config_mistral_v2 = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"api_type\": \"mistral_v2\",  # <-- Key: use V2 client architecture\n",
    "            \"model\": \"mistral-small-latest\",\n",
    "            \"api_key\": os.getenv(\"MISTRAL_API_KEY\"),\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "# For comparison: V1 Mistral client\n",
    "llm_config_mistral_v1 = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"api_type\": \"mistral\",  # <-- V1 client architecture\n",
    "            \"model\": \"mistral-small-latest\",\n",
    "            \"api_key\": os.getenv(\"MISTRAL_API_KEY\"),\n",
    "        }\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Direct Client Usage\n",
    "\n",
    "Let's use the Mistral V2 client directly to see the rich response format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.llm_clients import MistralAIClientV2\n",
    "from autogen.llm_clients.models import UnifiedResponse\n",
    "\n",
    "# Create client\n",
    "client = MistralAIClientV2(api_key=os.getenv(\"MISTRAL_API_KEY\"))\n",
    "\n",
    "# Make a request\n",
    "response = client.create({\n",
    "    \"model\": \"mistral-small-latest\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}],\n",
    "})\n",
    "\n",
    "# Verify it's a UnifiedResponse\n",
    "print(f\"Response type: {type(response)}\")\n",
    "print(f\"Is UnifiedResponse: {isinstance(response, UnifiedResponse)}\")\n",
    "print(f\"\\nProvider: {response.provider}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"\\nText content: {response.text}\")\n",
    "print(f\"\\nUsage: {response.usage}\")\n",
    "print(f\"Cost: ${response.cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Accessing Rich Content\n",
    "\n",
    "The V2 client preserves all content in typed blocks. Let's explore the message structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access message content\n",
    "if response.messages:\n",
    "    message = response.messages[0]\n",
    "    print(f\"Role: {message.role}\")\n",
    "    print(f\"\\nContent blocks ({len(message.content)}):\")\n",
    "\n",
    "    for i, block in enumerate(message.content):\n",
    "        print(f\"\\n  Block {i + 1}:\")\n",
    "        print(f\"    Type: {block.type}\")\n",
    "        print(f\"    Class: {type(block).__name__}\")\n",
    "\n",
    "        if hasattr(block, \"text\"):\n",
    "            print(f\"    Text: {block.text[:100]}...\")\n",
    "\n",
    "        if hasattr(block, \"name\"):\n",
    "            print(f\"    Tool name: {block.name}\")\n",
    "            print(f\"    Tool arguments: {block.arguments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Tool Calls Example\n",
    "\n",
    "Let's see how tool calls are preserved in the V2 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with tool calls\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"City name\"}},\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "response_with_tools = client.create({\n",
    "    \"model\": \"mistral-small-latest\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}],\n",
    "    \"tools\": tools,\n",
    "})\n",
    "\n",
    "# Access tool calls\n",
    "if response_with_tools.messages:\n",
    "    message = response_with_tools.messages[0]\n",
    "    tool_calls = message.get_tool_calls()\n",
    "\n",
    "    print(f\"Text content: {message.get_text()}\")\n",
    "    print(f\"\\nTool calls found: {len(tool_calls)}\")\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        print(\"\\n  Tool Call:\")\n",
    "        print(f\"    ID: {tool_call.id}\")\n",
    "        print(f\"    Name: {tool_call.name}\")\n",
    "        print(f\"    Arguments: {tool_call.arguments}\")\n",
    "\n",
    "        # Parse arguments\n",
    "        import json\n",
    "\n",
    "        args = json.loads(tool_call.arguments)\n",
    "        print(f\"    Parsed args: {args}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Using with ConversableAgent\n",
    "\n",
    "The Mistral V2 client works seamlessly with AG2 agents. The agent automatically handles the rich response format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent\n",
    "\n",
    "# Create an agent using Mistral V2 client\n",
    "agent = ConversableAgent(\n",
    "    name=\"mistral_assistant\",\n",
    "    llm_config=llm_config_mistral_v2,\n",
    "    system_message=\"You are a helpful assistant powered by Mistral AI.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Chat with the agent\n",
    "reply = agent.generate_reply(messages=[{\"role\": \"user\", \"content\": \"Write a haiku about artificial intelligence.\"}])\n",
    "\n",
    "print(\"Agent Reply:\")\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Backward Compatibility: V1 Compatible Format\n",
    "\n",
    "The V2 client can also return responses in the legacy ChatCompletion format for compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get V1 compatible response\n",
    "v1_response = client.create_v1_compatible({\n",
    "    \"model\": \"mistral-small-latest\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "})\n",
    "\n",
    "print(\"V1 Compatible Response:\")\n",
    "print(f\"Type: {type(v1_response)}\")\n",
    "print(\"\\nStructure:\")\n",
    "print(f\"  ID: {v1_response['id']}\")\n",
    "print(f\"  Model: {v1_response['model']}\")\n",
    "print(f\"  Object: {v1_response['object']}\")\n",
    "print(f\"  Choices: {len(v1_response['choices'])}\")\n",
    "if v1_response[\"choices\"]:\n",
    "    print(f\"  Content: {v1_response['choices'][0]['message']['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Message Retrieval\n",
    "\n",
    "The `message_retrieval()` method works with UnifiedResponse and returns text or dicts based on content complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text response\n",
    "response = client.create({\n",
    "    \"model\": \"mistral-small-latest\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Say hello\"}],\n",
    "})\n",
    "\n",
    "messages = client.message_retrieval(response)\n",
    "print(f\"Retrieved messages: {messages}\")\n",
    "print(f\"Type: {type(messages[0])}\")\n",
    "\n",
    "# Response with tool calls\n",
    "response_with_tools = client.create({\n",
    "    \"model\": \"mistral-small-latest\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Get weather in NYC\"}],\n",
    "    \"tools\": tools,\n",
    "})\n",
    "\n",
    "messages_with_tools = client.message_retrieval(response_with_tools)\n",
    "print(f\"\\nRetrieved messages with tools: {messages_with_tools}\")\n",
    "print(f\"Type: {type(messages_with_tools[0])}\")\n",
    "if isinstance(messages_with_tools[0], dict):\n",
    "    print(f\"  Has tool_calls: {'tool_calls' in messages_with_tools[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Cost and Usage Tracking\n",
    "\n",
    "The V2 client provides easy access to usage statistics and cost calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.create({\n",
    "    \"model\": \"mistral-small-latest\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Write a short story about a robot.\"}],\n",
    "})\n",
    "\n",
    "# Direct access to cost\n",
    "print(f\"Cost: ${response.cost:.6f}\")\n",
    "\n",
    "# Get detailed usage\n",
    "usage = client.get_usage(response)\n",
    "print(\"\\nUsage Details:\")\n",
    "for key, value in usage.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Calculate cost separately\n",
    "calculated_cost = client.cost(response)\n",
    "print(f\"\\nCalculated cost: ${calculated_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Group Chat Example\n",
    "\n",
    "Mistral V2 client works seamlessly in group chats with other agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import GroupChat, GroupChatManager\n",
    "\n",
    "# Create agents with Mistral V2\n",
    "writer = ConversableAgent(\n",
    "    name=\"writer\",\n",
    "    llm_config=llm_config_mistral_v2,\n",
    "    system_message=\"You are a creative writer. Write engaging content.\",\n",
    "    description=\"Writes creative content\",\n",
    ")\n",
    "\n",
    "editor = ConversableAgent(\n",
    "    name=\"editor\",\n",
    "    llm_config=llm_config_mistral_v2,\n",
    "    system_message=\"You are an editor. Review and improve writing. Say DONE! when satisfied.\",\n",
    "    description=\"Reviews and edits content\",\n",
    ")\n",
    "\n",
    "# Setup group chat\n",
    "groupchat = GroupChat(\n",
    "    agents=[writer, editor],\n",
    "    speaker_selection_method=\"auto\",\n",
    "    messages=[],\n",
    ")\n",
    "\n",
    "manager = GroupChatManager(\n",
    "    name=\"manager\",\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config_mistral_v2,\n",
    "    is_termination_msg=lambda x: \"DONE!\" in (x.get(\"content\", \"\") or \"\").upper(),\n",
    ")\n",
    "\n",
    "# Start conversation\n",
    "result = writer.initiate_chat(\n",
    "    recipient=manager,\n",
    "    message=\"Write a blog post about the benefits of renewable energy.\",\n",
    ")\n",
    "\n",
    "print(\"\\nChat History:\")\n",
    "for msg in result.chat_history:\n",
    "    print(f\"\\n{msg['role']}: {msg.get('content', '')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Comparison: V1 vs V2 Client\n",
    "\n",
    "Let's compare the response formats between V1 and V2 clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.oai import OpenAIWrapper\n",
    "\n",
    "# V2 Client\n",
    "wrapper_v2 = OpenAIWrapper(config_list=llm_config_mistral_v2[\"config_list\"])\n",
    "response_v2 = wrapper_v2.create(messages=[{\"role\": \"user\", \"content\": \"Hello!\"}], cache_seed=None)\n",
    "\n",
    "print(\"V2 Response:\")\n",
    "print(f\"  Type: {type(response_v2)}\")\n",
    "print(f\"  Has .text property: {hasattr(response_v2, 'text')}\")\n",
    "if hasattr(response_v2, \"text\"):\n",
    "    print(f\"  Text: {response_v2.text}\")\n",
    "\n",
    "# V1 Client\n",
    "wrapper_v1 = OpenAIWrapper(config_list=llm_config_mistral_v1[\"config_list\"])\n",
    "response_v1 = wrapper_v1.create(messages=[{\"role\": \"user\", \"content\": \"Hello!\"}], cache_seed=None)\n",
    "\n",
    "print(\"\\nV1 Response:\")\n",
    "print(f\"  Type: {type(response_v1)}\")\n",
    "print(f\"  Has .text property: {hasattr(response_v1, 'text')}\")\n",
    "\n",
    "# Both work with OpenAIWrapper.extract_text_or_completion_object()\n",
    "text_v2 = wrapper_v2.extract_text_or_completion_object(response_v2)\n",
    "text_v1 = wrapper_v2.extract_text_or_completion_object(response_v1)\n",
    "\n",
    "print(f\"\\nExtracted text (V2): {text_v2}\")\n",
    "print(f\"Extracted text (V1): {text_v1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The Mistral V2 client provides:\n",
    "\n",
    "✅ **Rich UnifiedResponse** with typed content blocks\n",
    "✅ **Direct property access** (`response.text`, `response.cost`)\n",
    "✅ **Tool call preservation** as `ToolCallContent` blocks\n",
    "✅ **Backward compatibility** via `create_v1_compatible()` and `message_retrieval()`\n",
    "✅ **Seamless agent integration** - works with all AG2 agents\n",
    "✅ **Cost and usage tracking** built-in\n",
    "\n",
    "The V2 client maintains full compatibility with existing AG2 code while providing access to rich content features."
   ]
  }
 ],
 "metadata": {
  "front_matter": {
   "description": "Client V2 example: Mistral",
   "tags": [
    "mistral",
    "clients"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
