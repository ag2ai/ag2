{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list_deepseek_reasoner = [\n",
    "    {\n",
    "        \"model\": \"deepseek-reasoner\",\n",
    "        \"base_url\": \"https://api.deepseek.com/v1\",\n",
    "        \"api_key\": api_key,\n",
    "        \"api_type\": \"deepseek\",\n",
    "        \"tags\": [\"deepseek\"],\n",
    "    }\n",
    "]\n",
    "config_list_openai_4o_mini = [\n",
    "    {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    }\n",
    "]\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False},\n",
    ")\n",
    "\n",
    "supervisor = autogen.AssistantAgent(\n",
    "    \"supervisor\",\n",
    "    llm_config={\n",
    "        \"config_list\": config_list_deepseek_reasoner,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    \"assistant\",\n",
    "    llm_config={\n",
    "        \"config_list\": config_list_deepseek_reasoner,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, supervisor, assistant],\n",
    "    messages=[\"A group chat\"],\n",
    "    max_round=5,\n",
    ")\n",
    "\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config={\n",
    "        # Error is expected using deepseek_reasoner as manager\n",
    "        # \"config_list\": config_list_deepseek_reasoner,\n",
    "        #############\n",
    "        # raise self._make_status_error_from_response(err.response) from None\n",
    "        # openai.BadRequestError: Error code: 400 -\n",
    "        # {'error': {'message': 'The last message of deepseek-reasoner must be a user message,\n",
    "        # or an assistant message with prefix mode on (refer to https://api-docs.deepseek.com/guides/chat_prefix_completion).',\n",
    "        # 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
    "        #############\n",
    "        # Another expected using openai_4o_mini as manager\n",
    "        \"config_list\": config_list_openai_4o_mini,\n",
    "        #############\n",
    "        # Error code: 400 - {'error': {'message': 'deepseek-reasoner does not support successive\n",
    "        # user or assistant messages (messages[1] and messages[2] in your input).\n",
    "        # You should interleave the user/assistant messages in the message sequence.',\n",
    "        # 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}\n",
    "        #############\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# Group Chat\n",
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=\"\"\"\n",
    "\n",
    "    Save a chart of NVDA and TESLA stock price change YTD.\n",
    "\n",
    "    Start the chat with assistant.\n",
    "\n",
    "    And ask supervisor to check the work from assistant.\n",
    "\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import autogen\n",
    "# from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "\n",
    "# class DeepSeekReasonerAssistant(autogen.AssistantAgent):\n",
    "#     def __init__(self, name, **kwargs):\n",
    "#         super().__init__(name, **kwargs)\n",
    "\n",
    "#     # def generate_reply(self, messages=None, sender=None, config=None):\n",
    "#     #     print(f\"\\n=== {self.name} Processing Messages ===\")\n",
    "#     #     if messages is None:\n",
    "#     #         messages = self._oai_messages[sender]\n",
    "#     #     print(f\"Messages: {messages}\")\n",
    "\n",
    "#     #     # Get messages from chat history if available\n",
    "#     #     if hasattr(self, \"chat_messages\") and sender in self.chat_messages:\n",
    "#     #         chat_history = self.chat_messages[sender]\n",
    "#     #     else:\n",
    "#     #         chat_history = messages if messages else []\n",
    "#     #     print(f\"Chat History: {chat_history}\")\n",
    "\n",
    "#     #     if chat_history:\n",
    "#     #         # Create new message sequence\n",
    "#     #         new_messages = []\n",
    "\n",
    "#     #         # Add system message if it exists\n",
    "#     #         if hasattr(self, \"system_message\"):\n",
    "#     #             new_messages.append({\"role\": \"system\", \"content\": self.system_message})\n",
    "\n",
    "#     #         # Add original messages ensuring alternation\n",
    "#     #         for msg in chat_history:\n",
    "#     #             if len(new_messages) == 0 or msg[\"role\"] != new_messages[-1][\"role\"]:\n",
    "#     #                 new_messages.append(msg)\n",
    "\n",
    "#     #         # Check if last message was from assistant\n",
    "#     #         if new_messages and new_messages[-1][\"role\"] == \"assistant\":\n",
    "#     #             new_messages.append({\"role\": \"user\", \"content\": \"continue\"})\n",
    "#     #         print(f\"New Messages: {new_messages}\")\n",
    "\n",
    "#     #         # return super().generate_reply(messages=new_messages, sender=sender, config=config)\n",
    "#     #     return super().generate_reply(messages=messages, sender=sender, config=config)\n",
    "\n",
    "\n",
    "# # Create agents using the custom class\n",
    "# assistant = DeepSeekReasonerAssistant(\n",
    "#     \"assistant\",\n",
    "#     system_message=\"You are a coding assistant. Write code for the given task.\",\n",
    "#     llm_config={\n",
    "#         \"config_list\": config_list_deepseek_reasoner,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# supervisor = DeepSeekReasonerAssistant(\n",
    "#     \"supervisor\",\n",
    "#     system_message=\"\"\"You are a code reviewer. Review code for:\n",
    "#     1. Correctness\n",
    "#     2. Error handling\n",
    "#     3. Performance\n",
    "#     4. Best practices\"\"\",\n",
    "#     llm_config={\n",
    "#         \"config_list\": config_list_deepseek_reasoner,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "\n",
    "# user_proxy = autogen.UserProxyAgent(\n",
    "#     \"user_proxy\",\n",
    "#     human_input_mode=\"NEVER\",\n",
    "#     code_execution_config={\n",
    "#         \"executor\": LocalCommandLineCodeExecutor(work_dir=\"coding\"),\n",
    "#     },\n",
    "#     is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    "# )\n",
    "\n",
    "# # Create group chat\n",
    "# groupchat = autogen.GroupChat(agents=[user_proxy, assistant, supervisor], messages=[], max_round=5)\n",
    "\n",
    "# # Manager with chat model\n",
    "# manager = autogen.GroupChatManager(\n",
    "#     groupchat=groupchat,\n",
    "#     system_message=\"\"\"You are the coordinator. Follow this sequence:\n",
    "#     1. When user asks a question, select assistant to write code\n",
    "#     2. After assistant provides code, select supervisor to review it\n",
    "#     3. If the problem has been resolved respond with TERMINATE\"\"\",\n",
    "#     llm_config={\n",
    "#         \"config_list\": config_list_openai_4o_mini,\n",
    "#     },\n",
    "#     is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"TERMINATE\") >= 0,\n",
    "# )\n",
    "\n",
    "# print(\"Starting chat...\")\n",
    "# user_proxy.initiate_chat(manager, message=\"Give me some info about the stock market.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a code reviewer. Review code for:\\n    1. Correctness\\n    2. Error handling\\n    3. Performance\\n    4. Best practices\",\n",
    "            \"role\": \"system\",\n",
    "        },\n",
    "        {\"content\": \"Create a chart showing NVDA and TESLA stock prices.\", \"name\": \"user_proxy\", \"role\": \"user\"},\n",
    "        {\n",
    "            \"content\": \"To create a chart comparing NVDA and TESLA stock prices, you can use Python with libraries like `yfinance` (to fetch stock data) and `matplotlib` (for plotting). Here's a complete example:\\n\\n```python\\n# Install required libraries if needed\\n# !pip install yfinance matplotlib\\n\\nimport yfinance as yf\\nimport matplotlib.pyplot as plt\\n\\n# Define the tickers and time period\\ntickers = ['NVDA', 'TSLA']\\nstart_date = '2023-01-01'\\nend_date = '2024-01-01'\\n\\n# Download historical data\\ndata = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\\n\\n# Create plot\\nplt.figure(figsize=(12, 6))\\nplt.plot(data['NVDA'], label='NVIDIA (NVDA)', color='#76b900')  # NVIDIA green\\nplt.plot(data['TSLA'], label='Tesla (TSLA)', color='#cc0000')    # Tesla red\\n\\n# Formatting\\nplt.title('NVDA vs TESLA Stock Prices (2023)', fontsize=14)\\nplt.xlabel('Date', fontsize=12)\\nplt.ylabel('Adjusted Close Price (USD)', fontsize=12)\\nplt.legend()\\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\\nplt.xticks(rotation=45)\\nplt.tight_layout()\\n\\n# Show plot\\nplt.show()\\n```\\n\\nThis code will:\\n1. Fetch adjusted closing prices for both stocks from Yahoo Finance\\n2. Create a line chart comparing their performance\\n3. Use official brand colors (NVIDIA green and Tesla red)\\n4. Include proper labels and formatting\\n\\nThe chart will show the stock price trends from January 2023 to January 2024. You can modify the `start_date` and `end_date` variables to adjust the time period.\\n\\nMake sure you have an internet connection as the script needs to download stock data from Yahoo Finance. The first time you run it, you might need to install the required libraries using pip.\",\n",
    "            \"name\": \"assistant\",\n",
    "            \"role\": \"user\",\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"exitcode: 1 (execution failed)\\nCode output: Traceback (most recent call last):\\n  File \\\"/Users/robert/projects/ag2/notebook/coding/Install required libraries if needed\\\", line 4, in <module>\\n    import yfinance as yf\\nModuleNotFoundError: No module named 'yfinance'\\n\",\n",
    "            \"name\": \"user_proxy\",\n",
    "            \"role\": \"user\",\n",
    "        },\n",
    "    ],\n",
    "    \"model\": \"deepseek-reasoner\",\n",
    "    \"stream\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
