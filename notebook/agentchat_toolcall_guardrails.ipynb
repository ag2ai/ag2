{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool Call Guardrails\n",
    "\n",
    "Tool call guardrails allow you to inspect and validate the **arguments** of tool calls that agents suggest before they are executed. This provides a safety mechanism to detect potentially harmful, illegal, or policy-violating tool arguments.\n",
    "\n",
    "**Key Features:**\n",
    "- Inspects tool name and arguments extracted from agent messages\n",
    "- Uses LLM-based detection for context-aware validation\n",
    "- Can be used as input or output guardrails\n",
    "- Automatically handles cases where no tool calls are present\n",
    "\n",
    "Please find documentation about guardrails [here](https://docs.ag2.ai/latest/docs/user-guide/advanced-concepts/orchestration/group-chat/guardrails).\n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Install `ag2`:\n",
    "```bash\n",
    "pip install ag2[openai]\n",
    "```\n",
    "\n",
    "For more information, please refer to the [installation guide](https://docs.ag2.ai/latest/docs/user-guide/basic-concepts/installing-ag2).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LLM Configuration\n",
    "\n",
    "Configure your LLM for both the agents and the guardrail checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen import AssistantAgent, GroupChat, GroupChatManager, UserProxyAgent\n",
    "from autogen.agentchat.group.guardrails import ToolCallLLMGuardrail\n",
    "from autogen.agentchat.group.targets.transition_target import StayTarget\n",
    "from autogen.llm_config import LLMConfig\n",
    "\n",
    "# Configure LLM for agents and guardrails\n",
    "llm_config = LLMConfig(\n",
    "    model=\"gpt-5-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    api_type=\"openai\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Output Guardrail - Checking Tool Calls Before Execution\n",
    "\n",
    "In this example, we register a `ToolCallLLMGuardrail` as an **output guardrail** on an assistant. When the assistant suggests tool calls, the guardrail inspects the tool arguments before they are executed.\n",
    "\n",
    "The guardrail will:\n",
    "- Extract tool call information (name and arguments) from the assistant's reply\n",
    "- Send this information to an LLM with your condition\n",
    "- Return `activated=True` if the condition is met (e.g., harmful content detected)\n",
    "- Return `activated=False` if no tool calls are present or the condition is not met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tool call guardrail\n",
    "tool_call_guardrail = ToolCallLLMGuardrail(\n",
    "    name=\"tool_safety\",\n",
    "    condition=\"arguments contain harmful or illegal content.\",\n",
    "    target=StayTarget(),\n",
    "    llm_config=llm_config,\n",
    "    activation_message=\"Tool call blocked: Potentially harmful arguments detected.\",\n",
    ")\n",
    "\n",
    "# Create an assistant with the guardrail\n",
    "assistant = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=\"You are a helpful assistant. Use tools when needed.\",\n",
    ")\n",
    "\n",
    "# Register the guardrail as an output guardrail\n",
    "assistant.register_output_guardrail(tool_call_guardrail)\n",
    "\n",
    "print(f\"Guardrail condition: {tool_call_guardrail.condition}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Guardrail Condition\n",
    "\n",
    "Notice that `ToolCallLLMGuardrail` automatically prepends \"Here are arguments to a Tool call function.\" to your condition. This helps the LLM understand the context of what it's checking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Using Tool Call Guardrail in GroupChat\n",
    "\n",
    "Let's create a complete example with a GroupChat where an assistant can suggest tool calls, and the guardrail checks them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a guardrail for checking tool arguments\n",
    "joke_guardrail = ToolCallLLMGuardrail(\n",
    "    name=\"no_jokes\",\n",
    "    condition=\"arguments contain jokes or humor.\",\n",
    "    target=StayTarget(),\n",
    "    llm_config=llm_config,\n",
    "    activation_message=\"âš ï¸ Joke detected in tool arguments - request blocked.\",\n",
    ")\n",
    "\n",
    "def joke_tool(joke: str) -> str:\n",
    "    \"\"\"\n",
    "        This tool is used to detect PII in the code.\n",
    "    \"\"\"\n",
    "    return joke\n",
    "\n",
    "# Create agents\n",
    "assistant = AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    functions=[joke_tool]\n",
    ")\n",
    "assistant.register_input_guardrail(joke_guardrail)\n",
    "\n",
    "# Register function for execution so GroupChat can find an agent to execute it\n",
    "assistant.register_for_execution()(joke_tool)\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Create GroupChat\n",
    "groupchat = GroupChat(\n",
    "    agents=[assistant, user_proxy],\n",
    "    messages=[],\n",
    "    max_round=3,\n",
    ")\n",
    "manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "# Start a conversation\n",
    "user_proxy.run(\n",
    "    manager,\n",
    "    message=\"write a joke, use joke tool to write a joke\",\n",
    ").process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Direct Guardrail Check\n",
    "\n",
    "You can also call the guardrail's `check()` method directly with a context that contains tool calls. This is useful for testing or programmatic validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "# Create a guardrail\n",
    "guardrail = ToolCallLLMGuardrail(\n",
    "    name=\"safety_check\",\n",
    "    condition=\"arguments contain dangerous or illegal content.\",\n",
    "    target=StayTarget(),\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# Simulate a context with tool calls\n",
    "context_with_tool_calls: list[dict[str, Any]] = [\n",
    "    {\"role\": \"user\", \"content\": \"Run this code for me.\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": None,\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"id\": \"call_1\",\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\"name\": \"run_code\", \"arguments\": '{\"code\": \"1 + 1\"}'},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# Check the tool calls\n",
    "result = guardrail.check(context=context_with_tool_calls)\n",
    "\n",
    "print(f\"Guardrail activated: {result.activated}\")\n",
    "print(f\"Justification: {result.justification}\")\n",
    "print(f\"Guardrail name: {result.guardrail.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Input Guardrail\n",
    "\n",
    "You can also use `ToolCallLLMGuardrail` as an **input guardrail** to check tool calls before they reach an agent. This is useful when you want to validate tool calls from one agent before another agent processes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input guardrail\n",
    "input_guardrail = ToolCallLLMGuardrail(\n",
    "    name=\"input_safety\",\n",
    "    condition=\"arguments contain commands that could delete files or modify system settings.\",\n",
    "    target=StayTarget(),\n",
    "    llm_config=llm_config,\n",
    "    activation_message=\"ðŸš« Dangerous tool call detected - blocked.\",\n",
    ")\n",
    "\n",
    "# Register as input guardrail on an executor agent\n",
    "executor = AssistantAgent(\n",
    "    name=\"executor\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "executor.register_input_guardrail(input_guardrail)\n",
    "\n",
    "print(\"Input guardrail registered on executor agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "1. **Extraction**: When `check()` is called, `ToolCallLLMGuardrail` extracts tool call information from the context:\n",
    "   - If context is a list of messages, it checks the last message for `tool_calls`\n",
    "   - If context is a dict, it checks for a `tool_calls` key\n",
    "   - If no tool calls are found, it returns `activated=False` immediately\n",
    "\n",
    "2. **Parsing**: Tool calls are parsed into a string format: `name='tool_name' arguments='{...}'`\n",
    "\n",
    "3. **LLM Check**: The parsed tool call data is sent to the LLM with your condition (prefixed with \"Here are arguments to a Tool call function.\")\n",
    "\n",
    "4. **Result**: The LLM returns whether the condition is met, and a `GuardrailResult` is returned with `activated`, `justification`, and the `guardrail` reference.\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "- Use **output guardrails** when you want to check tool calls before they are executed\n",
    "- Use **input guardrails** when you want to validate tool calls before another agent processes them\n",
    "- Choose appropriate **transition targets** (e.g., `StayTarget`, `AgentTarget`) based on your workflow\n",
    "- Provide clear **activation messages** to help users understand why a guardrail was triggered\n",
    "- Test your guardrails with various tool call scenarios to ensure they work as expected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
